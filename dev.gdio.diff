diff --git a/docs/production/lesson-1-export-jit.ipynb b/docs/production/lesson-1-export-jit.ipynb
index 212d191..93fe944 100644
--- ./docs/production/lesson-1-export-jit.ipynb
+++ ./docs/production/lesson-1-export-jit.ipynb
@@ -236,6 +236,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/docs/production/lesson-1-export-jit.py b/docs/production/lesson-1-export-jit.py
new file mode 100644
index 0000000..de6ec35
--- /dev/null
+++ ./docs/production/lesson-1-export-jit.py
@@ -0,0 +1,103 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # fast.ai lesson 1 - training on Notebook Instance and export to torch.jit model
+
+# ## Overview
+# This notebook shows how to use the SageMaker Python SDK to train your fast.ai model on a SageMaker notebook instance then export it as a torch.jit model to be used for inference on AWS Lambda.
+#
+# ## Set up the environment
+#
+# You will need a Jupyter notebook with the `boto3` and `fastai` libraries installed. You can do this with the command `pip install boto3 fastai`
+#
+# This notebook was created and tested on a single ml.p3.2xlarge notebook instance. 
+#
+
+# ## Train your model
+#
+# We are going to train a fast.ai model as per [Lesson 1 of the fast.ai MOOC course](https://course.fast.ai/videos/?lesson=1) locally on the SageMaker Notebook instance. We will then save the model weights and upload them to S3.
+#
+#
+
+# %reload_ext autoreload
+# %autoreload 2
+# %matplotlib inline
+
+# +
+import os
+import io
+import tarfile
+
+import PIL
+
+import boto3
+
+from fastai.vision import *
+# -
+
+path = untar_data(URLs.PETS); path
+
+path_anno = path/'annotations'
+path_img = path/'images'
+fnames = get_image_files(path_img)
+np.random.seed(2)
+pat = re.compile(r'/([^/]+)_\d+.jpg$')
+
+bs=64
+img_size=299
+
+data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(),
+                                   size=img_size, bs=bs//2).normalize(imagenet_stats)
+
+learn = cnn_learner(data, models.resnet50, metrics=error_rate)
+
+learn.lr_find()
+learn.recorder.plot()
+
+learn.fit_one_cycle(8)
+
+learn.unfreeze()
+learn.fit_one_cycle(3, max_lr=slice(1e-6,1e-4))
+
+# ## Export model and upload to S3
+
+# Now that we have trained our model we need to export it, create a tarball of the artefacts and upload to S3.
+#
+
+# First we need to export the class names from the data object into a text file.
+
+save_texts(path_img/'models/classes.txt', data.classes)
+
+# Now we need to export the model in the [PyTorch TorchScript format](https://pytorch.org/docs/stable/jit.html) so we can load into an AWS Lambda function.
+
+trace_input = torch.ones(1,3,img_size,img_size).cuda()
+jit_model = torch.jit.trace(learn.model.float(), trace_input)
+model_file='resnet50_jit.pth'
+output_path = str(path_img/f'models/{model_file}')
+torch.jit.save(jit_model, output_path)
+
+# Next step is to create a tarfile of the exported classes file and model weights.
+
+tar_file=path_img/'models/model.tar.gz'
+classes_file='classes.txt'
+
+with tarfile.open(tar_file, 'w:gz') as f:
+    f.add(path_img/f'models/{model_file}', arcname=model_file)
+    f.add(path_img/f'models/{classes_file}', arcname=classes_file)
+
+# Now we need to upload the model tarball to S3.
+
+s3 = boto3.resource('s3')
+s3.meta.client.upload_file(str(tar_file), 'REPLACE_WITH_YOUR_BUCKET_NAME', 'fastai-models/lesson1/model.tar.gz')
diff --git a/install_fastai.sh b/install_fastai.sh
new file mode 100755
index 0000000..09a9a1e
--- /dev/null
+++ ./install_fastai.sh
@@ -0,0 +1,10 @@
+#!/bin/zsh
+SCRIPT=$(realpath "$0")
+SCRIPTPATH=$(dirname "$SCRIPT")
+cd "$SCRIPTPATH"
+
+conda update conda
+conda install -c pytorch -c fastai fastai pytorch
+conda install -c conda-forge \
+    jupytext \
+    neovim
diff --git a/nbs/dl1/00_notebook_tutorial.ipynb b/nbs/dl1/00_notebook_tutorial.ipynb
index 3ea1120..617548a 100644
--- ./nbs/dl1/00_notebook_tutorial.ipynb
+++ ./nbs/dl1/00_notebook_tutorial.ipynb
@@ -785,10 +785,25 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
    "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.6.10"
   }
  },
  "nbformat": 4,
diff --git a/nbs/dl1/00_notebook_tutorial.py b/nbs/dl1/00_notebook_tutorial.py
new file mode 100644
index 0000000..b705586
--- /dev/null
+++ ./nbs/dl1/00_notebook_tutorial.py
@@ -0,0 +1,252 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# **Important note:** You should always work on a duplicate of the course notebook. On the page you used to open this, tick the box next to the name of the notebook and click duplicate to easily create a new version of this notebook.
+#
+# You will get errors each time you try to update your course repository if you don't do this, and your changes will end up being erased by the original course version.
+
+# # Welcome to Jupyter Notebooks!
+
+# If you want to learn how to use this tool you've come to the right place. This article will teach you all you need to know to use Jupyter Notebooks effectively. You only need to go through Section 1 to learn the basics and you can go into Section 2 if you want to further increase your productivity.
+
+# You might be reading this tutorial in a web page (maybe Github or the course's webpage). We strongly suggest to read this tutorial in a (yes, you guessed it) Jupyter Notebook. This way you will be able to actually *try* the different commands we will introduce here.
+
+# ## Section 1: Need to Know
+
+# ### Introduction
+
+# Let's build up from the basics, what is a Jupyter Notebook? Well, you are reading one. It is a document made of cells. You can write like I am writing now (markdown cells) or you can perform calculations in Python (code cells) and run them like this:
+
+1+1
+
+# Cool huh? This combination of prose and code makes Jupyter Notebook ideal for experimentation: we can see the rationale for each experiment, the code and the results in one comprehensive document. In fast.ai, each lesson is documented in a notebook and you can later use that notebook to experiment yourself. 
+#
+# Other renowned institutions in academy and industry use Jupyter Notebook: Google, Microsoft, IBM, Bloomberg, Berkeley and NASA among others. Even Nobel-winning economists [use Jupyter Notebooks](https://paulromer.net/jupyter-mathematica-and-the-future-of-the-research-paper/)  for their experiments and some suggest that Jupyter Notebooks will be the [new format for research papers](https://www.theatlantic.com/science/archive/2018/04/the-scientific-paper-is-obsolete/556676/).
+
+# ### Writing
+
+# A type of cell in which you can write like this is called _Markdown_. [_Markdown_](https://en.wikipedia.org/wiki/Markdown) is a very popular markup language. To specify that a cell is _Markdown_ you need to click in the drop-down menu in the toolbar and select _Markdown_.
+
+# Click on the '+' button on the left and select _Markdown_ from the toolbar.
+
+# Now you can type your first _Markdown_ cell. Write 'My first markdown cell' and press run.
+
+# ![add](images/notebook_tutorial/add.png)
+
+# You should see something like this:
+
+# My first markdown cell
+
+# Now try making your first _Code_ cell: follow the same steps as before but don't change the cell type (when you add a cell its default type is _Code_). Type something like 3/2. You should see '1.5' as output.
+
+3/2
+
+# ### Modes
+
+# If you made a mistake in your *Markdown* cell and you have already ran it, you will notice that you cannot edit it just by clicking on it. This is because you are in **Command Mode**. Jupyter Notebooks have two distinct modes:
+#
+# 1. **Edit Mode**: Allows you to edit a cell's content.
+#
+# 2. **Command Mode**: Allows you to edit the notebook as a whole and use keyboard shortcuts but not edit a cell's content. 
+#
+# You can toggle between these two by either pressing <kbd>ESC</kbd> and <kbd>Enter</kbd> or clicking outside a cell or inside it (you need to double click if its a Markdown cell). You can always know which mode you're on since the current cell has a green border if in **Edit Mode** and a blue border in **Command Mode**. Try it!
+
+# ### Other Important Considerations
+
+# 1. Your notebook is autosaved every 120 seconds. If you want to manually save it you can just press the save button on the upper left corner or press <kbd>s</kbd> in **Command Mode**.
+
+# ![Save](images/notebook_tutorial/save.png)
+
+# 2. To know if your kernel is computing or not you can check the dot in your upper right corner. If the dot is full, it means that the kernel is working. If not, it is idle. You can place the mouse on it and see the state of the kernel be displayed.
+
+# ![Busy](images/notebook_tutorial/busy.png)
+
+# 3. There are a couple of shortcuts you must know about which we use **all** the time (always in **Command Mode**). These are:
+#
+# <kbd>Shift</kbd>+<kbd>Enter</kbd>: Runs the code or markdown on a cell
+#
+# <kbd>Up Arrow</kbd>+<kbd>Down Arrow</kbd>: Toggle across cells
+#
+# <kbd>b</kbd>: Create new cell
+#
+# <kbd>0</kbd>+<kbd>0</kbd>: Reset Kernel
+#
+# You can find more shortcuts in the Shortcuts section below.
+
+# 4. You may need to use a terminal in a Jupyter Notebook environment (for example to git pull on a repository). That is very easy to do, just press 'New' in your Home directory and 'Terminal'. Don't know how to use the Terminal? We made a tutorial for that as well. You can find it [here](https://course.fast.ai/terminal_tutorial.html).
+
+# ![Terminal](images/notebook_tutorial/terminal.png)
+
+# That's it. This is all you need to know to use Jupyter Notebooks. That said, we have more tips and tricks below ↓↓↓
+
+# ## Section 2: Going deeper
+
+# + [markdown] hide_input=false
+# ### Markdown formatting
+# -
+
+# #### Italics, Bold, Strikethrough, Inline, Blockquotes and Links
+
+# The five most important concepts to format your code appropriately when using markdown are:
+#     
+# 1. *Italics*: Surround your text with '\_' or '\*'
+# 2. **Bold**: Surround your text with '\__' or '\**'
+# 3. `inline`: Surround your text with '\`'
+# 4.  > blockquote: Place '\>' before your text.
+# 5.  [Links](https://course.fast.ai/): Surround the text you want to link with '\[\]' and place the link adjacent to the text, surrounded with '()'
+#
+
+# #### Headings
+
+# Notice that including a hashtag before the text in a markdown cell makes the text a heading. The number of hashtags you include will determine the priority of the header ('#' is level one, '##' is level two, '###' is level three and '####' is level four). We will add three new cells with the '+' button on the left to see how every level of heading looks.
+
+# Double click on some headings and find out what level they are!
+
+# #### Lists
+
+# There are three types of lists in markdown.
+
+# Ordered list:
+#
+# 1. Step 1
+#     2. Step 1B
+# 3. Step 3
+
+# Unordered list
+#
+# * learning rate
+# * cycle length
+# * weight decay
+
+# Task list
+#
+# - [x] Learn Jupyter Notebooks
+#     - [x] Writing
+#     - [x] Modes
+#     - [x] Other Considerations
+# - [ ] Change the world
+
+# Double click on each to see how they are built! 
+
+# ### Code Capabilities
+
+# **Code** cells are different than **Markdown** cells in that they have an output cell. This means that we can _keep_ the results of our code within the notebook and share them. Let's say we want to show a graph that explains the result of an experiment. We can just run the necessary cells and save the notebook. The output will be there when we open it again! Try it out by running the next four cells.
+
+# Import necessary libraries
+from fastai.vision import * 
+import matplotlib.pyplot as plt
+
+from PIL import Image
+
+a = 1
+b = a + 1
+c = b + a + 1
+d = c + b + a + 1
+a, b, c ,d
+
+plt.plot([a,b,c,d])
+plt.show()
+
+# We can also print images while experimenting. I am watching you.
+
+Image.open('images/notebook_tutorial/cat_example.jpg')
+
+# ### Running the app locally
+
+# You may be running Jupyter Notebook from an interactive coding environment like Gradient, Sagemaker or Salamander. You can also run a Jupyter Notebook server from your local computer. What's more, if you have installed Anaconda you don't even need to install Jupyter (if not, just `pip install jupyter`).
+#
+# You just need to run `jupyter notebook` in your terminal. Remember to run it from a folder that contains all the folders/files you will want to access. You will be able to open, view and edit files located within the directory in which you run this command but not files in parent directories.
+#
+# If a browser tab does not open automatically once you run the command, you should CTRL+CLICK the link starting with 'https://localhost:' and this will open a new tab in your default browser.
+
+# ### Creating a notebook
+
+# Click on 'New' in the upper right corner and 'Python 3' in the drop-down list (we are going to use a [Python kernel](https://github.com/ipython/ipython) for all our experiments).
+#
+# ![new_notebook](images/notebook_tutorial/new_notebook.png)
+#
+# Note: You will sometimes hear people talking about the Notebook 'kernel'. The 'kernel' is just the Python engine that performs the computations for you. 
+
+# ### Shortcuts and tricks
+
+# #### Command Mode Shortcuts
+
+# There are a couple of useful keyboard shortcuts in `Command Mode` that you can leverage to make Jupyter Notebook faster to use. Remember that to switch back and forth between `Command Mode` and `Edit Mode` with <kbd>Esc</kbd> and <kbd>Enter</kbd>.
+
+# <kbd>m</kbd>: Convert cell to Markdown
+
+# <kbd>y</kbd>: Convert cell to Code
+
+# <kbd>D</kbd>+<kbd>D</kbd>: Delete the cell(if it's not the only cell) or delete the content of the cell and reset cell to Code(if only one cell left)
+
+# <kbd>o</kbd>: Toggle between hide or show output
+
+# <kbd>Shift</kbd>+<kbd>Arrow up/Arrow down</kbd>: Selects multiple cells. Once you have selected them you can operate on them like a batch (run, copy, paste etc).
+
+# <kbd>Shift</kbd>+<kbd>M</kbd>: Merge selected cells.
+
+# <kbd>Shift</kbd>+<kbd>Tab</kbd>: [press these two buttons at the same time, once] Tells you which parameters to pass on a function
+#
+# <kbd>Shift</kbd>+<kbd>Tab</kbd>: [press these two buttons at the same time, three times] Gives additional information on the method
+
+# #### Cell Tricks
+
+from fastai import *
+from fastai.vision import *
+
+# There are also some tricks that you can code into a cell.
+
+# `?function-name`: Shows the definition and docstring for that function
+
+# ?ImageDataBunch
+
+# `??function-name`: Shows the source code for that function
+
+??ImageDataBunch
+
+# `doc(function-name)`: Shows the definition, docstring **and links to the documentation** of the function
+# (only works with fastai library imported)
+
+doc(ImageDataBunch)
+
+# #### Line Magics
+
+# Line magics are functions that you can run on cells and take as an argument the rest of the line from where they are called. You call them by placing a '%' sign before the command. The most useful ones are:
+
+# `%matplotlib inline`: This command ensures that all matplotlib plots will be plotted in the output cell within the notebook and will be kept in the notebook when saved.
+
+# `%reload_ext autoreload`, `%autoreload 2`: Reload all modules before executing a new line. If a module is edited, it is not necessary to rerun the import commands, the modules will be reloaded automatically.
+
+# These three commands are always called together at the beginning of every notebook.
+
+# %matplotlib inline
+# %reload_ext autoreload
+# %autoreload 2
+
+# `%timeit`: Runs a line ten thousand times and displays the average time it took to run it.
+
+# %timeit [i+1 for i in range(1000)]
+
+# `%debug`: Allows to inspect a function which is showing an error using the [Python debugger](https://docs.python.org/3/library/pdb.html).
+
+for i in range(1000):
+    a = i+1
+    b = 'string'
+    c = b+1
+
+# %debug
+
+
diff --git a/nbs/dl1/lesson1-pets.ipynb b/nbs/dl1/lesson1-pets.ipynb
index ab39137..00e261e 100644
--- ./nbs/dl1/lesson1-pets.ipynb
+++ ./nbs/dl1/lesson1-pets.ipynb
@@ -20,9 +20,22 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 1,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "\n",
+      "Bad key \"text.kerning_factor\" on line 4 in\n",
+      "/root/anaconda3/envs/fastai/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
+      "You probably need to get an updated matplotlibrc file from\n",
+      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
+      "or from the matplotlib source distribution\n"
+     ]
+    }
+   ],
    "source": [
     "%reload_ext autoreload\n",
     "%autoreload 2\n",
@@ -38,7 +51,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 2,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -1328,10 +1341,25 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
    "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.6.10"
   }
  },
  "nbformat": 4,
diff --git a/nbs/dl1/lesson1-pets.md b/nbs/dl1/lesson1-pets.md
new file mode 100644
index 0000000..5cced01
--- /dev/null
+++ ./nbs/dl1/lesson1-pets.md
@@ -0,0 +1,305 @@
+---
+jupyter:
+  jupytext:
+    formats: ipynb,md
+    text_representation:
+      extension: .md
+      format_name: markdown
+      format_version: '1.2'
+      jupytext_version: 1.5.2
+  kernelspec:
+    display_name: Python 3
+    language: python
+    name: python3
+---
+
+# Lesson 1 - What's your pet
+
+
+Welcome to lesson 1! For those of you who are using a Jupyter Notebook for the first time, you can learn about this useful tool in a tutorial we prepared specially for you; click `File`->`Open` now and click `00_notebook_tutorial.ipynb`. 
+
+In this lesson we will build our first image classifier from scratch, and see if we can achieve world-class results. Let's dive in!
+
+Every notebook starts with the following three lines; they ensure that any edits to libraries you make are reloaded here automatically, and also that any charts or images displayed are shown in this notebook.
+
+```python
+%reload_ext autoreload
+%autoreload 2
+%matplotlib inline
+```
+
+We import all the necessary packages. We are going to work with the [fastai V1 library](http://www.fast.ai/2018/10/02/fastai-ai/) which sits on top of [Pytorch 1.0](https://hackernoon.com/pytorch-1-0-468332ba5163). The fastai library provides many useful functions that enable us to quickly and easily build neural networks and train our models.
+
+```python
+from fastai.vision import *
+from fastai.metrics import error_rate
+```
+
+If you're using a computer with an unusually small GPU, you may get an out of memory error when running this notebook. If this happens, click Kernel->Restart, uncomment the 2nd line below to use a smaller *batch size* (you'll learn all about what this means during the course), and try again.
+
+```python
+bs = 64
+# bs = 16   # uncomment this line if you run out of memory even after clicking Kernel->Restart
+```
+
+## Looking at the data
+
+
+We are going to use the [Oxford-IIIT Pet Dataset](http://www.robots.ox.ac.uk/~vgg/data/pets/) by [O. M. Parkhi et al., 2012](http://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf) which features 12 cat breeds and 25 dogs breeds. Our model will need to learn to differentiate between these 37 distinct categories. According to their paper, the best accuracy they could get in 2012 was 59.21%, using a complex model that was specific to pet detection, with separate "Image", "Head", and "Body" models for the pet photos. Let's see how accurate we can be using deep learning!
+
+We are going to use the `untar_data` function to which we must pass a URL as an argument and which will download and extract the data.
+
+```python
+help(untar_data)
+```
+
+```python
+path = untar_data(URLs.PETS); path
+```
+
+```python
+path.ls()
+```
+
+```python
+path_anno = path/'annotations'
+path_img = path/'images'
+```
+
+The first thing we do when we approach a problem is to take a look at the data. We _always_ need to understand very well what the problem is and what the data looks like before we can figure out how to solve it. Taking a look at the data means understanding how the data directories are structured, what the labels are and what some sample images look like.
+
+The main difference between the handling of image classification datasets is the way labels are stored. In this particular dataset, labels are stored in the filenames themselves. We will need to extract them to be able to classify the images into the correct categories. Fortunately, the fastai library has a handy function made exactly for this, `ImageDataBunch.from_name_re` gets the labels from the filenames using a [regular expression](https://docs.python.org/3.6/library/re.html).
+
+```python
+fnames = get_image_files(path_img)
+fnames[:5]
+```
+
+Set the random seed to two to guarantee that the same validation set is every time. This will give you consistent results with what you see in the lesson video.
+
+```python
+np.random.seed(2)
+pat = r'/([^/]+)_\d+.jpg$'
+```
+
+```python
+data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=bs
+                                  ).normalize(imagenet_stats)
+```
+
+```python
+data.show_batch(rows=3, figsize=(7,6))
+```
+
+```python
+print(data.classes)
+len(data.classes),data.c
+```
+
+## Training: resnet34
+
+
+Now we will start training our model. We will use a [convolutional neural network](http://cs231n.github.io/convolutional-networks/) backbone and a fully connected head with a single hidden layer as a classifier. Don't know what these things mean? Not to worry, we will dive deeper in the coming lessons. For the moment you need to know that we are building a model which will take images as input and will output the predicted probability for each of the categories (in this case, it will have 37 outputs).
+
+We will train for 4 epochs (4 cycles through all our data).
+
+```python
+learn = cnn_learner(data, models.resnet34, metrics=error_rate)
+```
+
+```python
+learn.model
+```
+
+```python
+learn.fit_one_cycle(4)
+```
+
+```python
+learn.save('stage-1')
+```
+
+## Results
+
+
+Let's see what results we have got. 
+
+We will first see which were the categories that the model most confused with one another. We will try to see if what the model predicted was reasonable or not. In this case the mistakes look reasonable (none of the mistakes seems obviously naive). This is an indicator that our classifier is working correctly. 
+
+Furthermore, when we plot the confusion matrix, we can see that the distribution is heavily skewed: the model makes the same mistakes over and over again but it rarely confuses other categories. This suggests that it just finds it difficult to distinguish some specific categories between each other; this is normal behaviour.
+
+```python
+interp = ClassificationInterpretation.from_learner(learn)
+
+losses,idxs = interp.top_losses()
+
+len(data.valid_ds)==len(losses)==len(idxs)
+```
+
+```python
+interp.plot_top_losses(9, figsize=(15,11))
+```
+
+```python
+doc(interp.plot_top_losses)
+```
+
+```python
+interp.plot_confusion_matrix(figsize=(12,12), dpi=60)
+```
+
+```python
+interp.most_confused(min_val=2)
+```
+
+## Unfreezing, fine-tuning, and learning rates
+
+
+Since our model is working as we expect it to, we will *unfreeze* our model and train some more.
+
+```python
+learn.unfreeze()
+```
+
+```python
+learn.fit_one_cycle(1)
+```
+
+```python
+learn.load('stage-1');
+```
+
+```python
+learn.lr_find()
+```
+
+```python
+learn.recorder.plot()
+```
+
+```python
+learn.unfreeze()
+learn.fit_one_cycle(2, max_lr=slice(1e-6,1e-4))
+```
+
+That's a pretty accurate model!
+
+
+## Training: resnet50
+
+
+Now we will train in the same way as before but with one caveat: instead of using resnet34 as our backbone we will use resnet50 (resnet34 is a 34 layer residual network while resnet50 has 50 layers. It will be explained later in the course and you can learn the details in the [resnet paper](https://arxiv.org/pdf/1512.03385.pdf)).
+
+Basically, resnet50 usually performs better because it is a deeper network with more parameters. Let's see if we can achieve a higher performance here. To help it along, let's use larger images too, since that way the network can see more detail. We reduce the batch size a bit since otherwise this larger network will require more GPU memory.
+
+```python
+data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(),
+                                   size=299, bs=bs//2).normalize(imagenet_stats)
+```
+
+```python
+learn = cnn_learner(data, models.resnet50, metrics=error_rate)
+```
+
+```python
+learn.lr_find()
+learn.recorder.plot()
+```
+
+```python
+learn.fit_one_cycle(8)
+```
+
+```python
+learn.save('stage-1-50')
+```
+
+It's astonishing that it's possible to recognize pet breeds so accurately! Let's see if full fine-tuning helps:
+
+```python
+learn.unfreeze()
+learn.fit_one_cycle(3, max_lr=slice(1e-6,1e-4))
+```
+
+If it doesn't, you can always go back to your previous model.
+
+```python
+learn.load('stage-1-50');
+```
+
+```python
+interp = ClassificationInterpretation.from_learner(learn)
+```
+
+```python
+interp.most_confused(min_val=2)
+```
+
+## Other data formats
+
+```python
+path = untar_data(URLs.MNIST_SAMPLE); path
+```
+
+```python
+tfms = get_transforms(do_flip=False)
+data = ImageDataBunch.from_folder(path, ds_tfms=tfms, size=26)
+```
+
+```python
+data.show_batch(rows=3, figsize=(5,5))
+```
+
+```python
+learn = cnn_learner(data, models.resnet18, metrics=accuracy)
+learn.fit(2)
+```
+
+```python
+df = pd.read_csv(path/'labels.csv')
+df.head()
+```
+
+```python
+data = ImageDataBunch.from_csv(path, ds_tfms=tfms, size=28)
+```
+
+```python
+data.show_batch(rows=3, figsize=(5,5))
+data.classes
+```
+
+```python
+data = ImageDataBunch.from_df(path, df, ds_tfms=tfms, size=24)
+data.classes
+```
+
+```python
+fn_paths = [path/name for name in df['name']]; fn_paths[:2]
+```
+
+```python
+pat = r"/(\d)/\d+\.png$"
+data = ImageDataBunch.from_name_re(path, fn_paths, pat=pat, ds_tfms=tfms, size=24)
+data.classes
+```
+
+```python
+data = ImageDataBunch.from_name_func(path, fn_paths, ds_tfms=tfms, size=24,
+        label_func = lambda x: '3' if '/3/' in str(x) else '7')
+data.classes
+```
+
+```python
+labels = [('3' if '/3/' in str(x) else '7') for x in fn_paths]
+labels[:5]
+```
+
+```python
+data = ImageDataBunch.from_lists(path, fn_paths, labels=labels, ds_tfms=tfms, size=24)
+data.classes
+```
+
+```python
+
+```
diff --git a/nbs/dl1/lesson1-pets.py b/nbs/dl1/lesson1-pets.py
new file mode 100644
index 0000000..f0240b2
--- /dev/null
+++ ./nbs/dl1/lesson1-pets.py
@@ -0,0 +1,200 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Lesson 1 - What's your pet
+
+# Welcome to lesson 1! For those of you who are using a Jupyter Notebook for the first time, you can learn about this useful tool in a tutorial we prepared specially for you; click `File`->`Open` now and click `00_notebook_tutorial.ipynb`. 
+#
+# In this lesson we will build our first image classifier from scratch, and see if we can achieve world-class results. Let's dive in!
+#
+# Every notebook starts with the following three lines; they ensure that any edits to libraries you make are reloaded here automatically, and also that any charts or images displayed are shown in this notebook.
+
+# %reload_ext autoreload
+# %autoreload 2
+# %matplotlib inline
+
+# We import all the necessary packages. We are going to work with the [fastai V1 library](http://www.fast.ai/2018/10/02/fastai-ai/) which sits on top of [Pytorch 1.0](https://hackernoon.com/pytorch-1-0-468332ba5163). The fastai library provides many useful functions that enable us to quickly and easily build neural networks and train our models.
+
+from fastai.vision import *
+from fastai.metrics import error_rate
+
+# If you're using a computer with an unusually small GPU, you may get an out of memory error when running this notebook. If this happens, click Kernel->Restart, uncomment the 2nd line below to use a smaller *batch size* (you'll learn all about what this means during the course), and try again.
+
+bs = 64
+# bs = 16   # uncomment this line if you run out of memory even after clicking Kernel->Restart
+
+# ## Looking at the data
+
+# We are going to use the [Oxford-IIIT Pet Dataset](http://www.robots.ox.ac.uk/~vgg/data/pets/) by [O. M. Parkhi et al., 2012](http://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf) which features 12 cat breeds and 25 dogs breeds. Our model will need to learn to differentiate between these 37 distinct categories. According to their paper, the best accuracy they could get in 2012 was 59.21%, using a complex model that was specific to pet detection, with separate "Image", "Head", and "Body" models for the pet photos. Let's see how accurate we can be using deep learning!
+#
+# We are going to use the `untar_data` function to which we must pass a URL as an argument and which will download and extract the data.
+
+help(untar_data)
+
+path = untar_data(URLs.PETS); path
+
+path.ls()
+
+path_anno = path/'annotations'
+path_img = path/'images'
+
+# The first thing we do when we approach a problem is to take a look at the data. We _always_ need to understand very well what the problem is and what the data looks like before we can figure out how to solve it. Taking a look at the data means understanding how the data directories are structured, what the labels are and what some sample images look like.
+#
+# The main difference between the handling of image classification datasets is the way labels are stored. In this particular dataset, labels are stored in the filenames themselves. We will need to extract them to be able to classify the images into the correct categories. Fortunately, the fastai library has a handy function made exactly for this, `ImageDataBunch.from_name_re` gets the labels from the filenames using a [regular expression](https://docs.python.org/3.6/library/re.html).
+
+fnames = get_image_files(path_img)
+fnames[:5]
+
+# Set the random seed to two to guarantee that the same validation set is every time. This will give you consistent results with what you see in the lesson video.
+
+np.random.seed(2)
+pat = r'/([^/]+)_\d+.jpg$'
+
+data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=bs
+                                  ).normalize(imagenet_stats)
+
+data.show_batch(rows=3, figsize=(7,6))
+
+print(data.classes)
+len(data.classes),data.c
+
+# ## Training: resnet34
+
+# Now we will start training our model. We will use a [convolutional neural network](http://cs231n.github.io/convolutional-networks/) backbone and a fully connected head with a single hidden layer as a classifier. Don't know what these things mean? Not to worry, we will dive deeper in the coming lessons. For the moment you need to know that we are building a model which will take images as input and will output the predicted probability for each of the categories (in this case, it will have 37 outputs).
+#
+# We will train for 4 epochs (4 cycles through all our data).
+
+learn = cnn_learner(data, models.resnet34, metrics=error_rate)
+
+learn.model
+
+learn.fit_one_cycle(4)
+
+learn.save('stage-1')
+
+# ## Results
+
+# Let's see what results we have got. 
+#
+# We will first see which were the categories that the model most confused with one another. We will try to see if what the model predicted was reasonable or not. In this case the mistakes look reasonable (none of the mistakes seems obviously naive). This is an indicator that our classifier is working correctly. 
+#
+# Furthermore, when we plot the confusion matrix, we can see that the distribution is heavily skewed: the model makes the same mistakes over and over again but it rarely confuses other categories. This suggests that it just finds it difficult to distinguish some specific categories between each other; this is normal behaviour.
+
+# +
+interp = ClassificationInterpretation.from_learner(learn)
+
+losses,idxs = interp.top_losses()
+
+len(data.valid_ds)==len(losses)==len(idxs)
+# -
+
+interp.plot_top_losses(9, figsize=(15,11))
+
+doc(interp.plot_top_losses)
+
+interp.plot_confusion_matrix(figsize=(12,12), dpi=60)
+
+interp.most_confused(min_val=2)
+
+# ## Unfreezing, fine-tuning, and learning rates
+
+# Since our model is working as we expect it to, we will *unfreeze* our model and train some more.
+
+learn.unfreeze()
+
+learn.fit_one_cycle(1)
+
+learn.load('stage-1');
+
+learn.lr_find()
+
+learn.recorder.plot()
+
+learn.unfreeze()
+learn.fit_one_cycle(2, max_lr=slice(1e-6,1e-4))
+
+# That's a pretty accurate model!
+
+# ## Training: resnet50
+
+# Now we will train in the same way as before but with one caveat: instead of using resnet34 as our backbone we will use resnet50 (resnet34 is a 34 layer residual network while resnet50 has 50 layers. It will be explained later in the course and you can learn the details in the [resnet paper](https://arxiv.org/pdf/1512.03385.pdf)).
+#
+# Basically, resnet50 usually performs better because it is a deeper network with more parameters. Let's see if we can achieve a higher performance here. To help it along, let's use larger images too, since that way the network can see more detail. We reduce the batch size a bit since otherwise this larger network will require more GPU memory.
+
+data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(),
+                                   size=299, bs=bs//2).normalize(imagenet_stats)
+
+learn = cnn_learner(data, models.resnet50, metrics=error_rate)
+
+learn.lr_find()
+learn.recorder.plot()
+
+learn.fit_one_cycle(8)
+
+learn.save('stage-1-50')
+
+# It's astonishing that it's possible to recognize pet breeds so accurately! Let's see if full fine-tuning helps:
+
+learn.unfreeze()
+learn.fit_one_cycle(3, max_lr=slice(1e-6,1e-4))
+
+# If it doesn't, you can always go back to your previous model.
+
+learn.load('stage-1-50');
+
+interp = ClassificationInterpretation.from_learner(learn)
+
+interp.most_confused(min_val=2)
+
+# ## Other data formats
+
+path = untar_data(URLs.MNIST_SAMPLE); path
+
+tfms = get_transforms(do_flip=False)
+data = ImageDataBunch.from_folder(path, ds_tfms=tfms, size=26)
+
+data.show_batch(rows=3, figsize=(5,5))
+
+learn = cnn_learner(data, models.resnet18, metrics=accuracy)
+learn.fit(2)
+
+df = pd.read_csv(path/'labels.csv')
+df.head()
+
+data = ImageDataBunch.from_csv(path, ds_tfms=tfms, size=28)
+
+data.show_batch(rows=3, figsize=(5,5))
+data.classes
+
+data = ImageDataBunch.from_df(path, df, ds_tfms=tfms, size=24)
+data.classes
+
+fn_paths = [path/name for name in df['name']]; fn_paths[:2]
+
+pat = r"/(\d)/\d+\.png$"
+data = ImageDataBunch.from_name_re(path, fn_paths, pat=pat, ds_tfms=tfms, size=24)
+data.classes
+
+data = ImageDataBunch.from_name_func(path, fn_paths, ds_tfms=tfms, size=24,
+        label_func = lambda x: '3' if '/3/' in str(x) else '7')
+data.classes
+
+labels = [('3' if '/3/' in str(x) else '7') for x in fn_paths]
+labels[:5]
+
+data = ImageDataBunch.from_lists(path, fn_paths, labels=labels, ds_tfms=tfms, size=24)
+data.classes
+
+
diff --git a/nbs/dl1/lesson2-download.ipynb b/nbs/dl1/lesson2-download.ipynb
index 5e612b0..7ab109f 100644
--- ./nbs/dl1/lesson2-download.ipynb
+++ ./nbs/dl1/lesson2-download.ipynb
@@ -1403,6 +1403,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl1/lesson2-download.py b/nbs/dl1/lesson2-download.py
new file mode 100644
index 0000000..e8e1e9d
--- /dev/null
+++ ./nbs/dl1/lesson2-download.py
@@ -0,0 +1,312 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# + [markdown] hide_input=false
+# # Creating your own dataset from Google Images
+#
+# *by: Francisco Ingham and Jeremy Howard. Inspired by [Adrian Rosebrock](https://www.pyimagesearch.com/2017/12/04/how-to-create-a-deep-learning-dataset-using-google-images/)*
+
+# + [markdown] hide_input=true
+# In this tutorial we will see how to easily create an image dataset through Google Images. **Note**: You will have to repeat these steps for any new category you want to Google (e.g once for dogs and once for cats).
+
+# + hide_input=false
+from fastai.vision import *
+# -
+
+# ## Get a list of URLs
+
+# ### Search and scroll
+
+# Go to [Google Images](http://images.google.com) and search for the images you are interested in. The more specific you are in your Google Search, the better the results and the less manual pruning you will have to do.
+#
+# Scroll down until you've seen all the images you want to download, or until you see a button that says 'Show more results'. All the images you scrolled past are now available to download. To get more, click on the button, and continue scrolling. The maximum number of images Google Images shows is 700.
+#
+# It is a good idea to put things you want to exclude into the search query, for instance if you are searching for the Eurasian wolf, "canis lupus lupus", it might be a good idea to exclude other variants:
+#
+#     "canis lupus lupus" -dog -arctos -familiaris -baileyi -occidentalis
+#
+# You can also limit your results to show only photos by clicking on Tools and selecting Photos from the Type dropdown.
+
+# ### Download into file
+
+# Now you must run some Javascript code in your browser which will save the URLs of all the images you want for you dataset.
+#
+# In Google Chrome press <kbd>Ctrl</kbd><kbd>Shift</kbd><kbd>j</kbd> on Windows/Linux and <kbd>Cmd</kbd><kbd>Opt</kbd><kbd>j</kbd> on macOS, and a small window the javascript 'Console' will appear. In Firefox press <kbd>Ctrl</kbd><kbd>Shift</kbd><kbd>k</kbd> on Windows/Linux or <kbd>Cmd</kbd><kbd>Opt</kbd><kbd>k</kbd> on macOS. That is where you will paste the JavaScript commands.
+#
+# You will need to get the urls of each of the images. Before running the following commands, you may want to disable ad blocking extensions (uBlock, AdBlockPlus etc.) in Chrome. Otherwise the window.open() command doesn't work. Then you can run the following commands:
+#
+# ```javascript
+# urls=Array.from(document.querySelectorAll('.rg_i')).map(el=> el.hasAttribute('data-src')?el.getAttribute('data-src'):el.getAttribute('data-iurl'));
+# window.open('data:text/csv;charset=utf-8,' + escape(urls.join('\n')));
+# ```
+
+# ### Create directory and upload urls file into your server
+
+# Choose an appropriate name for your labeled images. You can run these steps multiple times to create different labels.
+
+folder = 'black'
+file = 'urls_black.csv'
+
+folder = 'teddys'
+file = 'urls_teddys.csv'
+
+folder = 'grizzly'
+file = 'urls_grizzly.csv'
+
+# You will need to run this cell once per each category.
+
+path = Path('data/bears')
+dest = path/folder
+dest.mkdir(parents=True, exist_ok=True)
+
+path.ls()
+
+# Finally, upload your urls file. You just need to press 'Upload' in your working directory and select your file, then click 'Upload' for each of the displayed files.
+#
+# ![uploaded file](images/download_images/upload.png)
+
+# ## Download images
+
+# Now you will need to download your images from their respective urls.
+#
+# fast.ai has a function that allows you to do just that. You just have to specify the urls filename as well as the destination folder and this function will download and save all images that can be opened. If they have some problem in being opened, they will not be saved.
+#
+# Let's download our images! Notice you can choose a maximum number of images to be downloaded. In this case we will not download all the urls.
+#
+# You will need to run this line once for every category.
+
+classes = ['teddys','grizzly','black']
+
+download_images(path/file, dest, max_pics=200)
+
+# If you have problems download, try with `max_workers=0` to see exceptions:
+download_images(path/file, dest, max_pics=20, max_workers=0)
+
+# Then we can remove any images that can't be opened:
+
+for c in classes:
+    print(c)
+    verify_images(path/c, delete=True, max_size=500)
+
+# ## View data
+
+np.random.seed(42)
+data = ImageDataBunch.from_folder(path, train=".", valid_pct=0.2,
+        ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)
+
+# +
+# If you already cleaned your data, run this cell instead of the one before
+# np.random.seed(42)
+# data = ImageDataBunch.from_csv(path, folder=".", valid_pct=0.2, csv_labels='cleaned.csv',
+#         ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)
+# -
+
+# Good! Let's take a look at some of our pictures then.
+
+data.classes
+
+data.show_batch(rows=3, figsize=(7,8))
+
+data.classes, data.c, len(data.train_ds), len(data.valid_ds)
+
+# ## Train model
+
+learn = cnn_learner(data, models.resnet34, metrics=error_rate)
+
+learn.fit_one_cycle(4)
+
+learn.save('stage-1')
+
+learn.unfreeze()
+
+learn.lr_find()
+
+# If the plot is not showing try to give a start and end learning rate
+# learn.lr_find(start_lr=1e-5, end_lr=1e-1)
+learn.recorder.plot()
+
+learn.fit_one_cycle(2, max_lr=slice(3e-5,3e-4))
+
+learn.save('stage-2')
+
+# ## Interpretation
+
+learn.load('stage-2');
+
+interp = ClassificationInterpretation.from_learner(learn)
+
+interp.plot_confusion_matrix()
+
+# ## Cleaning Up
+#
+# Some of our top losses aren't due to bad performance by our model. There are images in our data set that shouldn't be.
+#
+# Using the `ImageCleaner` widget from `fastai.widgets` we can prune our top losses, removing photos that don't belong.
+
+from fastai.widgets import *
+
+# First we need to get the file paths from our top_losses. We can do this with `.from_toplosses`. We then feed the top losses indexes and corresponding dataset to `ImageCleaner`.
+#
+# Notice that the widget will not delete images directly from disk but it will create a new csv file `cleaned.csv` from where you can create a new ImageDataBunch with the corrected labels to continue training your model.
+
+# In order to clean the entire set of images, we need to create a new dataset without the split. The video lecture demostrated the use of the `ds_type` param which no longer has any effect. See [the thread](https://forums.fast.ai/t/duplicate-widget/30975/10) for more details.
+
+db = (ImageList.from_folder(path)
+                   .split_none()
+                   .label_from_folder()
+                   .transform(get_transforms(), size=224)
+                   .databunch()
+     )
+
+# +
+# If you already cleaned your data using indexes from `from_toplosses`,
+# run this cell instead of the one before to proceed with removing duplicates.
+# Otherwise all the results of the previous step would be overwritten by
+# the new run of `ImageCleaner`.
+
+# db = (ImageList.from_csv(path, 'cleaned.csv', folder='.')
+#                    .split_none()
+#                    .label_from_df()
+#                    .transform(get_transforms(), size=224)
+#                    .databunch()
+#      )
+# -
+
+# Then we create a new learner to use our new databunch with all the images.
+
+# +
+learn_cln = cnn_learner(db, models.resnet34, metrics=error_rate)
+
+learn_cln.load('stage-2');
+# -
+
+ds, idxs = DatasetFormatter().from_toplosses(learn_cln)
+
+# Make sure you're running this notebook in Jupyter Notebook, not Jupyter Lab. That is accessible via [/tree](/tree), not [/lab](/lab). Running the `ImageCleaner` widget in Jupyter Lab is [not currently supported](https://github.com/fastai/fastai/issues/1539).
+
+# Don't run this in google colab or any other instances running jupyter lab.
+# If you do run this on Jupyter Lab, you need to restart your runtime and
+# runtime state including all local variables will be lost.
+ImageCleaner(ds, idxs, path)
+
+#
+# If the code above does not show any GUI(contains images and buttons) rendered by widgets but only text output, that may caused by the configuration problem of ipywidgets. Try the solution in this [link](https://github.com/fastai/fastai/issues/1539#issuecomment-505999861) to solve it.
+#
+
+# Flag photos for deletion by clicking 'Delete'. Then click 'Next Batch' to delete flagged photos and keep the rest in that row. `ImageCleaner` will show you a new row of images until there are no more to show. In this case, the widget will show you images until there are none left from `top_losses.ImageCleaner(ds, idxs)`
+
+# You can also find duplicates in your dataset and delete them! To do this, you need to run `.from_similars` to get the potential duplicates' ids and then run `ImageCleaner` with `duplicates=True`. The API works in a similar way as with misclassified images: just choose the ones you want to delete and click 'Next Batch' until there are no more images left.
+
+# Make sure to recreate the databunch and `learn_cln` from the `cleaned.csv` file. Otherwise the file would be overwritten from scratch, losing all the results from cleaning the data from toplosses.
+
+ds, idxs = DatasetFormatter().from_similars(learn_cln)
+
+ImageCleaner(ds, idxs, path, duplicates=True)
+
+# Remember to recreate your ImageDataBunch from your `cleaned.csv` to include the changes you made in your data!
+
+# ## Putting your model in production
+
+# First thing first, let's export the content of our `Learner` object for production:
+
+learn.export()
+
+# This will create a file named 'export.pkl' in the directory where we were working that contains everything we need to deploy our model (the model, the weights but also some metadata like the classes or the transforms/normalization used).
+
+# You probably want to use CPU for inference, except at massive scale (and you almost certainly don't need to train in real-time). If you don't have a GPU that happens automatically. You can test your model on CPU like so:
+
+defaults.device = torch.device('cpu')
+
+img = open_image(path/'black'/'00000021.jpg')
+img
+
+# We create our `Learner` in production enviromnent like this, just make sure that `path` contains the file 'export.pkl' from before.
+
+learn = load_learner(path)
+
+pred_class,pred_idx,outputs = learn.predict(img)
+pred_class.obj
+
+# So you might create a route something like this ([thanks](https://github.com/simonw/cougar-or-not) to Simon Willison for the structure of this code):
+#
+# ```python
+# @app.route("/classify-url", methods=["GET"])
+# async def classify_url(request):
+#     bytes = await get_bytes(request.query_params["url"])
+#     img = open_image(BytesIO(bytes))
+#     _,_,losses = learner.predict(img)
+#     return JSONResponse({
+#         "predictions": sorted(
+#             zip(cat_learner.data.classes, map(float, losses)),
+#             key=lambda p: p[1],
+#             reverse=True
+#         )
+#     })
+# ```
+#
+# (This example is for the [Starlette](https://www.starlette.io/) web app toolkit.)
+
+# ## Things that can go wrong
+
+# - Most of the time things will train fine with the defaults
+# - There's not much you really need to tune (despite what you've heard!)
+# - Most likely are
+#   - Learning rate
+#   - Number of epochs
+
+# ### Learning rate (LR) too high
+
+learn = cnn_learner(data, models.resnet34, metrics=error_rate)
+
+learn.fit_one_cycle(1, max_lr=0.5)
+
+# ### Learning rate (LR) too low
+
+learn = cnn_learner(data, models.resnet34, metrics=error_rate)
+
+# Previously we had this result:
+#
+# ```
+# Total time: 00:57
+# epoch  train_loss  valid_loss  error_rate
+# 1      1.030236    0.179226    0.028369    (00:14)
+# 2      0.561508    0.055464    0.014184    (00:13)
+# 3      0.396103    0.053801    0.014184    (00:13)
+# 4      0.316883    0.050197    0.021277    (00:15)
+# ```
+
+learn.fit_one_cycle(5, max_lr=1e-5)
+
+learn.recorder.plot_losses()
+
+# As well as taking a really long time, it's getting too many looks at each image, so may overfit.
+
+# ### Too few epochs
+
+learn = cnn_learner(data, models.resnet34, metrics=error_rate, pretrained=False)
+
+learn.fit_one_cycle(1)
+
+# ### Too many epochs
+
+np.random.seed(42)
+data = ImageDataBunch.from_folder(path, train=".", valid_pct=0.9, bs=32, 
+        ds_tfms=get_transforms(do_flip=False, max_rotate=0, max_zoom=1, max_lighting=0, max_warp=0
+                              ),size=224, num_workers=4).normalize(imagenet_stats)
+
+learn = cnn_learner(data, models.resnet50, metrics=error_rate, ps=0, wd=0)
+learn.unfreeze()
+
+learn.fit_one_cycle(40, slice(1e-6,1e-4))
diff --git a/nbs/dl1/lesson2-sgd.ipynb b/nbs/dl1/lesson2-sgd.ipynb
index bfd48f5..7eeda76 100644
--- ./nbs/dl1/lesson2-sgd.ipynb
+++ ./nbs/dl1/lesson2-sgd.ipynb
@@ -18648,6 +18648,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl1/lesson2-sgd.py b/nbs/dl1/lesson2-sgd.py
new file mode 100644
index 0000000..f6ebda2
--- /dev/null
+++ ./nbs/dl1/lesson2-sgd.py
@@ -0,0 +1,115 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# %matplotlib inline
+from fastai.basics import *
+
+# In this part of the lecture we explain Stochastic Gradient Descent (SGD) which is an **optimization** method commonly used in neural networks. We will illustrate the concepts with concrete examples.
+
+# #  Linear Regression problem
+
+# The goal of linear regression is to fit a line to a set of points.
+
+n=100
+
+x = torch.ones(n,2) 
+x[:,0].uniform_(-1.,1)
+x[:5]
+
+a = tensor(3.,2); a
+
+y = x@a + 0.25*torch.randn(n)
+
+plt.scatter(x[:,0], y);
+
+
+# You want to find **parameters** (weights) `a` such that you minimize the *error* between the points and the line `x@a`. Note that here `a` is unknown. For a regression problem the most common *error function* or *loss function* is the **mean squared error**. 
+
+def mse(y_hat, y): return ((y_hat-y)**2).mean()
+
+
+# Suppose we believe `a = (-1.0,1.0)` then we can compute `y_hat` which is our *prediction* and then compute our error.
+
+a = tensor(-1.,1)
+
+y_hat = x@a
+mse(y_hat, y)
+
+plt.scatter(x[:,0],y)
+plt.scatter(x[:,0],y_hat);
+
+# So far we have specified the *model* (linear regression) and the *evaluation criteria* (or *loss function*). Now we need to handle *optimization*; that is, how do we find the best values for `a`? How do we find the best *fitting* linear regression.
+
+# # Gradient Descent
+
+# We would like to find the values of `a` that minimize `mse_loss`.
+#
+# **Gradient descent** is an algorithm that minimizes functions. Given a function defined by a set of parameters, gradient descent starts with an initial set of parameter values and iteratively moves toward a set of parameter values that minimize the function. This iterative minimization is achieved by taking steps in the negative direction of the function gradient.
+#
+# Here is gradient descent implemented in [PyTorch](http://pytorch.org/).
+
+a = nn.Parameter(a); a
+
+
+def update():
+    y_hat = x@a
+    loss = mse(y, y_hat)
+    if t % 10 == 0: print(loss)
+    loss.backward()
+    with torch.no_grad():
+        a.sub_(lr * a.grad)
+        a.grad.zero_()
+
+
+lr = 1e-1
+for t in range(100): update()
+
+plt.scatter(x[:,0],y)
+plt.scatter(x[:,0],x@a.detach());
+
+# ## Animate it!
+
+from matplotlib import animation, rc
+rc('animation', html='jshtml')
+
+# +
+a = nn.Parameter(tensor(-1.,1))
+
+fig = plt.figure()
+plt.scatter(x[:,0], y, c='orange')
+line, = plt.plot(x[:,0], x@a.detach())
+plt.close()
+
+def animate(i):
+    update()
+    line.set_ydata(x@a.detach())
+    return line,
+
+animation.FuncAnimation(fig, animate, np.arange(0, 100), interval=20)
+# -
+
+# In practice, we don't calculate on the whole file at once, but we use *mini-batches*.
+
+# ## Vocab
+
+# - Learning rate
+# - Epoch
+# - Minibatch
+# - SGD
+# - Model / Architecture
+# - Parameters
+# - Loss function
+#
+# For classification problems, we use *cross entropy loss*, also known as *negative log likelihood loss*. This penalizes incorrect confident predictions, and correct unconfident predictions.
diff --git a/nbs/dl1/lesson3-camvid-tiramisu.ipynb b/nbs/dl1/lesson3-camvid-tiramisu.ipynb
index 174121a..c3dfb1d 100644
--- ./nbs/dl1/lesson3-camvid-tiramisu.ipynb
+++ ./nbs/dl1/lesson3-camvid-tiramisu.ipynb
@@ -1300,6 +1300,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl1/lesson3-camvid-tiramisu.py b/nbs/dl1/lesson3-camvid-tiramisu.py
new file mode 100644
index 0000000..c94509c
--- /dev/null
+++ ./nbs/dl1/lesson3-camvid-tiramisu.py
@@ -0,0 +1,159 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# ## Image segmentation with CamVid
+
+# %reload_ext autoreload
+# %autoreload 2
+# %matplotlib inline
+
+from fastai import *
+from fastai.vision import *
+from fastai.callbacks.hooks import *
+
+# The One Hundred Layer Tiramisu paper used a modified version of Camvid, with smaller images and few classes. You can get it from the CamVid directory of this repo:
+#
+#     git clone https://github.com/alexgkendall/SegNet-Tutorial.git
+
+path = Path('./data/camvid-tiramisu')
+
+path.ls()
+
+# ## Data
+
+fnames = get_image_files(path/'val')
+fnames[:3]
+
+lbl_names = get_image_files(path/'valannot')
+lbl_names[:3]
+
+img_f = fnames[0]
+img = open_image(img_f)
+img.show(figsize=(5,5))
+
+
+# +
+def get_y_fn(x): return Path(str(x.parent)+'annot')/x.name
+
+codes = array(['Sky', 'Building', 'Pole', 'Road', 'Sidewalk', 'Tree',
+    'Sign', 'Fence', 'Car', 'Pedestrian', 'Cyclist', 'Void'])
+# -
+
+mask = open_mask(get_y_fn(img_f))
+mask.show(figsize=(5,5), alpha=1)
+
+src_size = np.array(mask.shape[1:])
+src_size,mask.data
+
+# ## Datasets
+
+bs = 8
+
+src = (SegmentationItemList.from_folder(path)
+       .split_by_folder(valid='val')
+       .label_from_func(get_y_fn, classes=codes))
+
+data = (src.transform(get_transforms(), tfm_y=True)
+        .databunch(bs=bs)
+        .normalize(imagenet_stats))
+
+data.show_batch(2, figsize=(10,7))
+
+# ## Model
+
+# +
+name2id = {v:k for k,v in enumerate(codes)}
+void_code = name2id['Void']
+
+def acc_camvid(input, target):
+    target = target.squeeze(1)
+    mask = target != void_code
+    return (input.argmax(dim=1)[mask]==target[mask]).float().mean()
+
+
+# -
+
+metrics=acc_camvid
+wd=1e-2
+
+learn = unet_learner(data, models.resnet34, metrics=metrics, wd=wd, bottle=True)
+
+lr_find(learn)
+learn.recorder.plot()
+
+lr=2e-3
+
+learn.fit_one_cycle(10, slice(lr), pct_start=0.8)
+
+learn.save('stage-1')
+
+learn.load('stage-1');
+
+learn.unfreeze()
+
+lrs = slice(lr/100,lr)
+
+learn.fit_one_cycle(12, lrs, pct_start=0.8)
+
+learn.save('stage-2');
+
+# ## Go big
+
+learn=None
+gc.collect()
+
+# You may have to restart your kernel and come back to this stage if you run out of memory, and may also need to decrease `bs`.
+
+size = src_size
+bs=8
+
+data = (src.transform(get_transforms(), size=size, tfm_y=True)
+        .databunch(bs=bs)
+        .normalize(imagenet_stats))
+
+learn = unet_learner(data, models.resnet34, metrics=metrics, wd=wd, bottle=True).load('stage-2');
+
+lr_find(learn)
+learn.recorder.plot()
+
+lr=1e-3
+
+learn.fit_one_cycle(10, slice(lr), pct_start=0.8)
+
+learn.save('stage-1-big')
+
+learn.load('stage-1-big');
+
+learn.unfreeze()
+
+lrs = slice(lr/1000,lr/10)
+
+learn.fit_one_cycle(10, lrs)
+
+learn.save('stage-2-big')
+
+learn.load('stage-2-big');
+
+learn.show_results(rows=3, figsize=(9,11))
+
+# ## fin
+
+# +
+# start: 480x360
+# -
+
+print(learn.summary())
+
+
diff --git a/nbs/dl1/lesson3-camvid.ipynb b/nbs/dl1/lesson3-camvid.ipynb
index 261f54e..09c344c 100644
--- ./nbs/dl1/lesson3-camvid.ipynb
+++ ./nbs/dl1/lesson3-camvid.ipynb
@@ -1062,6 +1062,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl1/lesson3-camvid.py b/nbs/dl1/lesson3-camvid.py
new file mode 100644
index 0000000..ee14d1f
--- /dev/null
+++ ./nbs/dl1/lesson3-camvid.py
@@ -0,0 +1,191 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# ## Image segmentation with CamVid
+
+# %reload_ext autoreload
+# %autoreload 2
+# %matplotlib inline
+
+from fastai.vision import *
+from fastai.callbacks.hooks import *
+from fastai.utils.mem import *
+
+path = untar_data(URLs.CAMVID)
+path.ls()
+
+path_lbl = path/'labels'
+path_img = path/'images'
+
+# ## Subset classes
+
+# +
+# path = Path('./data/camvid-small')
+
+# def get_y_fn(x): return Path(str(x.parent)+'annot')/x.name
+
+# codes = array(['Sky', 'Building', 'Pole', 'Road', 'Sidewalk', 'Tree',
+#     'Sign', 'Fence', 'Car', 'Pedestrian', 'Cyclist', 'Void'])
+
+# src = (SegmentationItemList.from_folder(path)
+#        .split_by_folder(valid='val')
+#        .label_from_func(get_y_fn, classes=codes))
+
+# bs=8
+# data = (src.transform(get_transforms(), tfm_y=True)
+#         .databunch(bs=bs)
+#         .normalize(imagenet_stats))
+# -
+
+# ## Data
+
+fnames = get_image_files(path_img)
+fnames[:3]
+
+lbl_names = get_image_files(path_lbl)
+lbl_names[:3]
+
+img_f = fnames[0]
+img = open_image(img_f)
+img.show(figsize=(5,5))
+
+get_y_fn = lambda x: path_lbl/f'{x.stem}_P{x.suffix}'
+
+mask = open_mask(get_y_fn(img_f))
+mask.show(figsize=(5,5), alpha=1)
+
+src_size = np.array(mask.shape[1:])
+src_size,mask.data
+
+codes = np.loadtxt(path/'codes.txt', dtype=str); codes
+
+# ## Datasets
+
+# +
+size = src_size//2
+
+free = gpu_mem_get_free_no_cache()
+# the max size of bs depends on the available GPU RAM
+if free > 8200: bs=8
+else:           bs=4
+print(f"using bs={bs}, have {free}MB of GPU RAM free")
+# -
+
+src = (SegmentationItemList.from_folder(path_img)
+       .split_by_fname_file('../valid.txt')
+       .label_from_func(get_y_fn, classes=codes))
+
+data = (src.transform(get_transforms(), size=size, tfm_y=True)
+        .databunch(bs=bs)
+        .normalize(imagenet_stats))
+
+data.show_batch(2, figsize=(10,7))
+
+data.show_batch(2, figsize=(10,7), ds_type=DatasetType.Valid)
+
+# ## Model
+
+# +
+name2id = {v:k for k,v in enumerate(codes)}
+void_code = name2id['Void']
+
+def acc_camvid(input, target):
+    target = target.squeeze(1)
+    mask = target != void_code
+    return (input.argmax(dim=1)[mask]==target[mask]).float().mean()
+
+
+# -
+
+metrics=acc_camvid
+# metrics=accuracy
+
+wd=1e-2
+
+learn = unet_learner(data, models.resnet34, metrics=metrics, wd=wd)
+
+lr_find(learn)
+learn.recorder.plot()
+
+lr=3e-3
+
+learn.fit_one_cycle(10, slice(lr), pct_start=0.9)
+
+learn.save('stage-1')
+
+learn.load('stage-1');
+
+learn.show_results(rows=3, figsize=(8,9))
+
+learn.unfreeze()
+
+lrs = slice(lr/400,lr/4)
+
+learn.fit_one_cycle(12, lrs, pct_start=0.8)
+
+learn.save('stage-2');
+
+# ## Go big
+
+# You may have to restart your kernel and come back to this stage if you run out of memory, and may also need to decrease `bs`.
+
+# +
+learn.destroy()
+
+size = src_size
+
+free = gpu_mem_get_free_no_cache()
+# the max size of bs depends on the available GPU RAM
+if free > 8200: bs=3
+else:           bs=1
+print(f"using bs={bs}, have {free}MB of GPU RAM free")
+# -
+
+data = (src.transform(get_transforms(), size=size, tfm_y=True)
+        .databunch(bs=bs)
+        .normalize(imagenet_stats))
+
+learn = unet_learner(data, models.resnet34, metrics=metrics, wd=wd)
+
+learn.load('stage-2');
+
+lr_find(learn)
+learn.recorder.plot()
+
+lr=1e-3
+
+learn.fit_one_cycle(10, slice(lr), pct_start=0.8)
+
+learn.save('stage-1-big')
+
+learn.load('stage-1-big');
+
+learn.unfreeze()
+
+lrs = slice(1e-6,lr/10)
+
+learn.fit_one_cycle(10, lrs)
+
+learn.save('stage-2-big')
+
+learn.load('stage-2-big');
+
+learn.show_results(rows=3, figsize=(10,10))
+
+
+
+# ## fin
+
+
diff --git a/nbs/dl1/lesson3-head-pose.ipynb b/nbs/dl1/lesson3-head-pose.ipynb
index a0b5241..5fb60bd 100644
--- ./nbs/dl1/lesson3-head-pose.ipynb
+++ ./nbs/dl1/lesson3-head-pose.ipynb
@@ -453,6 +453,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl1/lesson3-head-pose.py b/nbs/dl1/lesson3-head-pose.py
new file mode 100644
index 0000000..f3057f0
--- /dev/null
+++ ./nbs/dl1/lesson3-head-pose.py
@@ -0,0 +1,113 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# ## Regression with BIWI head pose dataset
+
+# This is a more advanced example to show how to create custom datasets and do regression with images. Our task is to find the center of the head in each image. The data comes from the [BIWI head pose dataset](https://data.vision.ee.ethz.ch/cvl/gfanelli/head_pose/head_forest.html#db), thanks to Gabriele Fanelli et al. We have converted the images to jpeg format, so you should download the converted dataset from [this link](https://s3.amazonaws.com/fast-ai-imagelocal/biwi_head_pose.tgz).
+
+# %reload_ext autoreload
+# %autoreload 2
+# %matplotlib inline
+
+from fastai.vision import *
+
+# ## Getting and converting the data
+
+path = untar_data(URLs.BIWI_HEAD_POSE)
+
+cal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6); cal
+
+fname = '09/frame_00667_rgb.jpg'
+
+
+def img2txt_name(f): return path/f'{str(f)[:-7]}pose.txt'
+
+
+img = open_image(path/fname)
+img.show()
+
+ctr = np.genfromtxt(img2txt_name(fname), skip_header=3); ctr
+
+
+# +
+def convert_biwi(coords):
+    c1 = coords[0] * cal[0][0]/coords[2] + cal[0][2]
+    c2 = coords[1] * cal[1][1]/coords[2] + cal[1][2]
+    return tensor([c2,c1])
+
+def get_ctr(f):
+    ctr = np.genfromtxt(img2txt_name(f), skip_header=3)
+    return convert_biwi(ctr)
+
+def get_ip(img,pts): return ImagePoints(FlowField(img.size, pts), scale=True)
+
+
+# -
+
+get_ctr(fname)
+
+ctr = get_ctr(fname)
+img.show(y=get_ip(img, ctr), figsize=(6, 6))
+
+# ## Creating a dataset
+
+data = (PointsItemList.from_folder(path)
+        .split_by_valid_func(lambda o: o.parent.name=='13')
+        .label_from_func(get_ctr)
+        .transform(get_transforms(), tfm_y=True, size=(120,160))
+        .databunch().normalize(imagenet_stats)
+       )
+
+data.show_batch(3, figsize=(9,6))
+
+# ## Train model
+
+learn = cnn_learner(data, models.resnet34)
+
+learn.lr_find()
+learn.recorder.plot()
+
+lr = 2e-2
+
+learn.fit_one_cycle(5, slice(lr))
+
+learn.save('stage-1')
+
+learn.load('stage-1');
+
+learn.show_results()
+
+# ## Data augmentation
+
+# +
+tfms = get_transforms(max_rotate=20, max_zoom=1.5, max_lighting=0.5, max_warp=0.4, p_affine=1., p_lighting=1.)
+
+data = (PointsItemList.from_folder(path)
+        .split_by_valid_func(lambda o: o.parent.name=='13')
+        .label_from_func(get_ctr)
+        .transform(tfms, tfm_y=True, size=(120,160))
+        .databunch().normalize(imagenet_stats)
+       )
+
+
+# +
+def _plot(i,j,ax):
+    x,y = data.train_ds[0]
+    x.show(ax, y=y)
+
+plot_multi(_plot, 3, 3, figsize=(8,6))
+# -
+
+
diff --git a/nbs/dl1/lesson3-imdb.ipynb b/nbs/dl1/lesson3-imdb.ipynb
index fdafed7..9aeafcc 100644
--- ./nbs/dl1/lesson3-imdb.ipynb
+++ ./nbs/dl1/lesson3-imdb.ipynb
@@ -1285,6 +1285,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl1/lesson3-imdb.py b/nbs/dl1/lesson3-imdb.py
new file mode 100644
index 0000000..64dc76b
--- /dev/null
+++ ./nbs/dl1/lesson3-imdb.py
@@ -0,0 +1,232 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # IMDB
+
+# %reload_ext autoreload
+# %autoreload 2
+# %matplotlib inline
+
+from fastai.text import *
+
+# ## Preparing the data
+
+# First let's download the dataset we are going to study. The [dataset](http://ai.stanford.edu/~amaas/data/sentiment/) has been curated by Andrew Maas et al. and contains a total of 100,000 reviews on IMDB. 25,000 of them are labelled as positive and negative for training, another 25,000 are labelled for testing (in both cases they are highly polarized). The remaning 50,000 is an additional unlabelled data (but we will find a use for it nonetheless).
+#
+# We'll begin with a sample we've prepared for you, so that things run quickly before going over the full dataset.
+
+path = untar_data(URLs.IMDB_SAMPLE)
+path.ls()
+
+# It only contains one csv file, let's have a look at it.
+
+df = pd.read_csv(path/'texts.csv')
+df.head()
+
+df['text'][1]
+
+# It contains one line per review, with the label ('negative' or 'positive'), the text and a flag to determine if it should be part of the validation set or the training set. If we ignore this flag, we can create a DataBunch containing this data in one line of code:
+
+data_lm = TextDataBunch.from_csv(path, 'texts.csv')
+
+# By executing this line a process was launched that took a bit of time. Let's dig a bit into it. Images could be fed (almost) directly into a model because they're just a big array of pixel values that are floats between 0 and 1. A text is composed of words, and we can't apply mathematical functions to them directly. We first have to convert them to numbers. This is done in two differents steps: tokenization and numericalization. A `TextDataBunch` does all of that behind the scenes for you.
+#
+# Before we delve into the explanations, let's take the time to save the things that were calculated.
+
+data_lm.save()
+
+# Next time we launch this notebook, we can skip the cell above that took a bit of time (and that will take a lot more when you get to the full dataset) and load those results like this:
+
+data = load_data(path)
+
+# ### Tokenization
+
+# The first step of processing we make the texts go through is to split the raw sentences into words, or more exactly tokens. The easiest way to do this would be to split the string on spaces, but we can be smarter:
+#
+# - we need to take care of punctuation
+# - some words are contractions of two different words, like isn't or don't
+# - we may need to clean some parts of our texts, if there's HTML code for instance
+#
+# To see what the tokenizer had done behind the scenes, let's have a look at a few texts in a batch.
+
+data = TextClasDataBunch.from_csv(path, 'texts.csv')
+data.show_batch()
+
+# The texts are truncated at 100 tokens for more readability. We can see that it did more than just split on space and punctuation symbols: 
+# - the "'s" are grouped together in one token
+# - the contractions are separated like this: "did", "n't"
+# - content has been cleaned for any HTML symbol and lower cased
+# - there are several special tokens (all those that begin by xx), to replace unknown tokens (see below) or to introduce different text fields (here we only have one).
+
+# ### Numericalization
+
+# Once we have extracted tokens from our texts, we convert to integers by creating a list of all the words used. We only keep the ones that appear at least twice with a maximum vocabulary size of 60,000 (by default) and replace the ones that don't make the cut by the unknown token `UNK`.
+#
+# The correspondance from ids to tokens is stored in the `vocab` attribute of our datasets, in a dictionary called `itos` (for int to string).
+
+data.vocab.itos[:10]
+
+# And if we look at what a what's in our datasets, we'll see the tokenized text as a representation:
+
+data.train_ds[0][0]
+
+# But the underlying data is all numbers
+
+data.train_ds[0][0].data[:10]
+
+# ### With the data block API
+
+# We can use the data block API with NLP and have a lot more flexibility than what the default factory methods offer. In the previous example for instance, the data was randomly split between train and validation instead of reading the third column of the csv.
+#
+# With the data block API though, we have to manually call the tokenize and numericalize steps. This allows more flexibility, and if you're not using the defaults from fastai, the various arguments to pass will appear in the step they're revelant, so it'll be more readable.
+
+data = (TextList.from_csv(path, 'texts.csv', cols='text')
+                .split_from_df(col=2)
+                .label_from_df(cols=0)
+                .databunch())
+
+# ## Language model
+
+# Note that language models can use a lot of GPU, so you may need to decrease batchsize here.
+
+bs=48
+
+# Now let's grab the full dataset for what follows.
+
+path = untar_data(URLs.IMDB)
+path.ls()
+
+(path/'train').ls()
+
+# The reviews are in a training and test set following an imagenet structure. The only difference is that there is an `unsup` folder on top of `train` and `test` that contains the unlabelled data.
+#
+# We're not going to train a model that classifies the reviews from scratch. Like in computer vision, we'll use a model pretrained on a bigger dataset (a cleaned subset of wikipedia called [wikitext-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)). That model has been trained to guess what the next word is, its input being all the previous words. It has a recurrent structure and a hidden state that is updated each time it sees a new word. This hidden state thus contains information about the sentence up to that point.
+#
+# We are going to use that 'knowledge' of the English language to build our classifier, but first, like for computer vision, we need to fine-tune the pretrained model to our particular dataset. Because the English of the reviews left by people on IMDB isn't the same as the English of wikipedia, we'll need to adjust the parameters of our model by a little bit. Plus there might be some words that would be extremely common in the reviews dataset but would be barely present in wikipedia, and therefore might not be part of the vocabulary the model was trained on.
+
+# This is where the unlabelled data is going to be useful to us, as we can use it to fine-tune our model. Let's create our data object with the data block API (next line takes a few minutes).
+
+data_lm = (TextList.from_folder(path)
+           #Inputs: all the text files in path
+            .filter_by_folder(include=['train', 'test', 'unsup']) 
+           #We may have other temp folders that contain text files so we only keep what's in train and test
+            .split_by_rand_pct(0.1)
+           #We randomly split and keep 10% (10,000 reviews) for validation
+            .label_for_lm()           
+           #We want to do a language model so we label accordingly
+            .databunch(bs=bs))
+data_lm.save('data_lm.pkl')
+
+# We have to use a special kind of `TextDataBunch` for the language model, that ignores the labels (that's why we put 0 everywhere), will shuffle the texts at each epoch before concatenating them all together (only for training, we don't shuffle for the validation set) and will send batches that read that text in order with targets that are the next word in the sentence.
+#
+# The line before being a bit long, we want to load quickly the final ids by using the following cell.
+
+data_lm = load_data(path, 'data_lm.pkl', bs=bs)
+
+data_lm.show_batch()
+
+# We can then put this in a learner object very easily with a model loaded with the pretrained weights. They'll be downloaded the first time you'll execute the following line and stored in `~/.fastai/models/` (or elsewhere if you specified different paths in your config file).
+
+learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)
+
+learn.lr_find()
+
+learn.recorder.plot(skip_end=15)
+
+learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))
+
+learn.save('fit_head')
+
+learn.load('fit_head');
+
+# To complete the fine-tuning, we can then unfeeze and launch a new training.
+
+learn.unfreeze()
+
+learn.fit_one_cycle(10, 1e-3, moms=(0.8,0.7))
+
+learn.save('fine_tuned')
+
+# How good is our model? Well let's try to see what it predicts after a few given words.
+
+learn.load('fine_tuned');
+
+TEXT = "I liked this movie because"
+N_WORDS = 40
+N_SENTENCES = 2
+
+print("\n".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))
+
+# We have to save not only the model, but also its encoder, the part that's responsible for creating and updating the hidden state. For the next part, we don't care about the part that tries to guess the next word.
+
+learn.save_encoder('fine_tuned_enc')
+
+# ## Classifier
+
+# Now, we'll create a new data object that only grabs the labelled data and keeps those labels. Again, this line takes a bit of time.
+
+path = untar_data(URLs.IMDB)
+
+# +
+data_clas = (TextList.from_folder(path, vocab=data_lm.vocab)
+             #grab all the text files in path
+             .split_by_folder(valid='test')
+             #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)
+             .label_from_folder(classes=['neg', 'pos'])
+             #label them all with their folders
+             .databunch(bs=bs))
+
+data_clas.save('data_clas.pkl')
+# -
+
+data_clas = load_data(path, 'data_clas.pkl', bs=bs)
+
+data_clas.show_batch()
+
+# We can then create a model to classify those reviews and load the encoder we saved before.
+
+learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)
+learn.load_encoder('fine_tuned_enc')
+
+learn.lr_find()
+
+learn.recorder.plot()
+
+learn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7))
+
+learn.save('first')
+
+learn.load('first');
+
+learn.freeze_to(-2)
+learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))
+
+learn.save('second')
+
+learn.load('second');
+
+learn.freeze_to(-3)
+learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))
+
+learn.save('third')
+
+learn.load('third');
+
+learn.unfreeze()
+learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))
+
+learn.predict("I really loved that movie, it was awesome!")
+
+
diff --git a/nbs/dl1/lesson3-planet.ipynb b/nbs/dl1/lesson3-planet.ipynb
index d669923..1977526 100644
--- ./nbs/dl1/lesson3-planet.ipynb
+++ ./nbs/dl1/lesson3-planet.ipynb
@@ -1019,6 +1019,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl1/lesson3-planet.py b/nbs/dl1/lesson3-planet.py
new file mode 100644
index 0000000..2408f5e
--- /dev/null
+++ ./nbs/dl1/lesson3-planet.py
@@ -0,0 +1,192 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# ## Multi-label prediction with Planet Amazon dataset
+
+# %reload_ext autoreload
+# %autoreload 2
+# %matplotlib inline
+
+from fastai.vision import *
+
+# ## Getting the data
+
+# The planet dataset isn't available on the [fastai dataset page](https://course.fast.ai/datasets) due to copyright restrictions. You can download it from Kaggle however. Let's see how to do this by using the [Kaggle API](https://github.com/Kaggle/kaggle-api) as it's going to be pretty useful to you if you want to join a competition or use other Kaggle datasets later on.
+#
+# First, install the Kaggle API by uncommenting the following line and executing it, or by executing it in your terminal (depending on your platform you may need to modify this slightly to either add `source activate fastai` or similar, or prefix `pip` with a path. Have a look at how `conda install` is called for your platform in the appropriate *Returning to work* section of https://course.fast.ai/. (Depending on your environment, you may also need to append "--user" to the command.)
+
+# +
+# ! {sys.executable} -m pip install kaggle --upgrade
+# -
+
+# Then you need to upload your credentials from Kaggle on your instance. Login to kaggle and click on your profile picture on the top left corner, then 'My account'. Scroll down until you find a button named 'Create New API Token' and click on it. This will trigger the download of a file named 'kaggle.json'.
+#
+# Upload this file to the directory this notebook is running in, by clicking "Upload" on your main Jupyter page, then uncomment and execute the next two commands (or run them in a terminal). For Windows, uncomment the last two commands.
+
+# +
+# # ! mkdir -p ~/.kaggle/
+# # ! mv kaggle.json ~/.kaggle/
+
+# For Windows, uncomment these two commands
+# # ! mkdir %userprofile%\.kaggle
+# # ! move kaggle.json %userprofile%\.kaggle
+# -
+
+# You're all set to download the data from [planet competition](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space). You **first need to go to its main page and accept its rules**, and run the two cells below (uncomment the shell commands to download and unzip the data). If you get a `403 forbidden` error it means you haven't accepted the competition rules yet (you have to go to the competition page, click on *Rules* tab, and then scroll to the bottom to find the *accept* button).
+
+path = Config.data_path()/'planet'
+path.mkdir(parents=True, exist_ok=True)
+path
+
+# +
+# # ! kaggle competitions download -c planet-understanding-the-amazon-from-space -f train-jpg.tar.7z -p {path}  
+# # ! kaggle competitions download -c planet-understanding-the-amazon-from-space -f train_v2.csv -p {path}  
+# # ! unzip -q -n {path}/train_v2.csv.zip -d {path}
+# -
+
+# To extract the content of this file, we'll need 7zip, so uncomment the following line if you need to install it (or run `sudo apt install p7zip-full` in your terminal).
+
+# +
+# # ! conda install --yes --prefix {sys.prefix} -c haasad eidl7zip
+# -
+
+# And now we can unpack the data (uncomment to run - this might take a few minutes to complete).
+
+# +
+# ! 7za -bd -y -so x {path}/train-jpg.tar.7z | tar xf - -C {path.as_posix()}
+# -
+
+# ## Multiclassification
+
+# Contrary to the pets dataset studied in last lesson, here each picture can have multiple labels. If we take a look at the csv file containing the labels (in 'train_v2.csv' here) we see that each 'image_name' is associated to several tags separated by spaces.
+
+df = pd.read_csv(path/'train_v2.csv')
+df.head()
+
+# To put this in a `DataBunch` while using the [data block API](https://docs.fast.ai/data_block.html), we then need to using `ImageList` (and not `ImageDataBunch`). This will make sure the model created has the proper loss function to deal with the multiple classes.
+
+tfms = get_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0.)
+
+# We use parentheses around the data block pipeline below, so that we can use a multiline statement without needing to add '\\'.
+
+np.random.seed(42)
+src = (ImageList.from_csv(path, 'train_v2.csv', folder='train-jpg', suffix='.jpg')
+       .split_by_rand_pct(0.2)
+       .label_from_df(label_delim=' '))
+
+data = (src.transform(tfms, size=128)
+        .databunch().normalize(imagenet_stats))
+
+# `show_batch` still works, and show us the different labels separated by `;`.
+
+data.show_batch(rows=3, figsize=(12,9))
+
+# To create a `Learner` we use the same function as in lesson 1. Our base architecture is resnet50 again, but the metrics are a little bit differeent: we use `accuracy_thresh` instead of `accuracy`. In lesson 1, we determined the predicition for a given class by picking the final activation that was the biggest, but here, each activation can be 0. or 1. `accuracy_thresh` selects the ones that are above a certain threshold (0.5 by default) and compares them to the ground truth.
+#
+# As for Fbeta, it's the metric that was used by Kaggle on this competition. See [here](https://en.wikipedia.org/wiki/F1_score) for more details.
+
+arch = models.resnet50
+
+acc_02 = partial(accuracy_thresh, thresh=0.2)
+f_score = partial(fbeta, thresh=0.2)
+learn = cnn_learner(data, arch, metrics=[acc_02, f_score])
+
+# We use the LR Finder to pick a good learning rate.
+
+learn.lr_find()
+
+learn.recorder.plot()
+
+# Then we can fit the head of our network.
+
+lr = 0.01
+
+learn.fit_one_cycle(5, slice(lr))
+
+learn.save('stage-1-rn50')
+
+# ...And fine-tune the whole model:
+
+learn.unfreeze()
+
+learn.lr_find()
+learn.recorder.plot()
+
+learn.fit_one_cycle(5, slice(1e-5, lr/5))
+
+learn.save('stage-2-rn50')
+
+# +
+data = (src.transform(tfms, size=256)
+        .databunch().normalize(imagenet_stats))
+
+learn.data = data
+data.train_ds[0][0].shape
+# -
+
+learn.freeze()
+
+learn.lr_find()
+learn.recorder.plot()
+
+lr=1e-2/2
+
+learn.fit_one_cycle(5, slice(lr))
+
+learn.save('stage-1-256-rn50')
+
+learn.unfreeze()
+
+learn.fit_one_cycle(5, slice(1e-5, lr/5))
+
+learn.recorder.plot_losses()
+
+learn.save('stage-2-256-rn50')
+
+# You won't really know how you're going until you submit to Kaggle, since the leaderboard isn't using the same subset as we have for training. But as a guide, 50th place (out of 938 teams) on the private leaderboard was a score of `0.930`.
+
+learn.export()
+
+# ## fin
+
+# (This section will be covered in part 2 - please don't ask about it just yet! :) )
+
+# +
+# #! kaggle competitions download -c planet-understanding-the-amazon-from-space -f test-jpg.tar.7z -p {path}  
+#! 7za -bd -y -so x {path}/test-jpg.tar.7z | tar xf - -C {path}
+# #! kaggle competitions download -c planet-understanding-the-amazon-from-space -f test-jpg-additional.tar.7z -p {path}  
+#! 7za -bd -y -so x {path}/test-jpg-additional.tar.7z | tar xf - -C {path}
+# -
+
+test = ImageList.from_folder(path/'test-jpg').add(ImageList.from_folder(path/'test-jpg-additional'))
+len(test)
+
+learn = load_learner(path, test=test)
+preds, _ = learn.get_preds(ds_type=DatasetType.Test)
+
+thresh = 0.2
+labelled_preds = [' '.join([learn.data.classes[i] for i,p in enumerate(pred) if p > thresh]) for pred in preds]
+
+labelled_preds[:5]
+
+fnames = [f.name[:-4] for f in learn.data.test_ds.items]
+
+df = pd.DataFrame({'image_name':fnames, 'tags':labelled_preds}, columns=['image_name', 'tags'])
+
+df.to_csv(path/'submission.csv', index=False)
+
+# ! kaggle competitions submit planet-understanding-the-amazon-from-space -f {path/'submission.csv'} -m "My submission"
+
+# Private Leaderboard score: 0.9296 (around 80th)
diff --git a/nbs/dl1/lesson4-collab.ipynb b/nbs/dl1/lesson4-collab.ipynb
index 516dd68..755c60d 100644
--- ./nbs/dl1/lesson4-collab.ipynb
+++ ./nbs/dl1/lesson4-collab.ipynb
@@ -1257,6 +1257,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl1/lesson4-collab.py b/nbs/dl1/lesson4-collab.py
new file mode 100644
index 0000000..6a3fee9
--- /dev/null
+++ ./nbs/dl1/lesson4-collab.py
@@ -0,0 +1,134 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+from fastai.collab import *
+from fastai.tabular import *
+
+# ## Collaborative filtering example
+
+# `collab` models use data in a `DataFrame` of user, items, and ratings.
+
+user,item,title = 'userId','movieId','title'
+
+path = untar_data(URLs.ML_SAMPLE)
+path
+
+ratings = pd.read_csv(path/'ratings.csv')
+ratings.head()
+
+# That's all we need to create and train a model:
+
+data = CollabDataBunch.from_df(ratings, seed=42)
+
+y_range = [0,5.5]
+
+learn = collab_learner(data, n_factors=50, y_range=y_range)
+
+learn.fit_one_cycle(3, 5e-3)
+
+# ## Movielens 100k
+
+# Let's try with the full Movielens 100k data dataset, available from http://files.grouplens.org/datasets/movielens/ml-100k.zip
+
+path=Config.data_path()/'ml-100k'
+
+ratings = pd.read_csv(path/'u.data', delimiter='\t', header=None,
+                      names=[user,item,'rating','timestamp'])
+ratings.head()
+
+movies = pd.read_csv(path/'u.item',  delimiter='|', encoding='latin-1', header=None,
+                    names=[item, 'title', 'date', 'N', 'url', *[f'g{i}' for i in range(19)]])
+movies.head()
+
+len(ratings)
+
+rating_movie = ratings.merge(movies[[item, title]])
+rating_movie.head()
+
+data = CollabDataBunch.from_df(rating_movie, seed=42, valid_pct=0.1, item_name=title)
+
+data.show_batch()
+
+y_range = [0,5.5]
+
+learn = collab_learner(data, n_factors=40, y_range=y_range, wd=1e-1)
+
+learn.lr_find()
+learn.recorder.plot(skip_end=15)
+
+learn.fit_one_cycle(5, 5e-3)
+
+learn.save('dotprod')
+
+# Here's [some benchmarks](https://www.librec.net/release/v1.3/example.html) on the same dataset for the popular Librec system for collaborative filtering. They show best results based on RMSE of 0.91, which corresponds to an MSE of `0.91**2 = 0.83`.
+
+# ## Interpretation
+
+# ### Setup
+
+learn.load('dotprod');
+
+learn.model
+
+g = rating_movie.groupby(title)['rating'].count()
+top_movies = g.sort_values(ascending=False).index.values[:1000]
+top_movies[:10]
+
+# ### Movie bias
+
+movie_bias = learn.bias(top_movies, is_item=True)
+movie_bias.shape
+
+mean_ratings = rating_movie.groupby(title)['rating'].mean()
+movie_ratings = [(b, i, mean_ratings.loc[i]) for i,b in zip(top_movies,movie_bias)]
+
+item0 = lambda o:o[0]
+
+sorted(movie_ratings, key=item0)[:15]
+
+sorted(movie_ratings, key=lambda o: o[0], reverse=True)[:15]
+
+# ### Movie weights
+
+movie_w = learn.weight(top_movies, is_item=True)
+movie_w.shape
+
+movie_pca = movie_w.pca(3)
+movie_pca.shape
+
+fac0,fac1,fac2 = movie_pca.t()
+movie_comp = [(f, i) for f,i in zip(fac0, top_movies)]
+
+sorted(movie_comp, key=itemgetter(0), reverse=True)[:10]
+
+sorted(movie_comp, key=itemgetter(0))[:10]
+
+movie_comp = [(f, i) for f,i in zip(fac1, top_movies)]
+
+sorted(movie_comp, key=itemgetter(0), reverse=True)[:10]
+
+sorted(movie_comp, key=itemgetter(0))[:10]
+
+idxs = np.random.choice(len(top_movies), 50, replace=False)
+idxs = list(range(50))
+X = fac0[idxs]
+Y = fac2[idxs]
+plt.figure(figsize=(15,15))
+plt.scatter(X, Y)
+for i, x, y in zip(top_movies[idxs], X, Y):
+    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)
+plt.show()
+
+
diff --git a/nbs/dl1/lesson4-tabular.ipynb b/nbs/dl1/lesson4-tabular.ipynb
index 59484e1..7eb3e28 100644
--- ./nbs/dl1/lesson4-tabular.ipynb
+++ ./nbs/dl1/lesson4-tabular.ipynb
@@ -321,6 +321,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl1/lesson4-tabular.py b/nbs/dl1/lesson4-tabular.py
new file mode 100644
index 0000000..ef7814a
--- /dev/null
+++ ./nbs/dl1/lesson4-tabular.py
@@ -0,0 +1,50 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Tabular models
+
+from fastai.tabular import *
+
+# Tabular data should be in a Pandas `DataFrame`.
+
+path = untar_data(URLs.ADULT_SAMPLE)
+df = pd.read_csv(path/'adult.csv')
+
+dep_var = 'salary'
+cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']
+cont_names = ['age', 'fnlwgt', 'education-num']
+procs = [FillMissing, Categorify, Normalize]
+
+test = TabularList.from_df(df.iloc[800:1000].copy(), path=path, cat_names=cat_names, cont_names=cont_names)
+
+data = (TabularList.from_df(df, path=path, cat_names=cat_names, cont_names=cont_names, procs=procs)
+                           .split_by_idx(list(range(800,1000)))
+                           .label_from_df(cols=dep_var)
+                           .add_test(test)
+                           .databunch())
+
+data.show_batch(rows=10)
+
+learn = tabular_learner(data, layers=[200,100], metrics=accuracy)
+
+learn.fit(1, 1e-2)
+
+# ## Inference
+
+row = df.iloc[0]
+
+learn.predict(row)
+
+
diff --git a/nbs/dl1/lesson5-sgd-mnist.ipynb b/nbs/dl1/lesson5-sgd-mnist.ipynb
index 7fb5d6a..4193729 100644
--- ./nbs/dl1/lesson5-sgd-mnist.ipynb
+++ ./nbs/dl1/lesson5-sgd-mnist.ipynb
@@ -651,6 +651,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl1/lesson5-sgd-mnist.py b/nbs/dl1/lesson5-sgd-mnist.py
new file mode 100644
index 0000000..61e6c6e
--- /dev/null
+++ ./nbs/dl1/lesson5-sgd-mnist.py
@@ -0,0 +1,150 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# %matplotlib inline
+from fastai.basics import *
+
+# ## MNIST SGD
+
+# Get the 'pickled' MNIST dataset from http://deeplearning.net/data/mnist/mnist.pkl.gz. We're going to treat it as a standard flat dataset with fully connected layers, rather than using a CNN.
+
+path = Config().data_path()/'mnist'
+
+path.ls()
+
+with gzip.open(path/'mnist.pkl.gz', 'rb') as f:
+    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')
+
+plt.imshow(x_train[0].reshape((28,28)), cmap="gray")
+x_train.shape
+
+x_train,y_train,x_valid,y_valid = map(torch.tensor, (x_train,y_train,x_valid,y_valid))
+n,c = x_train.shape
+x_train.shape, y_train.min(), y_train.max()
+
+# In lesson2-sgd we did these things ourselves:
+#
+# ```python
+# x = torch.ones(n,2) 
+# def mse(y_hat, y): return ((y_hat-y)**2).mean()
+# y_hat = x@a
+# ```
+#
+# Now instead we'll use PyTorch's functions to do it for us, and also to handle mini-batches (which we didn't do last time, since our dataset was so small).
+
+bs=64
+train_ds = TensorDataset(x_train, y_train)
+valid_ds = TensorDataset(x_valid, y_valid)
+data = DataBunch.create(train_ds, valid_ds, bs=bs)
+
+x,y = next(iter(data.train_dl))
+x.shape,y.shape
+
+
+class Mnist_Logistic(nn.Module):
+    def __init__(self):
+        super().__init__()
+        self.lin = nn.Linear(784, 10, bias=True)
+
+    def forward(self, xb): return self.lin(xb)
+
+
+model = Mnist_Logistic().cuda()
+
+model
+
+model.lin
+
+model(x).shape
+
+[p.shape for p in model.parameters()]
+
+lr=2e-2
+
+loss_func = nn.CrossEntropyLoss()
+
+
+def update(x,y,lr):
+    wd = 1e-5
+    y_hat = model(x)
+    # weight decay
+    w2 = 0.
+    for p in model.parameters(): w2 += (p**2).sum()
+    # add to regular loss
+    loss = loss_func(y_hat, y) + w2*wd
+    loss.backward()
+    with torch.no_grad():
+        for p in model.parameters():
+            p.sub_(lr * p.grad)
+            p.grad.zero_()
+    return loss.item()
+
+
+losses = [update(x,y,lr) for x,y in data.train_dl]
+
+plt.plot(losses);
+
+
+class Mnist_NN(nn.Module):
+    def __init__(self):
+        super().__init__()
+        self.lin1 = nn.Linear(784, 50, bias=True)
+        self.lin2 = nn.Linear(50, 10, bias=True)
+
+    def forward(self, xb):
+        x = self.lin1(xb)
+        x = F.relu(x)
+        return self.lin2(x)
+
+
+model = Mnist_NN().cuda()
+
+losses = [update(x,y,lr) for x,y in data.train_dl]
+
+plt.plot(losses);
+
+model = Mnist_NN().cuda()
+
+
+def update(x,y,lr):
+    opt = optim.Adam(model.parameters(), lr)
+    y_hat = model(x)
+    loss = loss_func(y_hat, y)
+    loss.backward()
+    opt.step()
+    opt.zero_grad()
+    return loss.item()
+
+
+losses = [update(x,y,1e-3) for x,y in data.train_dl]
+
+plt.plot(losses);
+
+learn = Learner(data, Mnist_NN(), loss_func=loss_func, metrics=accuracy)
+
+# %debug
+
+learn.lr_find()
+learn.recorder.plot()
+
+learn.fit_one_cycle(1, 1e-2)
+
+learn.recorder.plot_lr(show_moms=True)
+
+learn.recorder.plot_losses()
+
+# ## fin
+
+
diff --git a/nbs/dl1/lesson6-pets-more.ipynb b/nbs/dl1/lesson6-pets-more.ipynb
index 792cfbd..bbb0a19 100644
--- ./nbs/dl1/lesson6-pets-more.ipynb
+++ ./nbs/dl1/lesson6-pets-more.ipynb
@@ -1333,6 +1333,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl1/lesson6-pets-more.py b/nbs/dl1/lesson6-pets-more.py
new file mode 100644
index 0000000..99a302e
--- /dev/null
+++ ./nbs/dl1/lesson6-pets-more.py
@@ -0,0 +1,198 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Lesson 6: pets revisited
+
+# +
+# %reload_ext autoreload
+# %autoreload 2
+# %matplotlib inline
+
+from fastai.vision import *
+# -
+
+bs = 64
+
+path = untar_data(URLs.PETS)/'images'
+
+# ## Data augmentation
+
+tfms = get_transforms(max_rotate=20, max_zoom=1.3, max_lighting=0.4, max_warp=0.4,
+                      p_affine=1., p_lighting=1.)
+
+doc(get_transforms)
+
+src = ImageList.from_folder(path).split_by_rand_pct(0.2, seed=2)
+
+
+def get_data(size, bs, padding_mode='reflection'):
+    return (src.label_from_re(r'([^/]+)_\d+.jpg$')
+           .transform(tfms, size=size, padding_mode=padding_mode)
+           .databunch(bs=bs).normalize(imagenet_stats))
+
+
+data = get_data(224, bs, 'zeros')
+
+
+# +
+def _plot(i,j,ax):
+    x,y = data.train_ds[3]
+    x.show(ax, y=y)
+
+plot_multi(_plot, 3, 3, figsize=(8,8))
+# -
+
+data = get_data(224,bs)
+
+plot_multi(_plot, 3, 3, figsize=(8,8))
+
+# ## Train a model
+
+gc.collect()
+learn = cnn_learner(data, models.resnet34, metrics=error_rate, bn_final=True)
+
+learn.fit_one_cycle(3, slice(1e-2), pct_start=0.8)
+
+learn.unfreeze()
+learn.fit_one_cycle(2, max_lr=slice(1e-6,1e-3), pct_start=0.8)
+
+data = get_data(352,bs)
+learn.data = data
+
+learn.fit_one_cycle(2, max_lr=slice(1e-6,1e-4))
+
+learn.save('352')
+
+# ## Convolution kernel
+
+data = get_data(352,16)
+
+learn = cnn_learner(data, models.resnet34, metrics=error_rate, bn_final=True).load('352')
+
+idx=0
+x,y = data.valid_ds[idx]
+x.show()
+data.valid_ds.y[idx]
+
+k = tensor([
+    [0.  ,-5/3,1],
+    [-5/3,-5/3,1],
+    [1.  ,1   ,1],
+]).expand(1,3,3,3)/6
+
+k
+
+k.shape
+
+t = data.valid_ds[0][0].data; t.shape
+
+t[None].shape
+
+edge = F.conv2d(t[None], k)
+
+show_image(edge[0], figsize=(5,5));
+
+data.c
+
+learn.model
+
+print(learn.summary())
+
+# ## Heatmap
+
+m = learn.model.eval();
+
+xb,_ = data.one_item(x)
+xb_im = Image(data.denorm(xb)[0])
+xb = xb.cuda()
+
+from fastai.callbacks.hooks import *
+
+
+def hooked_backward(cat=y):
+    with hook_output(m[0]) as hook_a: 
+        with hook_output(m[0], grad=True) as hook_g:
+            preds = m(xb)
+            preds[0,int(cat)].backward()
+    return hook_a,hook_g
+
+
+hook_a,hook_g = hooked_backward()
+
+acts  = hook_a.stored[0].cpu()
+acts.shape
+
+avg_acts = acts.mean(0)
+avg_acts.shape
+
+
+def show_heatmap(hm):
+    _,ax = plt.subplots()
+    xb_im.show(ax)
+    ax.imshow(hm, alpha=0.6, extent=(0,352,352,0),
+              interpolation='bilinear', cmap='magma');
+
+
+show_heatmap(avg_acts)
+
+# ## Grad-CAM
+
+# Paper: [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/abs/1610.02391)
+
+grad = hook_g.stored[0][0].cpu()
+grad_chan = grad.mean(1).mean(1)
+grad.shape,grad_chan.shape
+
+mult = (acts*grad_chan[...,None,None]).mean(0)
+
+show_heatmap(mult)
+
+fn = path/'../other/bulldog_maine.jpg' #Replace with your own image
+
+x = open_image(fn); x
+
+xb,_ = data.one_item(x)
+xb_im = Image(data.denorm(xb)[0])
+xb = xb.cuda()
+
+hook_a,hook_g = hooked_backward()
+
+# +
+acts = hook_a.stored[0].cpu()
+grad = hook_g.stored[0][0].cpu()
+
+grad_chan = grad.mean(1).mean(1)
+mult = (acts*grad_chan[...,None,None]).mean(0)
+# -
+
+show_heatmap(mult)
+
+data.classes[0]
+
+hook_a,hook_g = hooked_backward(0)
+
+# +
+acts = hook_a.stored[0].cpu()
+grad = hook_g.stored[0][0].cpu()
+
+grad_chan = grad.mean(1).mean(1)
+mult = (acts*grad_chan[...,None,None]).mean(0)
+# -
+
+show_heatmap(mult)
+
+# ## fin
+
+
diff --git a/nbs/dl1/lesson6-rossmann.ipynb b/nbs/dl1/lesson6-rossmann.ipynb
index 21e1d3f..21c6d11 100644
--- ./nbs/dl1/lesson6-rossmann.ipynb
+++ ./nbs/dl1/lesson6-rossmann.ipynb
@@ -1779,6 +1779,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl1/lesson6-rossmann.py b/nbs/dl1/lesson6-rossmann.py
new file mode 100644
index 0000000..0ad6342
--- /dev/null
+++ ./nbs/dl1/lesson6-rossmann.py
@@ -0,0 +1,144 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# %reload_ext autoreload
+# %autoreload 2
+
+from fastai.tabular import *
+
+# # Rossmann
+
+# ## Data preparation
+
+# To create the feature-engineered train_clean and test_clean from the Kaggle competition data, run `rossman_data_clean.ipynb`. One important step that deals with time series is this:
+#
+# ```python
+# add_datepart(train, "Date", drop=False)
+# add_datepart(test, "Date", drop=False)
+# ```
+
+path = Config().data_path()/'rossmann'
+train_df = pd.read_pickle(path/'train_clean')
+
+train_df.head().T
+
+n = len(train_df); n
+
+# ### Experimenting with a sample
+
+idx = np.random.permutation(range(n))[:2000]
+idx.sort()
+small_train_df = train_df.iloc[idx[:1000]]
+small_test_df = train_df.iloc[idx[1000:]]
+small_cont_vars = ['CompetitionDistance', 'Mean_Humidity']
+small_cat_vars =  ['Store', 'DayOfWeek', 'PromoInterval']
+small_train_df = small_train_df[small_cat_vars + small_cont_vars + ['Sales']]
+small_test_df = small_test_df[small_cat_vars + small_cont_vars + ['Sales']]
+
+small_train_df.head()
+
+small_test_df.head()
+
+categorify = Categorify(small_cat_vars, small_cont_vars)
+categorify(small_train_df)
+categorify(small_test_df, test=True)
+
+small_test_df.head()
+
+small_train_df.PromoInterval.cat.categories
+
+small_train_df['PromoInterval'].cat.codes[:5]
+
+fill_missing = FillMissing(small_cat_vars, small_cont_vars)
+fill_missing(small_train_df)
+fill_missing(small_test_df, test=True)
+
+small_train_df[small_train_df['CompetitionDistance_na'] == True]
+
+# ### Preparing full data set
+
+train_df = pd.read_pickle(path/'train_clean')
+test_df = pd.read_pickle(path/'test_clean')
+
+len(train_df),len(test_df)
+
+procs=[FillMissing, Categorify, Normalize]
+
+# +
+cat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',
+    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',
+    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',
+    'SchoolHoliday_fw', 'SchoolHoliday_bw']
+
+cont_vars = ['CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',
+   'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', 
+   'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',
+   'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday']
+# -
+
+dep_var = 'Sales'
+df = train_df[cat_vars + cont_vars + [dep_var,'Date']].copy()
+
+test_df['Date'].min(), test_df['Date'].max()
+
+cut = train_df['Date'][(train_df['Date'] == train_df['Date'][len(test_df)])].index.max()
+cut
+
+valid_idx = range(cut)
+
+df[dep_var].head()
+
+data = (TabularList.from_df(df, path=path, cat_names=cat_vars, cont_names=cont_vars, procs=procs,)
+                .split_by_idx(valid_idx)
+                .label_from_df(cols=dep_var, label_cls=FloatList, log=True)
+                .add_test(TabularList.from_df(test_df, path=path, cat_names=cat_vars, cont_names=cont_vars))
+                .databunch())
+
+doc(FloatList)
+
+# ## Model
+
+max_log_y = np.log(np.max(train_df['Sales'])*1.2)
+y_range = torch.tensor([0, max_log_y], device=defaults.device)
+
+learn = tabular_learner(data, layers=[1000,500], ps=[0.001,0.01], emb_drop=0.04, 
+                        y_range=y_range, metrics=exp_rmspe)
+
+learn.model
+
+len(data.train_ds.cont_names)
+
+learn.lr_find()
+
+learn.recorder.plot()
+
+learn.fit_one_cycle(5, 1e-3, wd=0.2)
+
+learn.save('1')
+
+learn.recorder.plot_losses(skip_start=10000)
+
+learn.load('1');
+
+learn.fit_one_cycle(5, 3e-4)
+
+learn.fit_one_cycle(5, 3e-4)
+
+# (10th place in the competition was 0.108)
+
+test_preds=learn.get_preds(DatasetType.Test)
+test_df["Sales"]=np.exp(test_preds[0].data).numpy().T[0]
+test_df[["Id","Sales"]]=test_df[["Id","Sales"]].astype("int")
+test_df[["Id","Sales"]].to_csv("rossmann_submission.csv",index=False)
diff --git a/nbs/dl1/lesson7-human-numbers.ipynb b/nbs/dl1/lesson7-human-numbers.ipynb
index 932284c..343f583 100644
--- ./nbs/dl1/lesson7-human-numbers.ipynb
+++ ./nbs/dl1/lesson7-human-numbers.ipynb
@@ -1488,6 +1488,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl1/lesson7-human-numbers.py b/nbs/dl1/lesson7-human-numbers.py
new file mode 100644
index 0000000..4da0f97
--- /dev/null
+++ ./nbs/dl1/lesson7-human-numbers.py
@@ -0,0 +1,254 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Human numbers
+
+from fastai.text import *
+
+bs=64
+
+# ## Data
+
+path = untar_data(URLs.HUMAN_NUMBERS)
+path.ls()
+
+
+def readnums(d): return [', '.join(o.strip() for o in open(path/d).readlines())]
+
+
+train_txt = readnums('train.txt'); train_txt[0][:80]
+
+valid_txt = readnums('valid.txt'); valid_txt[0][-80:]
+
+# +
+train = TextList(train_txt, path=path)
+valid = TextList(valid_txt, path=path)
+
+src = ItemLists(path=path, train=train, valid=valid).label_for_lm()
+data = src.databunch(bs=bs)
+# -
+
+train[0].text[:80]
+
+len(data.valid_ds[0][0].data)
+
+data.bptt, len(data.valid_dl)
+
+13017/70/bs
+
+it = iter(data.valid_dl)
+x1,y1 = next(it)
+x2,y2 = next(it)
+x3,y3 = next(it)
+it.close()
+
+x1.numel()+x2.numel()+x3.numel()
+
+x1.shape,y1.shape
+
+x2.shape,y2.shape
+
+x1[:,0]
+
+y1[:,0]
+
+v = data.valid_ds.vocab
+
+v.textify(x1[0])
+
+v.textify(y1[0])
+
+v.textify(x2[0])
+
+v.textify(x3[0])
+
+v.textify(x1[1])
+
+v.textify(x2[1])
+
+v.textify(x3[1])
+
+v.textify(x3[-1])
+
+data.show_batch(ds_type=DatasetType.Valid)
+
+# ## Single fully connected model
+
+data = src.databunch(bs=bs, bptt=3)
+
+x,y = data.one_batch()
+x.shape,y.shape
+
+nv = len(v.itos); nv
+
+nh=64
+
+
+def loss4(input,target): return F.cross_entropy(input, target[:,-1])
+def acc4 (input,target): return accuracy(input, target[:,-1])
+
+
+class Model0(nn.Module):
+    def __init__(self):
+        super().__init__()
+        self.i_h = nn.Embedding(nv,nh)  # green arrow
+        self.h_h = nn.Linear(nh,nh)     # brown arrow
+        self.h_o = nn.Linear(nh,nv)     # blue arrow
+        self.bn = nn.BatchNorm1d(nh)
+        
+    def forward(self, x):
+        h = self.bn(F.relu(self.h_h(self.i_h(x[:,0]))))
+        if x.shape[1]>1:
+            h = h + self.i_h(x[:,1])
+            h = self.bn(F.relu(self.h_h(h)))
+        if x.shape[1]>2:
+            h = h + self.i_h(x[:,2])
+            h = self.bn(F.relu(self.h_h(h)))
+        return self.h_o(h)
+
+
+learn = Learner(data, Model0(), loss_func=loss4, metrics=acc4)
+
+learn.fit_one_cycle(6, 1e-4)
+
+
+# ## Same thing with a loop
+
+class Model1(nn.Module):
+    def __init__(self):
+        super().__init__()
+        self.i_h = nn.Embedding(nv,nh)  # green arrow
+        self.h_h = nn.Linear(nh,nh)     # brown arrow
+        self.h_o = nn.Linear(nh,nv)     # blue arrow
+        self.bn = nn.BatchNorm1d(nh)
+        
+    def forward(self, x):
+        h = torch.zeros(x.shape[0], nh).to(device=x.device)
+        for i in range(x.shape[1]):
+            h = h + self.i_h(x[:,i])
+            h = self.bn(F.relu(self.h_h(h)))
+        return self.h_o(h)
+
+
+learn = Learner(data, Model1(), loss_func=loss4, metrics=acc4)
+
+learn.fit_one_cycle(6, 1e-4)
+
+# ## Multi fully connected model
+
+data = src.databunch(bs=bs, bptt=20)
+
+x,y = data.one_batch()
+x.shape,y.shape
+
+
+class Model2(nn.Module):
+    def __init__(self):
+        super().__init__()
+        self.i_h = nn.Embedding(nv,nh)
+        self.h_h = nn.Linear(nh,nh)
+        self.h_o = nn.Linear(nh,nv)
+        self.bn = nn.BatchNorm1d(nh)
+        
+    def forward(self, x):
+        h = torch.zeros(x.shape[0], nh).to(device=x.device)
+        res = []
+        for i in range(x.shape[1]):
+            h = h + self.i_h(x[:,i])
+            h = F.relu(self.h_h(h))
+            res.append(self.h_o(self.bn(h)))
+        return torch.stack(res, dim=1)
+
+
+learn = Learner(data, Model2(), metrics=accuracy)
+
+learn.fit_one_cycle(10, 1e-4, pct_start=0.1)
+
+
+# ## Maintain state
+
+class Model3(nn.Module):
+    def __init__(self):
+        super().__init__()
+        self.i_h = nn.Embedding(nv,nh)
+        self.h_h = nn.Linear(nh,nh)
+        self.h_o = nn.Linear(nh,nv)
+        self.bn = nn.BatchNorm1d(nh)
+        self.h = torch.zeros(bs, nh).cuda()
+        
+    def forward(self, x):
+        res = []
+        h = self.h
+        for i in range(x.shape[1]):
+            h = h + self.i_h(x[:,i])
+            h = F.relu(self.h_h(h))
+            res.append(self.bn(h))
+        self.h = h.detach()
+        res = torch.stack(res, dim=1)
+        res = self.h_o(res)
+        return res
+
+
+learn = Learner(data, Model3(), metrics=accuracy)
+
+learn.fit_one_cycle(20, 3e-3)
+
+
+# ## nn.RNN
+
+class Model4(nn.Module):
+    def __init__(self):
+        super().__init__()
+        self.i_h = nn.Embedding(nv,nh)
+        self.rnn = nn.RNN(nh,nh, batch_first=True)
+        self.h_o = nn.Linear(nh,nv)
+        self.bn = BatchNorm1dFlat(nh)
+        self.h = torch.zeros(1, bs, nh).cuda()
+        
+    def forward(self, x):
+        res,h = self.rnn(self.i_h(x), self.h)
+        self.h = h.detach()
+        return self.h_o(self.bn(res))
+
+
+learn = Learner(data, Model4(), metrics=accuracy)
+
+learn.fit_one_cycle(20, 3e-3)
+
+
+# ## 2-layer GRU
+
+class Model5(nn.Module):
+    def __init__(self):
+        super().__init__()
+        self.i_h = nn.Embedding(nv,nh)
+        self.rnn = nn.GRU(nh, nh, 2, batch_first=True)
+        self.h_o = nn.Linear(nh,nv)
+        self.bn = BatchNorm1dFlat(nh)
+        self.h = torch.zeros(2, bs, nh).cuda()
+        
+    def forward(self, x):
+        res,h = self.rnn(self.i_h(x), self.h)
+        self.h = h.detach()
+        return self.h_o(self.bn(res))
+
+
+learn = Learner(data, Model5(), metrics=accuracy)
+
+learn.fit_one_cycle(10, 1e-2)
+
+# ## fin
+
+
diff --git a/nbs/dl1/lesson7-resnet-mnist.ipynb b/nbs/dl1/lesson7-resnet-mnist.ipynb
index 556ef41..61816bb 100644
--- ./nbs/dl1/lesson7-resnet-mnist.ipynb
+++ ./nbs/dl1/lesson7-resnet-mnist.ipynb
@@ -1157,6 +1157,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl1/lesson7-resnet-mnist.py b/nbs/dl1/lesson7-resnet-mnist.py
new file mode 100644
index 0000000..1d2dd79
--- /dev/null
+++ ./nbs/dl1/lesson7-resnet-mnist.py
@@ -0,0 +1,186 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# ## MNIST CNN
+
+# %reload_ext autoreload
+# %autoreload 2
+# %matplotlib inline
+
+from fastai.vision import *
+
+# ### Data
+
+path = untar_data(URLs.MNIST)
+
+path.ls()
+
+il = ImageList.from_folder(path, convert_mode='L')
+
+il.items[0]
+
+defaults.cmap='binary'
+
+il
+
+il[0].show()
+
+sd = il.split_by_folder(train='training', valid='testing')
+
+sd
+
+(path/'training').ls()
+
+ll = sd.label_from_folder()
+
+ll
+
+x,y = ll.train[0]
+
+x.show()
+print(y,x.shape)
+
+tfms = ([*rand_pad(padding=3, size=28, mode='zeros')], [])
+
+ll = ll.transform(tfms)
+
+bs = 128
+
+# not using imagenet_stats because not using pretrained model
+data = ll.databunch(bs=bs).normalize()
+
+x,y = data.train_ds[0]
+
+x.show()
+print(y)
+
+
+def _plot(i,j,ax): data.train_ds[0][0].show(ax, cmap='gray')
+plot_multi(_plot, 3, 3, figsize=(8,8))
+
+xb,yb = data.one_batch()
+xb.shape,yb.shape
+
+data.show_batch(rows=3, figsize=(5,5))
+
+
+# ### Basic CNN with batchnorm
+
+def conv(ni,nf): return nn.Conv2d(ni, nf, kernel_size=3, stride=2, padding=1)
+
+
+model = nn.Sequential(
+    conv(1, 8), # 14
+    nn.BatchNorm2d(8),
+    nn.ReLU(),
+    conv(8, 16), # 7
+    nn.BatchNorm2d(16),
+    nn.ReLU(),
+    conv(16, 32), # 4
+    nn.BatchNorm2d(32),
+    nn.ReLU(),
+    conv(32, 16), # 2
+    nn.BatchNorm2d(16),
+    nn.ReLU(),
+    conv(16, 10), # 1
+    nn.BatchNorm2d(10),
+    Flatten()     # remove (1,1) grid
+)
+
+learn = Learner(data, model, loss_func = nn.CrossEntropyLoss(), metrics=accuracy)
+
+print(learn.summary())
+
+xb = xb.cuda()
+
+model(xb).shape
+
+learn.lr_find(end_lr=100)
+
+learn.recorder.plot()
+
+learn.fit_one_cycle(3, max_lr=0.1)
+
+
+# ### Refactor
+
+def conv2(ni,nf): return conv_layer(ni,nf,stride=2)
+
+
+model = nn.Sequential(
+    conv2(1, 8),   # 14
+    conv2(8, 16),  # 7
+    conv2(16, 32), # 4
+    conv2(32, 16), # 2
+    conv2(16, 10), # 1
+    Flatten()      # remove (1,1) grid
+)
+
+learn = Learner(data, model, loss_func = nn.CrossEntropyLoss(), metrics=accuracy)
+
+learn.fit_one_cycle(10, max_lr=0.1)
+
+
+# ### Resnet-ish
+
+class ResBlock(nn.Module):
+    def __init__(self, nf):
+        super().__init__()
+        self.conv1 = conv_layer(nf,nf)
+        self.conv2 = conv_layer(nf,nf)
+        
+    def forward(self, x): return x + self.conv2(self.conv1(x))
+
+
+help(res_block)
+
+model = nn.Sequential(
+    conv2(1, 8),
+    res_block(8),
+    conv2(8, 16),
+    res_block(16),
+    conv2(16, 32),
+    res_block(32),
+    conv2(32, 16),
+    res_block(16),
+    conv2(16, 10),
+    Flatten()
+)
+
+
+def conv_and_res(ni,nf): return nn.Sequential(conv2(ni, nf), res_block(nf))
+
+
+model = nn.Sequential(
+    conv_and_res(1, 8),
+    conv_and_res(8, 16),
+    conv_and_res(16, 32),
+    conv_and_res(32, 16),
+    conv2(16, 10),
+    Flatten()
+)
+
+learn = Learner(data, model, loss_func = nn.CrossEntropyLoss(), metrics=accuracy)
+
+learn.lr_find(end_lr=100)
+learn.recorder.plot()
+
+learn.fit_one_cycle(12, max_lr=0.05)
+
+print(learn.summary())
+
+# ## fin
+
+
diff --git a/nbs/dl1/lesson7-superres-gan.ipynb b/nbs/dl1/lesson7-superres-gan.ipynb
index dd9658f..44059f4 100644
--- ./nbs/dl1/lesson7-superres-gan.ipynb
+++ ./nbs/dl1/lesson7-superres-gan.ipynb
@@ -1119,6 +1119,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl1/lesson7-superres-gan.py b/nbs/dl1/lesson7-superres-gan.py
new file mode 100644
index 0000000..aeaadad
--- /dev/null
+++ ./nbs/dl1/lesson7-superres-gan.py
@@ -0,0 +1,196 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# ## Pretrained GAN
+
+import fastai
+from fastai.vision import *
+from fastai.callbacks import *
+from fastai.vision.gan import *
+
+path = untar_data(URLs.PETS)
+path_hr = path/'images'
+path_lr = path/'crappy'
+
+# ## Crappified data
+
+# Prepare the input data by crappifying images.
+
+from crappify import *
+
+# Uncomment the first time you run this notebook.
+
+# +
+#il = ImageList.from_folder(path_hr)
+#parallel(crappifier(path_lr, path_hr), il.items)
+# -
+
+# For gradual resizing we can change the commented line here.
+
+bs,size=32, 128
+# bs,size = 24,160
+#bs,size = 8,256
+arch = models.resnet34
+
+# ## Pre-train generator
+
+# Now let's pretrain the generator.
+
+arch = models.resnet34
+src = ImageImageList.from_folder(path_lr).split_by_rand_pct(0.1, seed=42)
+
+
+def get_data(bs,size):
+    data = (src.label_from_func(lambda x: path_hr/x.name)
+           .transform(get_transforms(max_zoom=2.), size=size, tfm_y=True)
+           .databunch(bs=bs).normalize(imagenet_stats, do_y=True))
+
+    data.c = 3
+    return data
+
+
+data_gen = get_data(bs,size)
+
+data_gen.show_batch(4)
+
+wd = 1e-3
+
+y_range = (-3.,3.)
+
+loss_gen = MSELossFlat()
+
+
+def create_gen_learner():
+    return unet_learner(data_gen, arch, wd=wd, blur=True, norm_type=NormType.Weight,
+                         self_attention=True, y_range=y_range, loss_func=loss_gen)
+
+
+learn_gen = create_gen_learner()
+
+learn_gen.fit_one_cycle(2, pct_start=0.8)
+
+learn_gen.unfreeze()
+
+learn_gen.fit_one_cycle(3, slice(1e-6,1e-3))
+
+learn_gen.show_results(rows=4)
+
+learn_gen.save('gen-pre2')
+
+# ## Save generated images
+
+learn_gen.load('gen-pre2');
+
+name_gen = 'image_gen'
+path_gen = path/name_gen
+
+# +
+# shutil.rmtree(path_gen)
+# -
+
+path_gen.mkdir(exist_ok=True)
+
+
+def save_preds(dl):
+    i=0
+    names = dl.dataset.items
+    
+    for b in dl:
+        preds = learn_gen.pred_batch(batch=b, reconstruct=True)
+        for o in preds:
+            o.save(path_gen/names[i].name)
+            i += 1
+
+
+save_preds(data_gen.fix_dl)
+
+PIL.Image.open(path_gen.ls()[0])
+
+# ## Train critic
+
+learn_gen=None
+gc.collect()
+
+
+# Pretrain the critic on crappy vs not crappy.
+
+def get_crit_data(classes, bs, size):
+    src = ImageList.from_folder(path, include=classes).split_by_rand_pct(0.1, seed=42)
+    ll = src.label_from_folder(classes=classes)
+    data = (ll.transform(get_transforms(max_zoom=2.), size=size)
+           .databunch(bs=bs).normalize(imagenet_stats))
+    data.c = 3
+    return data
+
+
+data_crit = get_crit_data([name_gen, 'images'], bs=bs, size=size)
+
+data_crit.show_batch(rows=3, ds_type=DatasetType.Train, imgsize=3)
+
+loss_critic = AdaptiveLoss(nn.BCEWithLogitsLoss())
+
+
+def create_critic_learner(data, metrics):
+    return Learner(data, gan_critic(), metrics=metrics, loss_func=loss_critic, wd=wd)
+
+
+learn_critic = create_critic_learner(data_crit, accuracy_thresh_expand)
+
+learn_critic.fit_one_cycle(6, 1e-3)
+
+learn_critic.save('critic-pre2')
+
+# ## GAN
+
+# Now we'll combine those pretrained model in a GAN.
+
+learn_crit=None
+learn_gen=None
+gc.collect()
+
+data_crit = get_crit_data(['crappy', 'images'], bs=bs, size=size)
+
+learn_crit = create_critic_learner(data_crit, metrics=None).load('critic-pre2')
+
+learn_gen = create_gen_learner().load('gen-pre2')
+
+# To define a GAN Learner, we just have to specify the learner objects foor the generator and the critic. The switcher is a callback that decides when to switch from discriminator to generator and vice versa. Here we do as many iterations of the discriminator as needed to get its loss back < 0.5 then one iteration of the generator.
+#
+# The loss of the critic is given by `learn_crit.loss_func`. We take the average of this loss function on the batch of real predictions (target 1) and the batch of fake predicitions (target 0). 
+#
+# The loss of the generator is weighted sum (weights in `weights_gen`) of `learn_crit.loss_func` on the batch of fake (passed throught the critic to become predictions) with a target of 1, and the `learn_gen.loss_func` applied to the output (batch of fake) and the target (corresponding batch of superres images).
+
+switcher = partial(AdaptiveGANSwitcher, critic_thresh=0.65)
+learn = GANLearner.from_learners(learn_gen, learn_crit, weights_gen=(1.,50.), show_img=False, switcher=switcher,
+                                 opt_func=partial(optim.Adam, betas=(0.,0.99)), wd=wd)
+learn.callback_fns.append(partial(GANDiscriminativeLR, mult_lr=5.))
+
+lr = 1e-4
+
+learn.fit(40,lr)
+
+learn.save('gan-1c')
+
+learn.data=get_data(16,192)
+
+learn.fit(10,lr/2)
+
+learn.show_results(rows=16)
+
+learn.save('gan-1c')
+
+# ## fin
+
+
diff --git a/nbs/dl1/lesson7-superres-imagenet.ipynb b/nbs/dl1/lesson7-superres-imagenet.ipynb
index 5da63c6..9f1aaa4 100644
--- ./nbs/dl1/lesson7-superres-imagenet.ipynb
+++ ./nbs/dl1/lesson7-superres-imagenet.ipynb
@@ -517,6 +517,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl1/lesson7-superres-imagenet.py b/nbs/dl1/lesson7-superres-imagenet.py
new file mode 100644
index 0000000..25e0dab
--- /dev/null
+++ ./nbs/dl1/lesson7-superres-imagenet.py
@@ -0,0 +1,178 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# ## Super resolution on Imagenet
+
+import fastai
+from fastai.vision import *
+from fastai.callbacks import *
+from fastai.utils.mem import *
+
+from torchvision.models import vgg16_bn
+
+torch.cuda.set_device(0)
+
+# +
+path = Path('data/imagenet')
+path_hr = path/'train'
+path_lr = path/'small-64/train'
+path_mr = path/'small-256/train'
+
+# note: this notebook relies on models created by lesson7-superres.ipynb
+path_pets = untar_data(URLs.PETS)
+# -
+
+il = ImageList.from_folder(path_hr)
+
+
+def resize_one(fn, i, path, size):
+    dest = path/fn.relative_to(path_hr)
+    dest.parent.mkdir(parents=True, exist_ok=True)
+    img = PIL.Image.open(fn)
+    targ_sz = resize_to(img, size, use_min=True)
+    img = img.resize(targ_sz, resample=PIL.Image.BILINEAR).convert('RGB')
+    img.save(dest, quality=60)
+
+
+assert path.exists(), f"need imagenet dataset @ {path}"
+# create smaller image sets the first time this nb is run
+sets = [(path_lr, 64), (path_mr, 256)]
+for p,size in sets:
+    if not p.exists(): 
+        print(f"resizing to {size} into {p}")
+        parallel(partial(resize_one, path=p, size=size), il.items)
+
+# +
+free = gpu_mem_get_free_no_cache()
+# the max size of the test image depends on the available GPU RAM 
+if free > 8200: bs,size=16,256  
+else:           bs,size=8,256
+print(f"using bs={bs}, size={size}, have {free}MB of GPU RAM free")
+
+arch = models.resnet34
+# sample = 0.1
+sample = False
+
+tfms = get_transforms()
+# -
+
+src = ImageImageList.from_folder(path_lr)
+
+if sample: src = src.filter_by_rand(sample, seed=42)
+
+src = src.split_by_rand_pct(0.1, seed=42)
+
+
+def get_data(bs,size):
+    data = (src.label_from_func(lambda x: path_hr/x.relative_to(path_lr))
+           .transform(get_transforms(max_zoom=2.), size=size, tfm_y=True)
+           .databunch(bs=bs).normalize(imagenet_stats, do_y=True))
+
+    data.c = 3
+    return data
+
+
+data = get_data(bs,size)
+
+
+# ## Feature loss
+
+def gram_matrix(x):
+    n,c,h,w = x.size()
+    x = x.view(n, c, -1)
+    return (x @ x.transpose(1,2))/(c*h*w)
+
+
+vgg_m = vgg16_bn(True).features.cuda().eval()
+requires_grad(vgg_m, False)
+blocks = [i-1 for i,o in enumerate(children(vgg_m)) if isinstance(o,nn.MaxPool2d)]
+
+# +
+base_loss = F.l1_loss
+
+class FeatureLoss(nn.Module):
+    def __init__(self, m_feat, layer_ids, layer_wgts):
+        super().__init__()
+        self.m_feat = m_feat
+        self.loss_features = [self.m_feat[i] for i in layer_ids]
+        self.hooks = hook_outputs(self.loss_features, detach=False)
+        self.wgts = layer_wgts
+        self.metric_names = ['pixel',] + [f'feat_{i}' for i in range(len(layer_ids))
+              ] + [f'gram_{i}' for i in range(len(layer_ids))]
+
+    def make_features(self, x, clone=False):
+        self.m_feat(x)
+        return [(o.clone() if clone else o) for o in self.hooks.stored]
+    
+    def forward(self, input, target):
+        out_feat = self.make_features(target, clone=True)
+        in_feat = self.make_features(input)
+        self.feat_losses = [base_loss(input,target)]
+        self.feat_losses += [base_loss(f_in, f_out)*w
+                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]
+        self.feat_losses += [base_loss(gram_matrix(f_in), gram_matrix(f_out))*w**2 * 5e3
+                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]
+        self.metrics = dict(zip(self.metric_names, self.feat_losses))
+        return sum(self.feat_losses)
+    
+    def __del__(self): self.hooks.remove()
+
+
+# -
+
+feat_loss = FeatureLoss(vgg_m, blocks[2:5], [5,15,2])
+
+# ## Train
+
+wd = 1e-3
+learn = unet_learner(data, arch, wd=wd, loss_func=feat_loss, callback_fns=LossMetrics, blur=True, norm_type=NormType.Weight)
+gc.collect();
+
+learn.unfreeze()
+
+# relies on first running lesson7-superres.ipynb which created the following model
+learn.load((path_pets/'small-96'/'models'/'2b').absolute());
+
+learn.fit_one_cycle(1, slice(1e-6,1e-4))
+
+learn.save('imagenet')
+
+learn.show_results(rows=3, imgsize=5)
+
+learn.recorder.plot_losses()
+
+# ## Test
+
+_=learn.load('imagenet')
+
+data_mr = (ImageImageList.from_folder(path_mr).split_by_rand_pct(0.1, seed=42)
+          .label_from_func(lambda x: path_hr/x.relative_to(path_mr))
+          .transform(get_transforms(), size=(820,1024), tfm_y=True)
+          .databunch(bs=2).normalize(imagenet_stats, do_y=True))
+
+learn.data = data_mr
+
+# here put some image you want to enhance, e.g. the original notebook uses a single video frame from a powerpoint presentation on dropout paper
+fn = path_pets/'other'/'dropout.jpg'
+
+img = open_image(fn); img.shape
+
+_,img_hr,b = learn.predict(img)
+
+show_image(img, figsize=(18,15), interpolation='nearest');
+
+Image(img_hr).show(figsize=(18,15))
+
+
diff --git a/nbs/dl1/lesson7-superres.ipynb b/nbs/dl1/lesson7-superres.ipynb
index fffab3b..479abaf 100644
--- ./nbs/dl1/lesson7-superres.ipynb
+++ ./nbs/dl1/lesson7-superres.ipynb
@@ -1291,6 +1291,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl1/lesson7-superres.py b/nbs/dl1/lesson7-superres.py
new file mode 100644
index 0000000..b709164
--- /dev/null
+++ ./nbs/dl1/lesson7-superres.py
@@ -0,0 +1,202 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# ## Super resolution
+
+# +
+import fastai
+from fastai.vision import *
+from fastai.callbacks import *
+from fastai.utils.mem import *
+
+from torchvision.models import vgg16_bn
+# -
+
+path = untar_data(URLs.PETS)
+path_hr = path/'images'
+path_lr = path/'small-96'
+path_mr = path/'small-256'
+
+il = ImageList.from_folder(path_hr)
+
+
+def resize_one(fn, i, path, size):
+    dest = path/fn.relative_to(path_hr)
+    dest.parent.mkdir(parents=True, exist_ok=True)
+    img = PIL.Image.open(fn)
+    targ_sz = resize_to(img, size, use_min=True)
+    img = img.resize(targ_sz, resample=PIL.Image.BILINEAR).convert('RGB')
+    img.save(dest, quality=60)
+
+
+# create smaller image sets the first time this nb is run
+sets = [(path_lr, 96), (path_mr, 256)]
+for p,size in sets:
+    if not p.exists(): 
+        print(f"resizing to {size} into {p}")
+        parallel(partial(resize_one, path=p, size=size), il.items)
+
+# +
+bs,size=32,128
+arch = models.resnet34
+
+src = ImageImageList.from_folder(path_lr).split_by_rand_pct(0.1, seed=42)
+
+
+# -
+
+def get_data(bs,size):
+    data = (src.label_from_func(lambda x: path_hr/x.name)
+           .transform(get_transforms(max_zoom=2.), size=size, tfm_y=True)
+           .databunch(bs=bs).normalize(imagenet_stats, do_y=True))
+
+    data.c = 3
+    return data
+
+
+data = get_data(bs,size)
+
+data.show_batch(ds_type=DatasetType.Valid, rows=2, figsize=(9,9))
+
+# ## Feature loss
+
+t = data.valid_ds[0][1].data
+t = torch.stack([t,t])
+
+
+def gram_matrix(x):
+    n,c,h,w = x.size()
+    x = x.view(n, c, -1)
+    return (x @ x.transpose(1,2))/(c*h*w)
+
+
+gram_matrix(t)
+
+base_loss = F.l1_loss
+
+vgg_m = vgg16_bn(True).features.cuda().eval()
+requires_grad(vgg_m, False)
+
+blocks = [i-1 for i,o in enumerate(children(vgg_m)) if isinstance(o,nn.MaxPool2d)]
+blocks, [vgg_m[i] for i in blocks]
+
+
+class FeatureLoss(nn.Module):
+    def __init__(self, m_feat, layer_ids, layer_wgts):
+        super().__init__()
+        self.m_feat = m_feat
+        self.loss_features = [self.m_feat[i] for i in layer_ids]
+        self.hooks = hook_outputs(self.loss_features, detach=False)
+        self.wgts = layer_wgts
+        self.metric_names = ['pixel',] + [f'feat_{i}' for i in range(len(layer_ids))
+              ] + [f'gram_{i}' for i in range(len(layer_ids))]
+
+    def make_features(self, x, clone=False):
+        self.m_feat(x)
+        return [(o.clone() if clone else o) for o in self.hooks.stored]
+    
+    def forward(self, input, target):
+        out_feat = self.make_features(target, clone=True)
+        in_feat = self.make_features(input)
+        self.feat_losses = [base_loss(input,target)]
+        self.feat_losses += [base_loss(f_in, f_out)*w
+                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]
+        self.feat_losses += [base_loss(gram_matrix(f_in), gram_matrix(f_out))*w**2 * 5e3
+                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]
+        self.metrics = dict(zip(self.metric_names, self.feat_losses))
+        return sum(self.feat_losses)
+    
+    def __del__(self): self.hooks.remove()
+
+
+feat_loss = FeatureLoss(vgg_m, blocks[2:5], [5,15,2])
+
+# ## Train
+
+wd = 1e-3
+learn = unet_learner(data, arch, wd=wd, loss_func=feat_loss, callback_fns=LossMetrics,
+                     blur=True, norm_type=NormType.Weight)
+gc.collect();
+
+learn.lr_find()
+learn.recorder.plot()
+
+lr = 1e-3
+
+
+def do_fit(save_name, lrs=slice(lr), pct_start=0.9):
+    learn.fit_one_cycle(10, lrs, pct_start=pct_start)
+    learn.save(save_name)
+    learn.show_results(rows=1, imgsize=5)
+
+
+do_fit('1a', slice(lr*10))
+
+learn.unfreeze()
+
+do_fit('1b', slice(1e-5,lr))
+
+data = get_data(12,size*2)
+
+learn.data = data
+learn.freeze()
+gc.collect()
+
+learn.load('1b');
+
+do_fit('2a')
+
+learn.unfreeze()
+
+do_fit('2b', slice(1e-6,1e-4), pct_start=0.3)
+
+# ## Test
+
+learn = None
+gc.collect();
+
+256/320*1024
+
+256/320*1600
+
+free = gpu_mem_get_free_no_cache()
+# the max size of the test image depends on the available GPU RAM 
+if free > 8000: size=(1280, 1600) # >  8GB RAM
+else:           size=( 820, 1024) # <= 8GB RAM
+print(f"using size={size}, have {free}MB of GPU RAM free")
+
+learn = unet_learner(data, arch, loss_func=F.l1_loss, blur=True, norm_type=NormType.Weight)
+
+data_mr = (ImageImageList.from_folder(path_mr).split_by_rand_pct(0.1, seed=42)
+          .label_from_func(lambda x: path_hr/x.name)
+          .transform(get_transforms(), size=size, tfm_y=True)
+          .databunch(bs=1).normalize(imagenet_stats, do_y=True))
+data_mr.c = 3
+
+learn.load('2b');
+
+learn.data = data_mr
+
+fn = data_mr.valid_ds.x.items[0]; fn
+
+img = open_image(fn); img.shape
+
+p,img_hr,b = learn.predict(img)
+
+show_image(img, figsize=(18,15), interpolation='nearest');
+
+Image(img_hr).show(figsize=(18,15))
+
+
diff --git a/nbs/dl1/lesson7-wgan.ipynb b/nbs/dl1/lesson7-wgan.ipynb
index 0280f75..e596de5 100644
--- ./nbs/dl1/lesson7-wgan.ipynb
+++ ./nbs/dl1/lesson7-wgan.ipynb
@@ -412,6 +412,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl1/lesson7-wgan.py b/nbs/dl1/lesson7-wgan.py
new file mode 100644
index 0000000..91b0065
--- /dev/null
+++ ./nbs/dl1/lesson7-wgan.py
@@ -0,0 +1,80 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# %reload_ext autoreload
+# %autoreload 2
+# %matplotlib inline
+
+from fastai.vision import *
+from fastai.vision.gan import *
+
+# ## LSun bedroom data
+
+# For this lesson, we'll be using the bedrooms from the [LSUN dataset](http://lsun.cs.princeton.edu/2017/). The full dataset is a bit too large so we'll use a sample from [kaggle](https://www.kaggle.com/jhoward/lsun_bedroom).
+
+path = untar_data(URLs.LSUN_BEDROOMS)
+
+
+# We then grab all the images in the folder with the data block API. We don't create a validation set here for reasons we'll explain later. It consists of random noise of size 100 by default (can be changed below) as inputs and the images of bedrooms as targets. That's why we do `tfm_y=True` in the transforms, then apply the normalization to the ys and not the xs.
+
+def get_data(bs, size):
+    return (GANItemList.from_folder(path, noise_sz=100)
+               .split_none()
+               .label_from_func(noop)
+               .transform(tfms=[[crop_pad(size=size, row_pct=(0,1), col_pct=(0,1))], []], size=size, tfm_y=True)
+               .databunch(bs=bs)
+               .normalize(stats = [torch.tensor([0.5,0.5,0.5]), torch.tensor([0.5,0.5,0.5])], do_x=False, do_y=True))
+
+
+# We'll begin with a small side and use gradual resizing.
+
+data = get_data(128, 64)
+
+data.show_batch(rows=5)
+
+# ## Models
+
+# GAN stands for [Generative Adversarial Nets](https://arxiv.org/pdf/1406.2661.pdf) and were invented by Ian Goodfellow. The concept is that we will train two models at the same time: a generator and a critic. The generator will try to make new images similar to the ones in our dataset, and the critic will try to classify real images from the ones the generator does. The generator returns images, the critic a single number (usually 0. for fake images and 1. for real ones).
+#
+# We train them against each other in the sense that at each step (more or less), we:
+# 1. Freeze the generator and train the critic for one step by:
+#   - getting one batch of true images (let's call that `real`)
+#   - generating one batch of fake images (let's call that `fake`)
+#   - have the critic evaluate each batch and compute a loss function from that; the important part is that it rewards positively the detection of real images and penalizes the fake ones
+#   - update the weights of the critic with the gradients of this loss
+#   
+#   
+# 2. Freeze the critic and train the generator for one step by:
+#   - generating one batch of fake images
+#   - evaluate the critic on it
+#   - return a loss that rewards posisitivly the critic thinking those are real images; the important part is that it rewards positively the detection of real images and penalizes the fake ones
+#   - update the weights of the generator with the gradients of this loss
+#   
+# Here, we'll use the [Wassertein GAN](https://arxiv.org/pdf/1701.07875.pdf).
+
+# We create a generator and a critic that we pass to `gan_learner`. The noise_size is the size of the random vector from which our generator creates images.
+
+generator = basic_generator(in_size=64, n_channels=3, n_extra_layers=1)
+critic    = basic_critic   (in_size=64, n_channels=3, n_extra_layers=1)
+
+learn = GANLearner.wgan(data, generator, critic, switch_eval=False,
+                        opt_func = partial(optim.Adam, betas = (0.,0.99)), wd=0.)
+
+learn.fit(30,2e-4)
+
+learn.gan_trainer.switch(gen_mode=True)
+learn.show_results(ds_type=DatasetType.Train, rows=16, figsize=(8,8))
+
+
diff --git a/nbs/dl1/rossman_data_clean.ipynb b/nbs/dl1/rossman_data_clean.ipynb
index dc42ffd..c70ce1b 100644
--- ./nbs/dl1/rossman_data_clean.ipynb
+++ ./nbs/dl1/rossman_data_clean.ipynb
@@ -1040,6 +1040,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl1/rossman_data_clean.py b/nbs/dl1/rossman_data_clean.py
new file mode 100644
index 0000000..217ec00
--- /dev/null
+++ ./nbs/dl1/rossman_data_clean.py
@@ -0,0 +1,310 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# %reload_ext autoreload
+# %autoreload 2
+
+from fastai.basics import *
+
+# # Rossmann
+
+# ## Data preparation / Feature engineering
+
+# In addition to the provided data, we will be using external datasets put together by participants in the Kaggle competition. You can download all of them [here](http://files.fast.ai/part2/lesson14/rossmann.tgz). Then you shold untar them in the dirctory to which `PATH` is pointing below.
+#
+# For completeness, the implementation used to put them together is included below.
+
+PATH=Config().data_path()/Path('rossmann/')
+table_names = ['train', 'store', 'store_states', 'state_names', 'googletrend', 'weather', 'test']
+tables = [pd.read_csv(PATH/f'{fname}.csv', low_memory=False) for fname in table_names]
+train, store, store_states, state_names, googletrend, weather, test = tables
+len(train),len(test)
+
+# We turn state Holidays to booleans, to make them more convenient for modeling. We can do calculations on pandas fields using notation very similar (often identical) to numpy.
+
+train.StateHoliday = train.StateHoliday!='0'
+test.StateHoliday = test.StateHoliday!='0'
+
+
+# `join_df` is a function for joining tables on specific fields. By default, we'll be doing a left outer join of `right` on the `left` argument using the given fields for each table.
+#
+# Pandas does joins using the `merge` method. The `suffixes` argument describes the naming convention for duplicate fields. We've elected to leave the duplicate field names on the left untouched, and append a "\_y" to those on the right.
+
+def join_df(left, right, left_on, right_on=None, suffix='_y'):
+    if right_on is None: right_on = left_on
+    return left.merge(right, how='left', left_on=left_on, right_on=right_on, 
+                      suffixes=("", suffix))
+
+
+# Join weather/state names.
+
+weather = join_df(weather, state_names, "file", "StateName")
+
+# In pandas you can add new columns to a dataframe by simply defining it. We'll do this for googletrends by extracting dates and state names from the given data and adding those columns.
+#
+# We're also going to replace all instances of state name 'NI' to match the usage in the rest of the data: 'HB,NI'. This is a good opportunity to highlight pandas indexing. We can use `.loc[rows, cols]` to select a list of rows and a list of columns from the dataframe. In this case, we're selecting rows w/ statename 'NI' by using a boolean list `googletrend.State=='NI'` and selecting "State".
+
+googletrend['Date'] = googletrend.week.str.split(' - ', expand=True)[0]
+googletrend['State'] = googletrend.file.str.split('_', expand=True)[2]
+googletrend.loc[googletrend.State=='NI', "State"] = 'HB,NI'
+
+
+# The following extracts particular date fields from a complete datetime for the purpose of constructing categoricals.
+#
+# You should *always* consider this feature extraction step when working with date-time. Without expanding your date-time into these additional fields, you can't capture any trend/cyclical behavior as a function of time at any of these granularities. We'll add to every table with a date field.
+
+def add_datepart(df, fldname, drop=True, time=False):
+    "Helper function that adds columns relevant to a date."
+    fld = df[fldname]
+    fld_dtype = fld.dtype
+    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):
+        fld_dtype = np.datetime64
+
+    if not np.issubdtype(fld_dtype, np.datetime64):
+        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)
+    targ_pre = re.sub('[Dd]ate$', '', fldname)
+    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',
+            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']
+    if time: attr = attr + ['Hour', 'Minute', 'Second']
+    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())
+    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9
+    if drop: df.drop(fldname, axis=1, inplace=True)
+
+
+add_datepart(weather, "Date", drop=False)
+add_datepart(googletrend, "Date", drop=False)
+add_datepart(train, "Date", drop=False)
+add_datepart(test, "Date", drop=False)
+
+# The Google trends data has a special category for the whole of the Germany - we'll pull that out so we can use it explicitly.
+
+trend_de = googletrend[googletrend.file == 'Rossmann_DE']
+
+# Now we can outer join all of our data into a single dataframe. Recall that in outer joins everytime a value in the joining field on the left table does not have a corresponding value on the right table, the corresponding row in the new table has Null values for all right table fields. One way to check that all records are consistent and complete is to check for Null values post-join, as we do here.
+#
+# *Aside*: Why not just do an inner join?
+# If you are assuming that all records are complete and match on the field you desire, an inner join will do the same thing as an outer join. However, in the event you are wrong or a mistake is made, an outer join followed by a null-check will catch it. (Comparing before/after # of rows for inner join is equivalent, but requires keeping track of before/after row #'s. Outer join is easier.)
+
+store = join_df(store, store_states, "Store")
+len(store[store.State.isnull()])
+
+joined = join_df(train, store, "Store")
+joined_test = join_df(test, store, "Store")
+len(joined[joined.StoreType.isnull()]),len(joined_test[joined_test.StoreType.isnull()])
+
+joined = join_df(joined, googletrend, ["State","Year", "Week"])
+joined_test = join_df(joined_test, googletrend, ["State","Year", "Week"])
+len(joined[joined.trend.isnull()]),len(joined_test[joined_test.trend.isnull()])
+
+joined = joined.merge(trend_de, 'left', ["Year", "Week"], suffixes=('', '_DE'))
+joined_test = joined_test.merge(trend_de, 'left', ["Year", "Week"], suffixes=('', '_DE'))
+len(joined[joined.trend_DE.isnull()]),len(joined_test[joined_test.trend_DE.isnull()])
+
+joined = join_df(joined, weather, ["State","Date"])
+joined_test = join_df(joined_test, weather, ["State","Date"])
+len(joined[joined.Mean_TemperatureC.isnull()]),len(joined_test[joined_test.Mean_TemperatureC.isnull()])
+
+for df in (joined, joined_test):
+    for c in df.columns:
+        if c.endswith('_y'):
+            if c in df.columns: df.drop(c, inplace=True, axis=1)
+
+# Next we'll fill in missing values to avoid complications with `NA`'s. `NA` (not available) is how Pandas indicates missing values; many models have problems when missing values are present, so it's always important to think about how to deal with them. In these cases, we are picking an arbitrary *signal value* that doesn't otherwise appear in the data.
+
+for df in (joined,joined_test):
+    df['CompetitionOpenSinceYear'] = df.CompetitionOpenSinceYear.fillna(1900).astype(np.int32)
+    df['CompetitionOpenSinceMonth'] = df.CompetitionOpenSinceMonth.fillna(1).astype(np.int32)
+    df['Promo2SinceYear'] = df.Promo2SinceYear.fillna(1900).astype(np.int32)
+    df['Promo2SinceWeek'] = df.Promo2SinceWeek.fillna(1).astype(np.int32)
+
+# Next we'll extract features "CompetitionOpenSince" and "CompetitionDaysOpen". Note the use of `apply()` in mapping a function across dataframe values.
+
+for df in (joined,joined_test):
+    df["CompetitionOpenSince"] = pd.to_datetime(dict(year=df.CompetitionOpenSinceYear, 
+                                                     month=df.CompetitionOpenSinceMonth, day=15))
+    df["CompetitionDaysOpen"] = df.Date.subtract(df.CompetitionOpenSince).dt.days
+
+# We'll replace some erroneous / outlying data.
+
+for df in (joined,joined_test):
+    df.loc[df.CompetitionDaysOpen<0, "CompetitionDaysOpen"] = 0
+    df.loc[df.CompetitionOpenSinceYear<1990, "CompetitionDaysOpen"] = 0
+
+# We add "CompetitionMonthsOpen" field, limiting the maximum to 2 years to limit number of unique categories.
+
+for df in (joined,joined_test):
+    df["CompetitionMonthsOpen"] = df["CompetitionDaysOpen"]//30
+    df.loc[df.CompetitionMonthsOpen>24, "CompetitionMonthsOpen"] = 24
+joined.CompetitionMonthsOpen.unique()
+
+# Same process for Promo dates. You may need to install the `isoweek` package first.
+
+# +
+# If needed, uncomment:
+# # ! pip install isoweek
+# -
+
+from isoweek import Week
+for df in (joined,joined_test):
+    df["Promo2Since"] = pd.to_datetime(df.apply(lambda x: Week(
+        x.Promo2SinceYear, x.Promo2SinceWeek).monday(), axis=1))
+    df["Promo2Days"] = df.Date.subtract(df["Promo2Since"]).dt.days
+
+for df in (joined,joined_test):
+    df.loc[df.Promo2Days<0, "Promo2Days"] = 0
+    df.loc[df.Promo2SinceYear<1990, "Promo2Days"] = 0
+    df["Promo2Weeks"] = df["Promo2Days"]//7
+    df.loc[df.Promo2Weeks<0, "Promo2Weeks"] = 0
+    df.loc[df.Promo2Weeks>25, "Promo2Weeks"] = 25
+    df.Promo2Weeks.unique()
+
+joined.to_pickle(PATH/'joined')
+joined_test.to_pickle(PATH/'joined_test')
+
+
+# ## Durations
+
+# It is common when working with time series data to extract data that explains relationships across rows as opposed to columns, e.g.:
+# * Running averages
+# * Time until next event
+# * Time since last event
+#
+# This is often difficult to do with most table manipulation frameworks, since they are designed to work with relationships across columns. As such, we've created a class to handle this type of data.
+#
+# We'll define a function `get_elapsed` for cumulative counting across a sorted dataframe. Given a particular field `fld` to monitor, this function will start tracking time since the last occurrence of that field. When the field is seen again, the counter is set to zero.
+#
+# Upon initialization, this will result in datetime na's until the field is encountered. This is reset every time a new store is seen. We'll see how to use this shortly.
+
+def get_elapsed(fld, pre):
+    day1 = np.timedelta64(1, 'D')
+    last_date = np.datetime64()
+    last_store = 0
+    res = []
+
+    for s,v,d in zip(df.Store.values,df[fld].values, df.Date.values):
+        if s != last_store:
+            last_date = np.datetime64()
+            last_store = s
+        if v: last_date = d
+        res.append(((d-last_date).astype('timedelta64[D]') / day1))
+    df[pre+fld] = res
+
+
+# We'll be applying this to a subset of columns:
+
+columns = ["Date", "Store", "Promo", "StateHoliday", "SchoolHoliday"]
+
+#df = train[columns]
+df = train[columns].append(test[columns])
+
+# Let's walk through an example.
+#
+# Say we're looking at School Holiday. We'll first sort by Store, then Date, and then call `add_elapsed('SchoolHoliday', 'After')`:
+# This will apply to each row with School Holiday:
+# * A applied to every row of the dataframe in order of store and date
+# * Will add to the dataframe the days since seeing a School Holiday
+# * If we sort in the other direction, this will count the days until another holiday.
+
+fld = 'SchoolHoliday'
+df = df.sort_values(['Store', 'Date'])
+get_elapsed(fld, 'After')
+df = df.sort_values(['Store', 'Date'], ascending=[True, False])
+get_elapsed(fld, 'Before')
+
+# We'll do this for two more fields.
+
+fld = 'StateHoliday'
+df = df.sort_values(['Store', 'Date'])
+get_elapsed(fld, 'After')
+df = df.sort_values(['Store', 'Date'], ascending=[True, False])
+get_elapsed(fld, 'Before')
+
+fld = 'Promo'
+df = df.sort_values(['Store', 'Date'])
+get_elapsed(fld, 'After')
+df = df.sort_values(['Store', 'Date'], ascending=[True, False])
+get_elapsed(fld, 'Before')
+
+# We're going to set the active index to Date.
+
+df = df.set_index("Date")
+
+# Then set null values from elapsed field calculations to 0.
+
+columns = ['SchoolHoliday', 'StateHoliday', 'Promo']
+
+for o in ['Before', 'After']:
+    for p in columns:
+        a = o+p
+        df[a] = df[a].fillna(0).astype(int)
+
+# Next we'll demonstrate window functions in pandas to calculate rolling quantities.
+#
+# Here we're sorting by date (`sort_index()`) and counting the number of events of interest (`sum()`) defined in `columns` in the following week (`rolling()`), grouped by Store (`groupby()`). We do the same in the opposite direction.
+
+bwd = df[['Store']+columns].sort_index().groupby("Store").rolling(7, min_periods=1).sum()
+
+fwd = df[['Store']+columns].sort_index(ascending=False
+                                      ).groupby("Store").rolling(7, min_periods=1).sum()
+
+# Next we want to drop the Store indices grouped together in the window function.
+#
+# Often in pandas, there is an option to do this in place. This is time and memory efficient when working with large datasets.
+
+bwd.drop('Store',1,inplace=True)
+bwd.reset_index(inplace=True)
+
+fwd.drop('Store',1,inplace=True)
+fwd.reset_index(inplace=True)
+
+df.reset_index(inplace=True)
+
+# Now we'll merge these values onto the df.
+
+df = df.merge(bwd, 'left', ['Date', 'Store'], suffixes=['', '_bw'])
+df = df.merge(fwd, 'left', ['Date', 'Store'], suffixes=['', '_fw'])
+
+df.drop(columns,1,inplace=True)
+
+df.head()
+
+# It's usually a good idea to back up large tables of extracted / wrangled features before you join them onto another one, that way you can go back to it easily if you need to make changes to it.
+
+df.to_pickle(PATH/'df')
+
+df["Date"] = pd.to_datetime(df.Date)
+
+df.columns
+
+joined = pd.read_pickle(PATH/'joined')
+joined_test = pd.read_pickle(PATH/f'joined_test')
+
+joined = join_df(joined, df, ['Store', 'Date'])
+
+joined_test = join_df(joined_test, df, ['Store', 'Date'])
+
+# The authors also removed all instances where the store had zero sale / was closed. We speculate that this may have cost them a higher standing in the competition. One reason this may be the case is that a little exploratory data analysis reveals that there are often periods where stores are closed, typically for refurbishment. Before and after these periods, there are naturally spikes in sales that one might expect. By ommitting this data from their training, the authors gave up the ability to leverage information about these periods to predict this otherwise volatile behavior.
+
+joined = joined[joined.Sales!=0]
+
+# We'll back this up as well.
+
+joined.reset_index(inplace=True)
+joined_test.reset_index(inplace=True)
+
+joined.to_pickle(PATH/'train_clean')
+joined_test.to_pickle(PATH/'test_clean')
+
+
diff --git a/nbs/dl2/00_exports.ipynb b/nbs/dl2/00_exports.ipynb
index 3e065bd..d91ca48 100644
--- ./nbs/dl2/00_exports.ipynb
+++ ./nbs/dl2/00_exports.ipynb
@@ -61,6 +61,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/00_exports.py b/nbs/dl2/00_exports.py
new file mode 100644
index 0000000..3c19a6c
--- /dev/null
+++ ./nbs/dl2/00_exports.py
@@ -0,0 +1,30 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+#export
+TEST = 'test'
+
+# ## Export
+
+# !python notebook2script.py 00_exports.ipynb
+
+# ### How it works:
+
+import json
+d = json.load(open('00_exports.ipynb','r'))['cells']
+
+d[0]
+
+
diff --git a/nbs/dl2/01_matmul.ipynb b/nbs/dl2/01_matmul.ipynb
index c9d5995..ee91e3f 100644
--- ./nbs/dl2/01_matmul.ipynb
+++ ./nbs/dl2/01_matmul.ipynb
@@ -1641,6 +1641,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/01_matmul.py b/nbs/dl2/01_matmul.py
new file mode 100644
index 0000000..81e9b6e
--- /dev/null
+++ ./nbs/dl2/01_matmul.py
@@ -0,0 +1,361 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# ## Matrix multiplication from foundations
+
+# The *foundations* we'll assume throughout this course are:
+#
+# - Python
+# - Python modules (non-DL)
+# - pytorch indexable tensor, and tensor creation (including RNGs - random number generators)
+# - fastai.datasets
+
+# ## Check imports
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+# [Jump_to lesson 8 video](https://course.fast.ai/videos/?lesson=8&t=1850)
+
+# +
+#export
+from exp.nb_00 import *
+import operator
+
+def test(a,b,cmp,cname=None):
+    if cname is None: cname=cmp.__name__
+    assert cmp(a,b),f"{cname}:\n{a}\n{b}"
+
+def test_eq(a,b): test(a,b,operator.eq,'==')
+
+
+# -
+
+test_eq(TEST,'test')
+
+# +
+# To run tests in console:
+# # ! python run_notebook.py 01_matmul.ipynb
+# -
+
+# ## Get data
+
+# [Jump_to lesson 8 video](https://course.fast.ai/videos/?lesson=8&t=2159)
+
+# +
+#export
+from pathlib import Path
+from IPython.core.debugger import set_trace
+from fastai import datasets
+import pickle, gzip, math, torch, matplotlib as mpl
+import matplotlib.pyplot as plt
+from torch import tensor
+
+MNIST_URL='http://deeplearning.net/data/mnist/mnist.pkl'
+# -
+
+path = datasets.download_data(MNIST_URL, ext='.gz'); path
+
+with gzip.open(path, 'rb') as f:
+    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')
+
+x_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))
+n,c = x_train.shape
+x_train, x_train.shape, y_train, y_train.shape, y_train.min(), y_train.max()
+
+assert n==y_train.shape[0]==50000
+test_eq(c,28*28)
+test_eq(y_train.min(),0)
+test_eq(y_train.max(),9)
+
+mpl.rcParams['image.cmap'] = 'gray'
+
+img = x_train[0]
+
+img.view(28,28).type()
+
+plt.imshow(img.view((28,28)));
+
+# ## Initial python model
+
+#  [Jump_to lesson 8 video](https://course.fast.ai/videos/?lesson=8&t=2342)
+
+#  [Jump_to lesson 8 video](https://course.fast.ai/videos/?lesson=8&t=2342)
+
+weights = torch.randn(784,10)
+
+bias = torch.zeros(10)
+
+
+# #### Matrix multiplication
+
+def matmul(a,b):
+    ar,ac = a.shape # n_rows * n_cols
+    br,bc = b.shape
+    assert ac==br
+    c = torch.zeros(ar, bc)
+    for i in range(ar):
+        for j in range(bc):
+            for k in range(ac): # or br
+                c[i,j] += a[i,k] * b[k,j]
+    return c
+
+
+m1 = x_valid[:5]
+m2 = weights
+
+m1.shape,m2.shape
+
+# %time t1=matmul(m1, m2)
+
+t1.shape
+
+# This is kinda slow - what if we could speed it up by 50,000 times? Let's try!
+
+len(x_train)
+
+# #### Elementwise ops
+
+# Operators (+,-,\*,/,>,<,==) are usually element-wise.
+#
+# Examples of element-wise operations:
+
+#  [Jump_to lesson 8 video](https://course.fast.ai/videos/?lesson=8&t=2682)
+
+a = tensor([10., 6, -4])
+b = tensor([2., 8, 7])
+a,b
+
+a + b
+
+(a < b).float().mean()
+
+m = tensor([[1., 2, 3], [4,5,6], [7,8,9]]); m
+
+# Frobenius norm:
+#
+# $$\| A \|_F = \left( \sum_{i,j=1}^n | a_{ij} |^2 \right)^{1/2}$$
+#
+# *Hint*: you don't normally need to write equations in LaTeX yourself, instead, you can click 'edit' in Wikipedia and copy the LaTeX from there (which is what I did for the above equation). Or on arxiv.org, click "Download: Other formats" in the top right, then "Download source"; rename the downloaded file to end in `.tgz` if it doesn't already, and you should find the source there, including the equations to copy and paste.
+
+(m*m).sum().sqrt()
+
+
+# #### Elementwise matmul
+
+def matmul(a,b):
+    ar,ac = a.shape
+    br,bc = b.shape
+    assert ac==br
+    c = torch.zeros(ar, bc)
+    for i in range(ar):
+        for j in range(bc):
+            # Any trailing ",:" can be removed
+            c[i,j] = (a[i,:] * b[:,j]).sum()
+    return c
+
+
+# %timeit -n 10 _=matmul(m1, m2)
+
+890.1/5
+
+
+#export
+def near(a,b): return torch.allclose(a, b, rtol=1e-3, atol=1e-5)
+def test_near(a,b): test(a,b,near)
+
+
+test_near(t1,matmul(m1, m2))
+
+# ### Broadcasting
+
+# The term **broadcasting** describes how arrays with different shapes are treated during arithmetic operations.  The term broadcasting was first used by Numpy.
+#
+# From the [Numpy Documentation](https://docs.scipy.org/doc/numpy-1.10.0/user/basics.broadcasting.html):
+#
+#     The term broadcasting describes how numpy treats arrays with 
+#     different shapes during arithmetic operations. Subject to certain 
+#     constraints, the smaller array is “broadcast” across the larger 
+#     array so that they have compatible shapes. Broadcasting provides a 
+#     means of vectorizing array operations so that looping occurs in C
+#     instead of Python. It does this without making needless copies of 
+#     data and usually leads to efficient algorithm implementations.
+#     
+# In addition to the efficiency of broadcasting, it allows developers to write less code, which typically leads to fewer errors.
+#
+# *This section was adapted from [Chapter 4](http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/4.%20Compressed%20Sensing%20of%20CT%20Scans%20with%20Robust%20Regression.ipynb#4.-Compressed-Sensing-of-CT-Scans-with-Robust-Regression) of the fast.ai [Computational Linear Algebra](https://github.com/fastai/numerical-linear-algebra) course.*
+
+# [Jump_to lesson 8 video](https://course.fast.ai/videos/?lesson=8&t=3110)
+
+# #### Broadcasting with a scalar
+
+a
+
+a > 0
+
+# How are we able to do a > 0?  0 is being **broadcast** to have the same dimensions as a.
+#
+# For instance you can normalize our dataset by subtracting the mean (a scalar) from the entire data set (a matrix) and dividing by the standard deviation (another scalar), using broadcasting.
+#
+# Other examples of broadcasting with a scalar:
+
+a + 1
+
+m
+
+2*m
+
+# #### Broadcasting a vector to a matrix
+
+# We can also broadcast a vector to a matrix:
+
+c = tensor([10.,20,30]); c
+
+m
+
+m.shape,c.shape
+
+m + c
+
+c + m
+
+# We don't really copy the rows, but it looks as if we did. In fact, the rows are given a *stride* of 0.
+
+t = c.expand_as(m)
+
+t
+
+m + t
+
+t.storage()
+
+t.stride(), t.shape
+
+# You can index with the special value [None] or use `unsqueeze()` to convert a 1-dimensional array into a 2-dimensional array (although one of those dimensions has value 1).
+
+c.unsqueeze(0)
+
+c.unsqueeze(1)
+
+m
+
+c.shape, c.unsqueeze(0).shape,c.unsqueeze(1).shape
+
+c.shape, c[None].shape,c[:,None].shape
+
+# You can always skip trailling ':'s. And '...' means '*all preceding dimensions*'
+
+c[None].shape,c[...,None].shape
+
+c[:,None].expand_as(m)
+
+m + c[:,None]
+
+c[:,None]
+
+
+# #### Matmul with broadcasting
+
+def matmul(a,b):
+    ar,ac = a.shape
+    br,bc = b.shape
+    assert ac==br
+    c = torch.zeros(ar, bc)
+    for i in range(ar):
+#       c[i,j] = (a[i,:]          * b[:,j]).sum() # previous
+        c[i]   = (a[i  ].unsqueeze(-1) * b).sum(dim=0)
+    return c
+
+
+# %timeit -n 10 _=matmul(m1, m2)
+
+885000/277
+
+test_near(t1, matmul(m1, m2))
+
+# #### Broadcasting Rules
+
+c[None,:]
+
+c[None,:].shape
+
+c[:,None]
+
+c[:,None].shape
+
+c[None,:] * c[:,None]
+
+c[None] > c[:,None]
+
+
+# When operating on two arrays/tensors, Numpy/PyTorch compares their shapes element-wise. It starts with the **trailing dimensions**, and works its way forward. Two dimensions are **compatible** when
+#
+# - they are equal, or
+# - one of them is 1, in which case that dimension is broadcasted to make it the same size
+#
+# Arrays do not need to have the same number of dimensions. For example, if you have a `256*256*3` array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:
+#
+#     Image  (3d array): 256 x 256 x 3
+#     Scale  (1d array):             3
+#     Result (3d array): 256 x 256 x 3
+#
+# The [numpy documentation](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html#general-broadcasting-rules) includes several examples of what dimensions can and can not be broadcast together.
+
+# ### Einstein summation
+
+# Einstein summation (`einsum`) is a compact representation for combining products and sums in a general way. From the numpy docs:
+#
+# "The subscripts string is a comma-separated list of subscript labels, where each label refers to a dimension of the corresponding operand. Whenever a label is repeated it is summed, so `np.einsum('i,i', a, b)` is equivalent to `np.inner(a,b)`. If a label appears only once, it is not summed, so `np.einsum('i', a)` produces a view of a with no changes."
+
+# [Jump_to lesson 8 video](https://course.fast.ai/videos/?lesson=8&t=4280)
+
+# c[i,j] += a[i,k] * b[k,j]
+# c[i,j] = (a[i,:] * b[:,j]).sum()
+def matmul(a,b): return torch.einsum('ik,kj->ij', a, b)
+
+
+# %timeit -n 10 _=matmul(m1, m2)
+
+885000/55
+
+test_near(t1, matmul(m1, m2))
+
+# ### pytorch op
+
+# We can use pytorch's function or operator directly for matrix multiplication.
+
+# [Jump_to lesson 8 video](https://course.fast.ai/videos/?lesson=8&t=4702)
+
+# %timeit -n 10 t2 = m1.matmul(m2)
+
+# time comparison vs pure python:
+885000/18
+
+t2 = m1@m2
+
+test_near(t1, t2)
+
+m1.shape,m2.shape
+
+# ## Export
+
+# !python notebook2script.py 01_matmul.ipynb
+
+
diff --git a/nbs/dl2/02_fully_connected.ipynb b/nbs/dl2/02_fully_connected.ipynb
index 0f225ac..a11df3e 100644
--- ./nbs/dl2/02_fully_connected.ipynb
+++ ./nbs/dl2/02_fully_connected.ipynb
@@ -1368,6 +1368,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/02_fully_connected.py b/nbs/dl2/02_fully_connected.py
new file mode 100644
index 0000000..fadea53
--- /dev/null
+++ ./nbs/dl2/02_fully_connected.py
@@ -0,0 +1,460 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+# ## The forward and backward passes
+
+# [Jump_to lesson 8 video](https://course.fast.ai/videos/?lesson=8&t=4960)
+
+# +
+#export
+from exp.nb_01 import *
+
+def get_data():
+    path = datasets.download_data(MNIST_URL, ext='.gz')
+    with gzip.open(path, 'rb') as f:
+        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')
+    return map(tensor, (x_train,y_train,x_valid,y_valid))
+
+def normalize(x, m, s): return (x-m)/s
+
+
+# -
+
+x_train,y_train,x_valid,y_valid = get_data()
+
+train_mean,train_std = x_train.mean(),x_train.std()
+train_mean,train_std
+
+x_train = normalize(x_train, train_mean, train_std)
+# NB: Use training, not validation mean for validation set
+x_valid = normalize(x_valid, train_mean, train_std)
+
+train_mean,train_std = x_train.mean(),x_train.std()
+train_mean,train_std
+
+
+#export
+def test_near_zero(a,tol=1e-3): assert a.abs()<tol, f"Near zero: {a}"
+
+
+test_near_zero(x_train.mean())
+test_near_zero(1-x_train.std())
+
+n,m = x_train.shape
+c = y_train.max()+1
+n,m,c
+
+# ## Foundations version
+
+# ### Basic architecture
+
+# [Jump_to lesson 8 video](https://course.fast.ai/videos/?lesson=8&t=5128)
+
+# num hidden
+nh = 50
+
+# [Tinker practice](https://course.fast.ai/videos/?lesson=8&t=5255)
+
+# standard xavier init
+w1 = torch.randn(m,nh)/math.sqrt(m)
+b1 = torch.zeros(nh)
+w2 = torch.randn(nh,1)/math.sqrt(nh)
+b2 = torch.zeros(1)
+
+test_near_zero(w1.mean())
+test_near_zero(w1.std()-1/math.sqrt(m))
+
+# This should be ~ (0,1) (mean,std)...
+x_valid.mean(),x_valid.std()
+
+
+def lin(x, w, b): return x@w + b
+
+
+t = lin(x_valid, w1, b1)
+
+#...so should this, because we used xavier init, which is designed to do this
+t.mean(),t.std()
+
+
+def relu(x): return x.clamp_min(0.)
+
+
+t = relu(lin(x_valid, w1, b1))
+
+#...actually it really should be this!
+t.mean(),t.std()
+
+# From pytorch docs: `a: the negative slope of the rectifier used after this layer (0 for ReLU by default)`
+#
+# $$\text{std} = \sqrt{\frac{2}{(1 + a^2) \times \text{fan_in}}}$$
+#
+# This was introduced in the paper that described the Imagenet-winning approach from *He et al*: [Delving Deep into Rectifiers](https://arxiv.org/abs/1502.01852), which was also the first paper that claimed "super-human performance" on Imagenet (and, most importantly, it introduced resnets!)
+
+# [Jump_to lesson 8 video](https://course.fast.ai/videos/?lesson=8&t=5128)
+
+# kaiming init / he init for relu
+w1 = torch.randn(m,nh)*math.sqrt(2/m)
+
+w1.mean(),w1.std()
+
+t = relu(lin(x_valid, w1, b1))
+t.mean(),t.std()
+
+#export
+from torch.nn import init
+
+w1 = torch.zeros(m,nh)
+init.kaiming_normal_(w1, mode='fan_out')
+t = relu(lin(x_valid, w1, b1))
+
+# +
+# init.kaiming_normal_??
+# -
+
+w1.mean(),w1.std()
+
+t.mean(),t.std()
+
+w1.shape
+
+import torch.nn
+
+torch.nn.Linear(m,nh).weight.shape
+
+
+# +
+# torch.nn.Linear.forward??
+
+# +
+# torch.nn.functional.linear??
+
+# +
+# torch.nn.Conv2d??
+
+# +
+# torch.nn.modules.conv._ConvNd.reset_parameters??
+# -
+
+# what if...?
+def relu(x): return x.clamp_min(0.) - 0.5
+
+
+# kaiming init / he init for relu
+w1 = torch.randn(m,nh)*math.sqrt(2./m )
+t1 = relu(lin(x_valid, w1, b1))
+t1.mean(),t1.std()
+
+
+def model(xb):
+    l1 = lin(xb, w1, b1)
+    l2 = relu(l1)
+    l3 = lin(l2, w2, b2)
+    return l3
+
+
+# %timeit -n 10 _=model(x_valid)
+
+assert model(x_valid).shape==torch.Size([x_valid.shape[0],1])
+
+# ### Loss function: MSE
+
+# [Jump_to lesson 8 video](https://course.fast.ai/videos/?lesson=8&t=6372)
+
+model(x_valid).shape
+
+
+# We need `squeeze()` to get rid of that trailing (,1), in order to use `mse`. (Of course, `mse` is not a suitable loss function for multi-class classification; we'll use a better loss function soon. We'll use `mse` for now to keep things simple.)
+
+#export
+def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()
+
+
+y_train,y_valid = y_train.float(),y_valid.float()
+
+preds = model(x_train)
+
+preds.shape
+
+mse(preds, y_train)
+
+
+# ### Gradients and backward pass
+
+# [Jump_to lesson 8 video](https://course.fast.ai/videos/?lesson=8&t=6493)
+
+def mse_grad(inp, targ): 
+    # grad of loss with respect to output of previous layer
+    inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]
+
+
+def relu_grad(inp, out):
+    # grad of relu with respect to input activations
+    inp.g = (inp>0).float() * out.g
+
+
+def lin_grad(inp, out, w, b):
+    # grad of matmul with respect to input
+    inp.g = out.g @ w.t()
+    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)
+    b.g = out.g.sum(0)
+
+
+def forward_and_backward(inp, targ):
+    # forward pass:
+    l1 = inp @ w1 + b1
+    l2 = relu(l1)
+    out = l2 @ w2 + b2
+    # we don't actually need the loss in backward!
+    loss = mse(out, targ)
+    
+    # backward pass:
+    mse_grad(out, targ)
+    lin_grad(l2, out, w2, b2)
+    relu_grad(l1, l2)
+    lin_grad(inp, l1, w1, b1)
+
+
+forward_and_backward(x_train, y_train)
+
+# Save for testing against later
+w1g = w1.g.clone()
+w2g = w2.g.clone()
+b1g = b1.g.clone()
+b2g = b2.g.clone()
+ig  = x_train.g.clone()
+
+# We cheat a little bit and use PyTorch autograd to check our results.
+
+xt2 = x_train.clone().requires_grad_(True)
+w12 = w1.clone().requires_grad_(True)
+w22 = w2.clone().requires_grad_(True)
+b12 = b1.clone().requires_grad_(True)
+b22 = b2.clone().requires_grad_(True)
+
+
+def forward(inp, targ):
+    # forward pass:
+    l1 = inp @ w12 + b12
+    l2 = relu(l1)
+    out = l2 @ w22 + b22
+    # we don't actually need the loss in backward!
+    return mse(out, targ)
+
+
+loss = forward(xt2, y_train)
+
+loss.backward()
+
+test_near(w22.grad, w2g)
+test_near(b22.grad, b2g)
+test_near(w12.grad, w1g)
+test_near(b12.grad, b1g)
+test_near(xt2.grad, ig )
+
+
+# ## Refactor model
+
+# ### Layers as classes
+
+# [Jump_to lesson 8 video](https://course.fast.ai/videos/?lesson=8&t=7112)
+
+class Relu():
+    def __call__(self, inp):
+        self.inp = inp
+        self.out = inp.clamp_min(0.)-0.5
+        return self.out
+    
+    def backward(self): self.inp.g = (self.inp>0).float() * self.out.g
+
+
+class Lin():
+    def __init__(self, w, b): self.w,self.b = w,b
+        
+    def __call__(self, inp):
+        self.inp = inp
+        self.out = inp@self.w + self.b
+        return self.out
+    
+    def backward(self):
+        self.inp.g = self.out.g @ self.w.t()
+        # Creating a giant outer product, just to sum it, is inefficient!
+        self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0)
+        self.b.g = self.out.g.sum(0)
+
+
+class Mse():
+    def __call__(self, inp, targ):
+        self.inp = inp
+        self.targ = targ
+        self.out = (inp.squeeze() - targ).pow(2).mean()
+        return self.out
+    
+    def backward(self):
+        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]
+
+
+class Model():
+    def __init__(self, w1, b1, w2, b2):
+        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]
+        self.loss = Mse()
+        
+    def __call__(self, x, targ):
+        for l in self.layers: x = l(x)
+        return self.loss(x, targ)
+    
+    def backward(self):
+        self.loss.backward()
+        for l in reversed(self.layers): l.backward()
+
+
+w1.g,b1.g,w2.g,b2.g = [None]*4
+model = Model(w1, b1, w2, b2)
+
+# %time loss = model(x_train, y_train)
+
+# %time model.backward()
+
+test_near(w2g, w2.g)
+test_near(b2g, b2.g)
+test_near(w1g, w1.g)
+test_near(b1g, b1.g)
+test_near(ig, x_train.g)
+
+
+# ### Module.forward()
+
+class Module():
+    def __call__(self, *args):
+        self.args = args
+        self.out = self.forward(*args)
+        return self.out
+    
+    def forward(self): raise Exception('not implemented')
+    def backward(self): self.bwd(self.out, *self.args)
+
+
+class Relu(Module):
+    def forward(self, inp): return inp.clamp_min(0.)-0.5
+    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g
+
+
+class Lin(Module):
+    def __init__(self, w, b): self.w,self.b = w,b
+        
+    def forward(self, inp): return inp@self.w + self.b
+    
+    def bwd(self, out, inp):
+        inp.g = out.g @ self.w.t()
+        self.w.g = torch.einsum("bi,bj->ij", inp, out.g)
+        self.b.g = out.g.sum(0)
+
+
+class Mse(Module):
+    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()
+    def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]
+
+
+class Model():
+    def __init__(self):
+        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]
+        self.loss = Mse()
+        
+    def __call__(self, x, targ):
+        for l in self.layers: x = l(x)
+        return self.loss(x, targ)
+    
+    def backward(self):
+        self.loss.backward()
+        for l in reversed(self.layers): l.backward()
+
+
+w1.g,b1.g,w2.g,b2.g = [None]*4
+model = Model()
+
+# %time loss = model(x_train, y_train)
+
+# %time model.backward()
+
+test_near(w2g, w2.g)
+test_near(b2g, b2.g)
+test_near(w1g, w1.g)
+test_near(b1g, b1.g)
+test_near(ig, x_train.g)
+
+
+# ### Without einsum
+
+# [Jump_to lesson 8 video](https://course.fast.ai/videos/?lesson=8&t=7484)
+
+class Lin(Module):
+    def __init__(self, w, b): self.w,self.b = w,b
+        
+    def forward(self, inp): return inp@self.w + self.b
+    
+    def bwd(self, out, inp):
+        inp.g = out.g @ self.w.t()
+        self.w.g = inp.t() @ out.g
+        self.b.g = out.g.sum(0)
+
+
+w1.g,b1.g,w2.g,b2.g = [None]*4
+model = Model()
+
+# %time loss = model(x_train, y_train)
+
+# %time model.backward()
+
+test_near(w2g, w2.g)
+test_near(b2g, b2.g)
+test_near(w1g, w1.g)
+test_near(b1g, b1.g)
+test_near(ig, x_train.g)
+
+# ### nn.Linear and nn.Module
+
+#export
+from torch import nn
+
+
+class Model(nn.Module):
+    def __init__(self, n_in, nh, n_out):
+        super().__init__()
+        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]
+        self.loss = mse
+        
+    def __call__(self, x, targ):
+        for l in self.layers: x = l(x)
+        return self.loss(x.squeeze(), targ)
+
+
+model = Model(m, nh, 1)
+
+# %time loss = model(x_train, y_train)
+
+# %time loss.backward()
+
+# ## Export
+
+!./notebook2script.py 02_fully_connected.ipynb
+
+
diff --git a/nbs/dl2/02a_why_sqrt5.ipynb b/nbs/dl2/02a_why_sqrt5.ipynb
index 1974190..09d44b7 100644
--- ./nbs/dl2/02a_why_sqrt5.ipynb
+++ ./nbs/dl2/02a_why_sqrt5.ipynb
@@ -708,6 +708,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/02a_why_sqrt5.py b/nbs/dl2/02a_why_sqrt5.py
new file mode 100644
index 0000000..b806a16
--- /dev/null
+++ ./nbs/dl2/02a_why_sqrt5.py
@@ -0,0 +1,174 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+# ## Does nn.Conv2d init work well?
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=21)
+
+# +
+#export
+from exp.nb_02 import *
+
+def get_data():
+    path = datasets.download_data(MNIST_URL, ext='.gz')
+    with gzip.open(path, 'rb') as f:
+        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')
+    return map(tensor, (x_train,y_train,x_valid,y_valid))
+
+def normalize(x, m, s): return (x-m)/s
+
+
+# +
+# torch.nn.modules.conv._ConvNd.reset_parameters??
+# -
+
+x_train,y_train,x_valid,y_valid = get_data()
+train_mean,train_std = x_train.mean(),x_train.std()
+x_train = normalize(x_train, train_mean, train_std)
+x_valid = normalize(x_valid, train_mean, train_std)
+
+x_train = x_train.view(-1,1,28,28)
+x_valid = x_valid.view(-1,1,28,28)
+x_train.shape,x_valid.shape
+
+n,*_ = x_train.shape
+c = y_train.max()+1
+nh = 32
+n,c
+
+l1 = nn.Conv2d(1, nh, 5)
+
+x = x_valid[:100]
+
+x.shape
+
+
+def stats(x): return x.mean(),x.std()
+
+
+l1.weight.shape
+
+stats(l1.weight),stats(l1.bias)
+
+t = l1(x)
+
+stats(t)
+
+init.kaiming_normal_(l1.weight, a=1.)
+stats(l1(x))
+
+import torch.nn.functional as F
+
+
+def f1(x,a=0): return F.leaky_relu(l1(x),a)
+
+
+init.kaiming_normal_(l1.weight, a=0)
+stats(f1(x))
+
+l1 = nn.Conv2d(1, nh, 5)
+stats(f1(x))
+
+l1.weight.shape
+
+# receptive field size
+rec_fs = l1.weight[0,0].numel()
+rec_fs
+
+nf,ni,*_ = l1.weight.shape
+nf,ni
+
+fan_in  = ni*rec_fs
+fan_out = nf*rec_fs
+fan_in,fan_out
+
+
+def gain(a): return math.sqrt(2.0 / (1 + a**2))
+
+
+gain(1),gain(0),gain(0.01),gain(0.1),gain(math.sqrt(5.))
+
+torch.zeros(10000).uniform_(-1,1).std()
+
+1/math.sqrt(3.)
+
+
+def kaiming2(x,a, use_fan_out=False):
+    nf,ni,*_ = x.shape
+    rec_fs = x[0,0].shape.numel()
+    fan = nf*rec_fs if use_fan_out else ni*rec_fs
+    std = gain(a) / math.sqrt(fan)
+    bound = math.sqrt(3.) * std
+    x.data.uniform_(-bound,bound)
+
+
+kaiming2(l1.weight, a=0);
+stats(f1(x))
+
+kaiming2(l1.weight, a=math.sqrt(5.))
+stats(f1(x))
+
+
+class Flatten(nn.Module):
+    def forward(self,x): return x.view(-1)
+
+
+m = nn.Sequential(
+    nn.Conv2d(1,8, 5,stride=2,padding=2), nn.ReLU(),
+    nn.Conv2d(8,16,3,stride=2,padding=1), nn.ReLU(),
+    nn.Conv2d(16,32,3,stride=2,padding=1), nn.ReLU(),
+    nn.Conv2d(32,1,3,stride=2,padding=1),
+    nn.AdaptiveAvgPool2d(1),
+    Flatten(),
+)
+
+y = y_valid[:100].float()
+
+t = m(x)
+stats(t)
+
+l = mse(t,y)
+l.backward()
+
+stats(m[0].weight.grad)
+
+# +
+# init.kaiming_uniform_??
+# -
+
+for l in m:
+    if isinstance(l,nn.Conv2d):
+        init.kaiming_uniform_(l.weight)
+        l.bias.data.zero_()
+
+t = m(x)
+stats(t)
+
+l = mse(t,y)
+l.backward()
+stats(m[0].weight.grad)
+
+# ## Export
+
+!./notebook2script.py 02a_why_sqrt5.ipynb
+
+
diff --git a/nbs/dl2/02b_initializing.ipynb b/nbs/dl2/02b_initializing.ipynb
index 78e41db..977f85b 100644
--- ./nbs/dl2/02b_initializing.ipynb
+++ ./nbs/dl2/02b_initializing.ipynb
@@ -510,6 +510,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/02b_initializing.py b/nbs/dl2/02b_initializing.py
new file mode 100644
index 0000000..eabb583
--- /dev/null
+++ ./nbs/dl2/02b_initializing.py
@@ -0,0 +1,159 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+import torch
+
+# ### Why you need a good init
+
+# To understand why initialization is important in a neural net, we'll focus on the basic operation you have there: matrix multiplications. So let's just take a vector `x`, and a matrix `a` initiliazed randomly, then multiply them 100 times (as if we had 100 layers). 
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=1132)
+
+x = torch.randn(512)
+a = torch.randn(512,512)
+
+for i in range(100): x = a @ x
+
+x.mean(),x.std()
+
+# The problem you'll get with that is activation explosion: very soon, your activations will go to nan. We can even ask the loop to break when that first happens:
+
+x = torch.randn(512)
+a = torch.randn(512,512)
+
+for i in range(100): 
+    x = a @ x
+    if x.std() != x.std(): break
+
+i
+
+# It only takes 27 multiplications! On the other hand, if you initialize your activations with a scale that is too low, then you'll get another problem:
+
+x = torch.randn(512)
+a = torch.randn(512,512) * 0.01
+
+for i in range(100): x = a @ x
+
+x.mean(),x.std()
+
+# Here, every activation vanished to 0. So to avoid that problem, people have come with several strategies to initialize their weight matices, such as:
+# - use a standard deviation that will make sure x and Ax have exactly the same scale
+# - use an orthogonal matrix to initialize the weight (orthogonal matrices have the special property that they preserve the L2 norm, so x and Ax would have the same sum of squares in that case)
+# - use [spectral normalization](https://arxiv.org/pdf/1802.05957.pdf) on the matrix A  (the spectral norm of A is the least possible number M such that `torch.norm(A@x) <= M*torch.norm(x)` so dividing A by this M insures you don't overflow. You can still vanish with this)
+
+# ### The magic number for scaling
+
+# Here we will focus on the first one, which is the Xavier initialization. It tells us that we should use a scale equal to `1/math.sqrt(n_in)` where `n_in` is the number of inputs of our matrix.
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=1273)
+
+import math
+
+x = torch.randn(512)
+a = torch.randn(512,512) / math.sqrt(512)
+
+for i in range(100): x = a @ x
+
+x.mean(),x.std()
+
+# And indeed it works. Note that this magic number isn't very far from the 0.01 we had earlier.
+
+1/ math.sqrt(512)
+
+# But where does it come from? It's not that mysterious if you remember the definition of the matrix multiplication. When we do `y = a @ x`, the coefficients of `y` are defined by
+#
+# $$y_{i} = a_{i,0} x_{0} + a_{i,1} x_{1} + \cdots + a_{i,n-1} x_{n-1} = \sum_{k=0}^{n-1} a_{i,k} x_{k}$$
+#
+# or in code:
+# ```
+# y[i] = sum([c*d for c,d in zip(a[i], x)])
+# ```
+#
+# Now at the very beginning, our `x` vector has a mean of roughly 0. and a standard deviation of roughly 1. (since we picked it that way).
+
+x = torch.randn(512)
+x.mean(), x.std()
+
+# NB: This is why it's extremely important to normalize your inputs in Deep Learning, the intialization rules have been designed with inputs that have a mean 0. and a standard deviation of 1.
+#
+# If you need a refresher from your statistics course, the mean is the sum of all the elements divided by the number of elements (a basic average). The standard deviation represents if the data stays close to the mean or on the contrary gets values that are far away. It's computed by the following formula:
+#
+# $$\sigma = \sqrt{\frac{1}{n}\left[(x_{0}-m)^{2} + (x_{1}-m)^{2} + \cdots + (x_{n-1}-m)^{2}\right]}$$
+#
+# where m is the mean and $\sigma$ (the greek letter sigma) is the standard deviation. Here we have a mean of 0, so it's just the square root of the mean of x squared.
+#
+# If we go back to `y = a @ x` and assume that we chose weights for `a` that also have a mean of 0, we can compute the standard deviation of `y` quite easily. Since it's random, and we may fall on bad numbers, we repeat the operation 100 times.
+
+mean,sqr = 0.,0.
+for i in range(100):
+    x = torch.randn(512)
+    a = torch.randn(512, 512)
+    y = a @ x
+    mean += y.mean().item()
+    sqr  += y.pow(2).mean().item()
+mean/100,sqr/100
+
+# Now that looks very close to the dimension of our matrix 512. And that's no coincidence! When you compute y, you sum 512 product of one element of a by one element of x. So what's the mean and the standard deviation of such a product? We can show mathematically that as long as the elements in `a` and the elements in `x` are independent, the mean is 0 and the std is 1. This can also be seen experimentally:
+
+mean,sqr = 0.,0.
+for i in range(10000):
+    x = torch.randn(1)
+    a = torch.randn(1)
+    y = a*x
+    mean += y.item()
+    sqr  += y.pow(2).item()
+mean/10000,sqr/10000
+
+# Then we sum 512 of those things that have a mean of zero, and a mean of squares of 1, so we get something that has a mean of 0, and mean of square of 512, hence `math.sqrt(512)` being our magic number. If we scale the weights of the matrix `a` and divide them by this `math.sqrt(512)`, it will give us a `y` of scale 1, and repeating the product has many times as we want won't overflow or vanish.
+
+# ### Adding ReLU in the mix
+
+# We can reproduce the previous experiment with a ReLU, to see that this time, the mean shifts and the standard deviation becomes 0.5. This time the magic number will be `math.sqrt(2/512)` to properly scale the weights of the matrix.
+
+mean,sqr = 0.,0.
+for i in range(10000):
+    x = torch.randn(1)
+    a = torch.randn(1)
+    y = a*x
+    y = 0 if y < 0 else y.item()
+    mean += y
+    sqr  += y ** 2
+mean/10000,sqr/10000
+
+# We can double check by running the experiment on the whole matrix product.
+
+mean,sqr = 0.,0.
+for i in range(100):
+    x = torch.randn(512)
+    a = torch.randn(512, 512)
+    y = a @ x
+    y = y.clamp(min=0)
+    mean += y.mean().item()
+    sqr  += y.pow(2).mean().item()
+mean/100,sqr/100
+
+# Or that scaling the coefficient with the magic number gives us a scale of 1.
+
+mean,sqr = 0.,0.
+for i in range(100):
+    x = torch.randn(512)
+    a = torch.randn(512, 512) * math.sqrt(2/512)
+    y = a @ x
+    y = y.clamp(min=0)
+    mean += y.mean().item()
+    sqr  += y.pow(2).mean().item()
+mean/100,sqr/100
+
+# The math behind is a tiny bit more complex, and you can find everything in the [Kaiming](https://arxiv.org/abs/1502.01852) and the [Xavier](http://proceedings.mlr.press/v9/glorot10a.html) paper but this gives the intuition behing those results.
diff --git a/nbs/dl2/03_minibatch_training.ipynb b/nbs/dl2/03_minibatch_training.ipynb
index b8d905f..87277e1 100644
--- ./nbs/dl2/03_minibatch_training.ipynb
+++ ./nbs/dl2/03_minibatch_training.ipynb
@@ -2081,6 +2081,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/03_minibatch_training.py b/nbs/dl2/03_minibatch_training.py
new file mode 100644
index 0000000..fad5e48
--- /dev/null
+++ ./nbs/dl2/03_minibatch_training.py
@@ -0,0 +1,683 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+#export
+from exp.nb_02 import *
+import torch.nn.functional as F
+
+# ## Initial setup
+
+# ### Data
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=1786)
+
+mpl.rcParams['image.cmap'] = 'gray'
+
+x_train,y_train,x_valid,y_valid = get_data()
+
+n,m = x_train.shape
+c = y_train.max()+1
+nh = 50
+
+
+class Model(nn.Module):
+    def __init__(self, n_in, nh, n_out):
+        super().__init__()
+        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]
+        
+    def __call__(self, x):
+        for l in self.layers: x = l(x)
+        return x
+
+
+model = Model(m, nh, 10)
+
+pred = model(x_train)
+
+
+# ### Cross entropy loss
+
+# First, we will need to compute the softmax of our activations. This is defined by:
+#
+# $$\hbox{softmax(x)}_{i} = \frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \cdots + e^{x_{n-1}}}$$
+#
+# or more concisely:
+#
+# $$\hbox{softmax(x)}_{i} = \frac{e^{x_{i}}}{\sum_{0 \leq j \leq n-1} e^{x_{j}}}$$ 
+#
+# In practice, we will need the log of the softmax when we calculate the loss.
+
+def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()
+
+
+sm_pred = log_softmax(pred)
+
+# The cross entropy loss for some target $x$ and some prediction $p(x)$ is given by:
+#
+# $$ -\sum x\, \log p(x) $$
+#
+# But since our $x$s are 1-hot encoded, this can be rewritten as $-\log(p_{i})$ where i is the index of the desired target.
+
+# This can be done using numpy-style [integer array indexing](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#integer-array-indexing). Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link.
+
+y_train[:3]
+
+sm_pred[[0,1,2], [5,0,4]]
+
+y_train.shape[0]
+
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=2081)
+
+def nll(input, target): return -input[range(target.shape[0]), target].mean()
+
+
+loss = nll(sm_pred, y_train)
+
+loss
+
+
+# Note that the formula 
+#
+# $$\log \left ( \frac{a}{b} \right ) = \log(a) - \log(b)$$ 
+#
+# gives a simplification when we compute the log softmax, which was previously defined as `(x.exp()/(x.exp().sum(-1,keepdim=True))).log()`
+
+def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()
+
+
+test_near(nll(log_softmax(pred), y_train), loss)
+
+
+# Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the [LogSumExp trick](https://en.wikipedia.org/wiki/LogSumExp). The idea is to use the following formula:
+#
+# $$\log \left ( \sum_{j=1}^{n} e^{x_{j}} \right ) = \log \left ( e^{a} \sum_{j=1}^{n} e^{x_{j}-a} \right ) = a + \log \left ( \sum_{j=1}^{n} e^{x_{j}-a} \right )$$
+#
+# where a is the maximum of the $x_{j}$.
+
+def logsumexp(x):
+    m = x.max(-1)[0]
+    return m + (x-m[:,None]).exp().sum(-1).log()
+
+
+# This way, we will avoid an overflow when taking the exponential of a big activation. In PyTorch, this is already implemented for us. 
+
+test_near(logsumexp(pred), pred.logsumexp(-1))
+
+
+# So we can use it for our `log_softmax` function.
+
+def log_softmax(x): return x - x.logsumexp(-1,keepdim=True)
+
+
+test_near(nll(log_softmax(pred), y_train), loss)
+
+# Then use PyTorch's implementation.
+
+test_near(F.nll_loss(F.log_softmax(pred, -1), y_train), loss)
+
+# In PyTorch, `F.log_softmax` and `F.nll_loss` are combined in one optimized function, `F.cross_entropy`.
+
+test_near(F.cross_entropy(pred, y_train), loss)
+
+# ## Basic training loop
+
+# Basically the training loop repeats over the following steps:
+# - get the output of the model on a batch of inputs
+# - compare the output to the labels we have and compute a loss
+# - calculate the gradients of the loss with respect to every parameter of the model
+# - update said parameters with those gradients to make them a little bit better
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=2542)
+
+loss_func = F.cross_entropy
+
+
+#export
+def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean()
+
+
+# +
+bs=64                  # batch size
+
+xb = x_train[0:bs]     # a mini-batch from x
+preds = model(xb)      # predictions
+preds[0], preds.shape
+# -
+
+yb = y_train[0:bs]
+loss_func(preds, yb)
+
+accuracy(preds, yb)
+
+lr = 0.5   # learning rate
+epochs = 1 # how many epochs to train for
+
+for epoch in range(epochs):
+    for i in range((n-1)//bs + 1):
+#         set_trace()
+        start_i = i*bs
+        end_i = start_i+bs
+        xb = x_train[start_i:end_i]
+        yb = y_train[start_i:end_i]
+        loss = loss_func(model(xb), yb)
+
+        loss.backward()
+        with torch.no_grad():
+            for l in model.layers:
+                if hasattr(l, 'weight'):
+                    l.weight -= l.weight.grad * lr
+                    l.bias   -= l.bias.grad   * lr
+                    l.weight.grad.zero_()
+                    l.bias  .grad.zero_()
+
+loss_func(model(xb), yb), accuracy(model(xb), yb)
+
+
+# ## Using parameters and optim
+
+# ### Parameters
+
+# Use `nn.Module.__setattr__` and move relu to functional:
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=2818)
+
+class Model(nn.Module):
+    def __init__(self, n_in, nh, n_out):
+        super().__init__()
+        self.l1 = nn.Linear(n_in,nh)
+        self.l2 = nn.Linear(nh,n_out)
+        
+    def __call__(self, x): return self.l2(F.relu(self.l1(x)))
+
+
+model = Model(m, nh, 10)
+
+for name,l in model.named_children(): print(f"{name}: {l}")
+
+model
+
+model.l1
+
+
+def fit():
+    for epoch in range(epochs):
+        for i in range((n-1)//bs + 1):
+            start_i = i*bs
+            end_i = start_i+bs
+            xb = x_train[start_i:end_i]
+            yb = y_train[start_i:end_i]
+            loss = loss_func(model(xb), yb)
+
+            loss.backward()
+            with torch.no_grad():
+                for p in model.parameters(): p -= p.grad * lr
+                model.zero_grad()
+
+
+fit()
+loss_func(model(xb), yb), accuracy(model(xb), yb)
+
+
+# Behind the scenes, PyTorch overrides the `__setattr__` function in `nn.Module` so that the submodules you define are properly registered as parameters of the model.
+
+class DummyModule():
+    def __init__(self, n_in, nh, n_out):
+        self._modules = {}
+        self.l1 = nn.Linear(n_in,nh)
+        self.l2 = nn.Linear(nh,n_out)
+        
+    def __setattr__(self,k,v):
+        if not k.startswith("_"): self._modules[k] = v
+        super().__setattr__(k,v)
+        
+    def __repr__(self): return f'{self._modules}'
+    
+    def parameters(self):
+        for l in self._modules.values():
+            for p in l.parameters(): yield p
+
+
+mdl = DummyModule(m,nh,10)
+mdl
+
+[o.shape for o in mdl.parameters()]
+
+# ### Registering modules
+
+# We can use the original `layers` approach, but we have to register the modules.
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=2997)
+
+layers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)]
+
+
+class Model(nn.Module):
+    def __init__(self, layers):
+        super().__init__()
+        self.layers = layers
+        for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)
+        
+    def __call__(self, x):
+        for l in self.layers: x = l(x)
+        return x
+
+
+model = Model(layers)
+
+model
+
+
+# ### nn.ModuleList
+
+# `nn.ModuleList` does this for us.
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=3173)
+
+class SequentialModel(nn.Module):
+    def __init__(self, layers):
+        super().__init__()
+        self.layers = nn.ModuleList(layers)
+        
+    def __call__(self, x):
+        for l in self.layers: x = l(x)
+        return x
+
+
+model = SequentialModel(layers)
+
+model
+
+fit()
+loss_func(model(xb), yb), accuracy(model(xb), yb)
+
+# ### nn.Sequential
+
+# `nn.Sequential` is a convenient class which does the same as the above:
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=3199)
+
+model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))
+
+fit()
+loss_func(model(xb), yb), accuracy(model(xb), yb)
+
+# +
+# nn.Sequential??
+# -
+
+model
+
+
+# ### optim
+
+# Let's replace our previous manually coded optimization step:
+#
+# ```python
+# with torch.no_grad():
+#     for p in model.parameters(): p -= p.grad * lr
+#     model.zero_grad()
+# ```
+#
+# and instead use just:
+#
+# ```python
+# opt.step()
+# opt.zero_grad()
+# ```
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=3278)
+
+class Optimizer():
+    def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr
+        
+    def step(self):
+        with torch.no_grad():
+            for p in self.params: p -= p.grad * self.lr
+
+    def zero_grad(self):
+        for p in self.params: p.grad.data.zero_()
+
+
+model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))
+
+opt = Optimizer(model.parameters())
+
+for epoch in range(epochs):
+    for i in range((n-1)//bs + 1):
+        start_i = i*bs
+        end_i = start_i+bs
+        xb = x_train[start_i:end_i]
+        yb = y_train[start_i:end_i]
+        pred = model(xb)
+        loss = loss_func(pred, yb)
+
+        loss.backward()
+        opt.step()
+        opt.zero_grad()
+
+loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)
+loss,acc
+
+# PyTorch already provides this exact functionality in `optim.SGD` (it also handles stuff like momentum, which we'll look at later - except we'll be doing it in a more flexible way!)
+
+#export
+from torch import optim
+
+
+# +
+# optim.SGD.step??
+# -
+
+def get_model():
+    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))
+    return model, optim.SGD(model.parameters(), lr=lr)
+
+
+model,opt = get_model()
+loss_func(model(xb), yb)
+
+for epoch in range(epochs):
+    for i in range((n-1)//bs + 1):
+        start_i = i*bs
+        end_i = start_i+bs
+        xb = x_train[start_i:end_i]
+        yb = y_train[start_i:end_i]
+        pred = model(xb)
+        loss = loss_func(pred, yb)
+
+        loss.backward()
+        opt.step()
+        opt.zero_grad()
+
+loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)
+loss,acc
+
+# Randomized tests can be very useful.
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=3442)
+
+assert acc>0.7
+
+
+# ## Dataset and DataLoader
+
+# ### Dataset
+
+# It's clunky to iterate through minibatches of x and y values separately:
+#
+# ```python
+#     xb = x_train[start_i:end_i]
+#     yb = y_train[start_i:end_i]
+# ```
+#
+# Instead, let's do these two steps together, by introducing a `Dataset` class:
+#
+# ```python
+#     xb,yb = train_ds[i*bs : i*bs+bs]
+# ```
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=3578)
+
+#export
+class Dataset():
+    def __init__(self, x, y): self.x,self.y = x,y
+    def __len__(self): return len(self.x)
+    def __getitem__(self, i): return self.x[i],self.y[i]
+
+
+train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)
+assert len(train_ds)==len(x_train)
+assert len(valid_ds)==len(x_valid)
+
+xb,yb = train_ds[0:5]
+assert xb.shape==(5,28*28)
+assert yb.shape==(5,)
+xb,yb
+
+model,opt = get_model()
+
+for epoch in range(epochs):
+    for i in range((n-1)//bs + 1):
+        xb,yb = train_ds[i*bs : i*bs+bs]
+        pred = model(xb)
+        loss = loss_func(pred, yb)
+
+        loss.backward()
+        opt.step()
+        opt.zero_grad()
+
+loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)
+assert acc>0.7
+loss,acc
+
+
+# ### DataLoader
+
+# Previously, our loop iterated over batches (xb, yb) like this:
+#
+# ```python
+# for i in range((n-1)//bs + 1):
+#     xb,yb = train_ds[i*bs : i*bs+bs]
+#     ...
+# ```
+#
+# Let's make our loop much cleaner, using a data loader:
+#
+# ```python
+# for xb,yb in train_dl:
+#     ...
+# ```
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=3674)
+
+class DataLoader():
+    def __init__(self, ds, bs): self.ds,self.bs = ds,bs
+    def __iter__(self):
+        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]
+
+
+train_dl = DataLoader(train_ds, bs)
+valid_dl = DataLoader(valid_ds, bs)
+
+xb,yb = next(iter(valid_dl))
+assert xb.shape==(bs,28*28)
+assert yb.shape==(bs,)
+
+plt.imshow(xb[0].view(28,28))
+yb[0]
+
+model,opt = get_model()
+
+
+def fit():
+    for epoch in range(epochs):
+        for xb,yb in train_dl:
+            pred = model(xb)
+            loss = loss_func(pred, yb)
+            loss.backward()
+            opt.step()
+            opt.zero_grad()
+
+
+fit()
+
+loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)
+assert acc>0.7
+loss,acc
+
+
+# ### Random sampling
+
+# We want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn't be randomized.
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=3942)
+
+class Sampler():
+    def __init__(self, ds, bs, shuffle=False):
+        self.n,self.bs,self.shuffle = len(ds),bs,shuffle
+        
+    def __iter__(self):
+        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)
+        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]
+
+
+small_ds = Dataset(*train_ds[:10])
+
+s = Sampler(small_ds,3,False)
+[o for o in s]
+
+s = Sampler(small_ds,3,True)
+[o for o in s]
+
+
+# +
+def collate(b):
+    xs,ys = zip(*b)
+    return torch.stack(xs),torch.stack(ys)
+
+class DataLoader():
+    def __init__(self, ds, sampler, collate_fn=collate):
+        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn
+        
+    def __iter__(self):
+        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])
+
+
+# -
+
+train_samp = Sampler(train_ds, bs, shuffle=True)
+valid_samp = Sampler(valid_ds, bs, shuffle=False)
+
+train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)
+valid_dl = DataLoader(valid_ds, sampler=valid_samp, collate_fn=collate)
+
+xb,yb = next(iter(valid_dl))
+plt.imshow(xb[0].view(28,28))
+yb[0]
+
+xb,yb = next(iter(train_dl))
+plt.imshow(xb[0].view(28,28))
+yb[0]
+
+xb,yb = next(iter(train_dl))
+plt.imshow(xb[0].view(28,28))
+yb[0]
+
+# +
+model,opt = get_model()
+fit()
+
+loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)
+assert acc>0.7
+loss,acc
+# -
+
+# ### PyTorch DataLoader
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=4171)
+
+#export
+from torch.utils.data import DataLoader, SequentialSampler, RandomSampler
+
+train_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)
+valid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)
+
+model,opt = get_model()
+fit()
+loss_func(model(xb), yb), accuracy(model(xb), yb)
+
+# PyTorch's defaults work fine for most things however:
+
+train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True)
+valid_dl = DataLoader(valid_ds, bs, shuffle=False)
+
+# +
+model,opt = get_model()
+fit()
+
+loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)
+assert acc>0.7
+loss,acc
+
+
+# -
+
+# Note that PyTorch's `DataLoader`, if you pass `num_workers`, will use multiple threads to call your `Dataset`.
+
+# ## Validation
+
+# You **always** should also have a [validation set](http://www.fast.ai/2017/11/13/validation-sets/), in order to identify if you are overfitting.
+#
+# We will calculate and print the validation loss at the end of each epoch.
+#
+# (Note that we always call `model.train()` before training, and `model.eval()` before inference, because these are used by layers such as `nn.BatchNorm2d` and `nn.Dropout` to ensure appropriate behaviour for these different phases.)
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=4260)
+
+def fit(epochs, model, loss_func, opt, train_dl, valid_dl):
+    for epoch in range(epochs):
+        # Handle batchnorm / dropout
+        model.train()
+#         print(model.training)
+        for xb,yb in train_dl:
+            loss = loss_func(model(xb), yb)
+            loss.backward()
+            opt.step()
+            opt.zero_grad()
+
+        model.eval()
+#         print(model.training)
+        with torch.no_grad():
+            tot_loss,tot_acc = 0.,0.
+            for xb,yb in valid_dl:
+                pred = model(xb)
+                tot_loss += loss_func(pred, yb)
+                tot_acc  += accuracy (pred,yb)
+        nv = len(valid_dl)
+        print(epoch, tot_loss/nv, tot_acc/nv)
+    return tot_loss/nv, tot_acc/nv
+
+
+# *Question*: Are these validation results correct if batch size varies?
+
+# `get_dls` returns dataloaders for the training and validation sets:
+
+#export
+def get_dls(train_ds, valid_ds, bs, **kwargs):
+    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),
+            DataLoader(valid_ds, batch_size=bs*2, **kwargs))
+
+
+# Now, our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code:
+
+train_dl,valid_dl = get_dls(train_ds, valid_ds, bs)
+model,opt = get_model()
+loss,acc = fit(5, model, loss_func, opt, train_dl, valid_dl)
+
+assert acc>0.9
+
+# ## Export
+
+# !python notebook2script.py 03_minibatch_training.ipynb
+
+
diff --git a/nbs/dl2/04_callbacks.ipynb b/nbs/dl2/04_callbacks.ipynb
index ea89662..521e23c 100644
--- ./nbs/dl2/04_callbacks.ipynb
+++ ./nbs/dl2/04_callbacks.ipynb
@@ -825,6 +825,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/04_callbacks.py b/nbs/dl2/04_callbacks.py
new file mode 100644
index 0000000..e989a6f
--- /dev/null
+++ ./nbs/dl2/04_callbacks.py
@@ -0,0 +1,465 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+#export
+from exp.nb_03 import *
+
+# ## DataBunch/Learner
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=4799)
+
+x_train,y_train,x_valid,y_valid = get_data()
+train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)
+nh,bs = 50,64
+c = y_train.max().item()+1
+loss_func = F.cross_entropy
+
+
+# Factor out the connected pieces of info out of the fit() argument list
+#
+# `fit(epochs, model, loss_func, opt, train_dl, valid_dl)`
+#
+# Let's replace it with something that looks like this:
+#
+# `fit(1, learn)`
+#
+# This will allow us to tweak what's happening inside the training loop in other places of the code because the `Learner` object will be mutable, so changing any of its attribute elsewhere will be seen in our training loop.
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=5363)
+
+#export
+class DataBunch():
+    def __init__(self, train_dl, valid_dl, c=None):
+        self.train_dl,self.valid_dl,self.c = train_dl,valid_dl,c
+        
+    @property
+    def train_ds(self): return self.train_dl.dataset
+        
+    @property
+    def valid_ds(self): return self.valid_dl.dataset
+
+
+data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)
+
+
+# +
+#export
+def get_model(data, lr=0.5, nh=50):
+    m = data.train_ds.x.shape[1]
+    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,data.c))
+    return model, optim.SGD(model.parameters(), lr=lr)
+
+class Learner():
+    def __init__(self, model, opt, loss_func, data):
+        self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data
+
+
+# -
+
+learn = Learner(*get_model(data), loss_func, data)
+
+
+def fit(epochs, learn):
+    for epoch in range(epochs):
+        learn.model.train()
+        for xb,yb in learn.data.train_dl:
+            loss = learn.loss_func(learn.model(xb), yb)
+            loss.backward()
+            learn.opt.step()
+            learn.opt.zero_grad()
+
+        learn.model.eval()
+        with torch.no_grad():
+            tot_loss,tot_acc = 0.,0.
+            for xb,yb in learn.data.valid_dl:
+                pred = learn.model(xb)
+                tot_loss += learn.loss_func(pred, yb)
+                tot_acc  += accuracy (pred,yb)
+        nv = len(learn.data.valid_dl)
+        print(epoch, tot_loss/nv, tot_acc/nv)
+    return tot_loss/nv, tot_acc/nv
+
+
+loss,acc = fit(1, learn)
+
+
+# ## CallbackHandler
+
+# This was our training loop (without validation) from the previous notebook, with the inner loop contents factored out:
+#
+# ```python
+# def one_batch(xb,yb):
+#     pred = model(xb)
+#     loss = loss_func(pred, yb)
+#     loss.backward()
+#     opt.step()
+#     opt.zero_grad()
+#     
+# def fit():
+#     for epoch in range(epochs):
+#         for b in train_dl: one_batch(*b)
+# ```
+
+# Add callbacks so we can remove complexity from loop, and make it flexible:
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=5628)
+
+# +
+def one_batch(xb, yb, cb):
+    if not cb.begin_batch(xb,yb): return
+    loss = cb.learn.loss_func(cb.learn.model(xb), yb)
+    if not cb.after_loss(loss): return
+    loss.backward()
+    if cb.after_backward(): cb.learn.opt.step()
+    if cb.after_step(): cb.learn.opt.zero_grad()
+
+def all_batches(dl, cb):
+    for xb,yb in dl:
+        one_batch(xb, yb, cb)
+        if cb.do_stop(): return
+
+def fit(epochs, learn, cb):
+    if not cb.begin_fit(learn): return
+    for epoch in range(epochs):
+        if not cb.begin_epoch(epoch): continue
+        all_batches(learn.data.train_dl, cb)
+        
+        if cb.begin_validate():
+            with torch.no_grad(): all_batches(learn.data.valid_dl, cb)
+        if cb.do_stop() or not cb.after_epoch(): break
+    cb.after_fit()
+
+
+# -
+
+class Callback():
+    def begin_fit(self, learn):
+        self.learn = learn
+        return True
+    def after_fit(self): return True
+    def begin_epoch(self, epoch):
+        self.epoch=epoch
+        return True
+    def begin_validate(self): return True
+    def after_epoch(self): return True
+    def begin_batch(self, xb, yb):
+        self.xb,self.yb = xb,yb
+        return True
+    def after_loss(self, loss):
+        self.loss = loss
+        return True
+    def after_backward(self): return True
+    def after_step(self): return True
+
+
+class CallbackHandler():
+    def __init__(self,cbs=None):
+        self.cbs = cbs if cbs else []
+
+    def begin_fit(self, learn):
+        self.learn,self.in_train = learn,True
+        learn.stop = False
+        res = True
+        for cb in self.cbs: res = res and cb.begin_fit(learn)
+        return res
+
+    def after_fit(self):
+        res = not self.in_train
+        for cb in self.cbs: res = res and cb.after_fit()
+        return res
+    
+    def begin_epoch(self, epoch):
+        self.learn.model.train()
+        self.in_train=True
+        res = True
+        for cb in self.cbs: res = res and cb.begin_epoch(epoch)
+        return res
+
+    def begin_validate(self):
+        self.learn.model.eval()
+        self.in_train=False
+        res = True
+        for cb in self.cbs: res = res and cb.begin_validate()
+        return res
+
+    def after_epoch(self):
+        res = True
+        for cb in self.cbs: res = res and cb.after_epoch()
+        return res
+    
+    def begin_batch(self, xb, yb):
+        res = True
+        for cb in self.cbs: res = res and cb.begin_batch(xb, yb)
+        return res
+
+    def after_loss(self, loss):
+        res = self.in_train
+        for cb in self.cbs: res = res and cb.after_loss(loss)
+        return res
+
+    def after_backward(self):
+        res = True
+        for cb in self.cbs: res = res and cb.after_backward()
+        return res
+
+    def after_step(self):
+        res = True
+        for cb in self.cbs: res = res and cb.after_step()
+        return res
+    
+    def do_stop(self):
+        try:     return self.learn.stop
+        finally: self.learn.stop = False
+
+
+class TestCallback(Callback):
+    def begin_fit(self,learn):
+        super().begin_fit(learn)
+        self.n_iters = 0
+        return True
+        
+    def after_step(self):
+        self.n_iters += 1
+        print(self.n_iters)
+        if self.n_iters>=10: self.learn.stop = True
+        return True
+
+
+fit(1, learn, cb=CallbackHandler([TestCallback()]))
+
+# This is roughly how fastai does it now (except that the handler can also change and return `xb`, `yb`, and `loss`). But let's see if we can make things simpler and more flexible, so that a single class has access to everything and can change anything at any time. The fact that we're passing `cb` to so many functions is a strong hint they should all be in the same class!
+
+# ## Runner
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=5811)
+
+# +
+#export
+import re
+
+_camel_re1 = re.compile('(.)([A-Z][a-z]+)')
+_camel_re2 = re.compile('([a-z0-9])([A-Z])')
+def camel2snake(name):
+    s1 = re.sub(_camel_re1, r'\1_\2', name)
+    return re.sub(_camel_re2, r'\1_\2', s1).lower()
+
+class Callback():
+    _order=0
+    def set_runner(self, run): self.run=run
+    def __getattr__(self, k): return getattr(self.run, k)
+    @property
+    def name(self):
+        name = re.sub(r'Callback$', '', self.__class__.__name__)
+        return camel2snake(name or 'callback')
+
+
+# -
+
+# This first callback is reponsible to switch the model back and forth in training or validation mode, as well as maintaining a count of the iterations, or the percentage of iterations ellapsed in the epoch.
+
+#export
+class TrainEvalCallback(Callback):
+    def begin_fit(self):
+        self.run.n_epochs=0.
+        self.run.n_iter=0
+    
+    def after_batch(self):
+        if not self.in_train: return
+        self.run.n_epochs += 1./self.iters
+        self.run.n_iter   += 1
+        
+    def begin_epoch(self):
+        self.run.n_epochs=self.epoch
+        self.model.train()
+        self.run.in_train=True
+
+    def begin_validate(self):
+        self.model.eval()
+        self.run.in_train=False
+
+
+# We'll also re-create our TestCallback
+
+class TestCallback(Callback):
+    def after_step(self):
+        if self.train_eval.n_iters>=10: return True
+
+
+cbname = 'TrainEvalCallback'
+camel2snake(cbname)
+
+TrainEvalCallback().name
+
+# +
+#export
+from typing import *
+
+def listify(o):
+    if o is None: return []
+    if isinstance(o, list): return o
+    if isinstance(o, str): return [o]
+    if isinstance(o, Iterable): return list(o)
+    return [o]
+
+
+# -
+
+#export
+class Runner():
+    def __init__(self, cbs=None, cb_funcs=None):
+        cbs = listify(cbs)
+        for cbf in listify(cb_funcs):
+            cb = cbf()
+            setattr(self, cb.name, cb)
+            cbs.append(cb)
+        self.stop,self.cbs = False,[TrainEvalCallback()]+cbs
+
+    @property
+    def opt(self):       return self.learn.opt
+    @property
+    def model(self):     return self.learn.model
+    @property
+    def loss_func(self): return self.learn.loss_func
+    @property
+    def data(self):      return self.learn.data
+
+    def one_batch(self, xb, yb):
+        self.xb,self.yb = xb,yb
+        if self('begin_batch'): return
+        self.pred = self.model(self.xb)
+        if self('after_pred'): return
+        self.loss = self.loss_func(self.pred, self.yb)
+        if self('after_loss') or not self.in_train: return
+        self.loss.backward()
+        if self('after_backward'): return
+        self.opt.step()
+        if self('after_step'): return
+        self.opt.zero_grad()
+
+    def all_batches(self, dl):
+        self.iters = len(dl)
+        for xb,yb in dl:
+            if self.stop: break
+            self.one_batch(xb, yb)
+            self('after_batch')
+        self.stop=False
+
+    def fit(self, epochs, learn):
+        self.epochs,self.learn = epochs,learn
+
+        try:
+            for cb in self.cbs: cb.set_runner(self)
+            if self('begin_fit'): return
+            for epoch in range(epochs):
+                self.epoch = epoch
+                if not self('begin_epoch'): self.all_batches(self.data.train_dl)
+
+                with torch.no_grad(): 
+                    if not self('begin_validate'): self.all_batches(self.data.valid_dl)
+                if self('after_epoch'): break
+            
+        finally:
+            self('after_fit')
+            self.learn = None
+
+    def __call__(self, cb_name):
+        for cb in sorted(self.cbs, key=lambda x: x._order):
+            f = getattr(cb, cb_name, None)
+            if f and f(): return True
+        return False
+
+
+# Third callback: how to compute metrics.
+
+# +
+#export
+class AvgStats():
+    def __init__(self, metrics, in_train): self.metrics,self.in_train = listify(metrics),in_train
+    
+    def reset(self):
+        self.tot_loss,self.count = 0.,0
+        self.tot_mets = [0.] * len(self.metrics)
+        
+    @property
+    def all_stats(self): return [self.tot_loss.item()] + self.tot_mets
+    @property
+    def avg_stats(self): return [o/self.count for o in self.all_stats]
+    
+    def __repr__(self):
+        if not self.count: return ""
+        return f"{'train' if self.in_train else 'valid'}: {self.avg_stats}"
+
+    def accumulate(self, run):
+        bn = run.xb.shape[0]
+        self.tot_loss += run.loss * bn
+        self.count += bn
+        for i,m in enumerate(self.metrics):
+            self.tot_mets[i] += m(run.pred, run.yb) * bn
+
+class AvgStatsCallback(Callback):
+    def __init__(self, metrics):
+        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)
+        
+    def begin_epoch(self):
+        self.train_stats.reset()
+        self.valid_stats.reset()
+        
+    def after_loss(self):
+        stats = self.train_stats if self.in_train else self.valid_stats
+        with torch.no_grad(): stats.accumulate(self.run)
+    
+    def after_epoch(self):
+        print(self.train_stats)
+        print(self.valid_stats)
+
+
+# -
+
+learn = Learner(*get_model(data), loss_func, data)
+
+stats = AvgStatsCallback([accuracy])
+run = Runner(cbs=stats)
+
+run.fit(2, learn)
+
+loss,acc = stats.valid_stats.avg_stats
+assert acc>0.9
+loss,acc
+
+#export
+from functools import partial
+
+acc_cbf = partial(AvgStatsCallback,accuracy)
+
+run = Runner(cb_funcs=acc_cbf)
+
+run.fit(1, learn)
+
+# Using Jupyter means we can get tab-completion even for dynamic code like this! :)
+
+run.avg_stats.valid_stats.avg_stats
+
+# ## Export
+
+# !python notebook2script.py 04_callbacks.ipynb
+
+
diff --git a/nbs/dl2/05_anneal.ipynb b/nbs/dl2/05_anneal.ipynb
index 716b1a8..9a57f21 100644
--- ./nbs/dl2/05_anneal.ipynb
+++ ./nbs/dl2/05_anneal.ipynb
@@ -540,6 +540,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/05_anneal.py b/nbs/dl2/05_anneal.py
new file mode 100644
index 0000000..2e53c68
--- /dev/null
+++ ./nbs/dl2/05_anneal.py
@@ -0,0 +1,206 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+#export
+from exp.nb_04 import *
+
+# ## Initial setup
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=7013)
+
+x_train,y_train,x_valid,y_valid = get_data()
+train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)
+nh,bs = 50,512
+c = y_train.max().item()+1
+loss_func = F.cross_entropy
+
+data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)
+
+
+#export
+def create_learner(model_func, loss_func, data):
+    return Learner(*model_func(data), loss_func, data)
+
+
+# +
+learn = create_learner(get_model, loss_func, data)
+run = Runner([AvgStatsCallback([accuracy])])
+
+run.fit(3, learn)
+
+# +
+learn = create_learner(partial(get_model, lr=0.3), loss_func, data)
+run = Runner([AvgStatsCallback([accuracy])])
+
+run.fit(3, learn)
+
+
+# -
+
+#export
+def get_model_func(lr=0.5): return partial(get_model, lr=lr)
+
+
+# ## Annealing
+
+# We define two new callbacks: the Recorder to save track of the loss and our scheduled learning rate, and a ParamScheduler that can schedule any hyperparameter as long as it's registered in the state_dict of the optimizer. 
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=7202)
+
+# +
+#export
+class Recorder(Callback):
+    def begin_fit(self): self.lrs,self.losses = [],[]
+
+    def after_batch(self):
+        if not self.in_train: return
+        self.lrs.append(self.opt.param_groups[-1]['lr'])
+        self.losses.append(self.loss.detach().cpu())        
+
+    def plot_lr  (self): plt.plot(self.lrs)
+    def plot_loss(self): plt.plot(self.losses)
+
+class ParamScheduler(Callback):
+    _order=1
+    def __init__(self, pname, sched_func): self.pname,self.sched_func = pname,sched_func
+
+    def set_param(self):
+        for pg in self.opt.param_groups:
+            pg[self.pname] = self.sched_func(self.n_epochs/self.epochs)
+            
+    def begin_batch(self): 
+        if self.in_train: self.set_param()
+
+
+# -
+
+# Let's start with a simple linear schedule going from start to end. It returns a function that takes a `pos` argument (going from 0 to 1) such that this function goes from `start` (at `pos=0`) to `end` (at `pos=1`) in a linear fashion.
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=7431)
+
+def sched_lin(start, end):
+    def _inner(start, end, pos): return start + pos*(end-start)
+    return partial(_inner, start, end)
+
+
+# We can refactor this with a decorator.
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=7526)
+
+# +
+#export
+def annealer(f):
+    def _inner(start, end): return partial(f, start, end)
+    return _inner
+
+@annealer
+def sched_lin(start, end, pos): return start + pos*(end-start)
+
+
+# +
+# shift-tab works too, in Jupyter!
+# sched_lin()
+# -
+
+f = sched_lin(1,2)
+f(0.3)
+
+
+# And here are other scheduler functions:
+
+# +
+#export
+@annealer
+def sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2
+@annealer
+def sched_no(start, end, pos):  return start
+@annealer
+def sched_exp(start, end, pos): return start * (end/start) ** pos
+
+def cos_1cycle_anneal(start, high, end):
+    return [sched_cos(start, high), sched_cos(high, end)]
+
+#This monkey-patch is there to be able to plot tensors
+torch.Tensor.ndim = property(lambda x: len(x.shape))
+# -
+
+# [Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=7730)
+
+# +
+annealings = "NO LINEAR COS EXP".split()
+
+a = torch.arange(0, 100)
+p = torch.linspace(0.01,1,100)
+
+fns = [sched_no, sched_lin, sched_cos, sched_exp]
+for fn, t in zip(fns, annealings):
+    f = fn(2, 1e-2)
+    plt.plot(a, [f(o) for o in p], label=t)
+plt.legend();
+
+
+# -
+
+# In practice, we'll often want to combine different schedulers, the following function does that: it uses `scheds[i]` for `pcts[i]` of the training.
+
+#export
+def combine_scheds(pcts, scheds):
+    assert sum(pcts) == 1.
+    pcts = tensor([0] + listify(pcts))
+    assert torch.all(pcts >= 0)
+    pcts = torch.cumsum(pcts, 0)
+    def _inner(pos):
+        idx = (pos >= pcts).nonzero().max()
+        if idx == 2: idx = 1
+        actual_pos = (pos-pcts[idx]) / (pcts[idx+1]-pcts[idx])
+        return scheds[idx](actual_pos)
+    return _inner
+
+
+# Here is an example: use 30% of the budget to go from 0.3 to 0.6 following a cosine, then the last 70% of the budget to go from 0.6 to 0.2, still following a cosine.
+
+sched = combine_scheds([0.3, 0.7], [sched_cos(0.3, 0.6), sched_cos(0.6, 0.2)]) 
+
+plt.plot(a, [sched(o) for o in p])
+
+# We can use it for training quite easily...
+
+cbfs = [Recorder,
+        partial(AvgStatsCallback,accuracy),
+        partial(ParamScheduler, 'lr', sched)]
+
+learn = create_learner(get_model_func(0.3), loss_func, data)
+run = Runner(cb_funcs=cbfs)
+
+run.fit(3, learn)
+
+# ... then check with our recorder if the learning rate followed the right schedule.
+
+run.recorder.plot_lr()
+
+run.recorder.plot_loss()
+
+# ## Export
+
+!./notebook2script.py 05_anneal.ipynb
+
+
diff --git a/nbs/dl2/05a_foundations.ipynb b/nbs/dl2/05a_foundations.ipynb
index 97cb4f5..ff2fe93 100644
--- ./nbs/dl2/05a_foundations.ipynb
+++ ./nbs/dl2/05a_foundations.ipynb
@@ -1493,6 +1493,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/05a_foundations.py b/nbs/dl2/05a_foundations.py
new file mode 100644
index 0000000..2bfc9ac
--- /dev/null
+++ ./nbs/dl2/05a_foundations.py
@@ -0,0 +1,391 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+import torch
+import matplotlib.pyplot as plt
+
+# [Jump_to opening comments and overview of lesson 10](https://course.fast.ai/videos/?lesson=10&t=108)
+
+# ## Callbacks
+
+# ### Callbacks as GUI events
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=432)
+
+import ipywidgets as widgets
+
+
+def f(o): print('hi')
+
+
+# From the [ipywidget docs](https://ipywidgets.readthedocs.io/en/stable/examples/Widget%20Events.html):
+#
+# - *the button widget is used to handle mouse clicks. The on_click method of the Button can be used to register function to be called when the button is clicked*
+
+w = widgets.Button(description='Click me')
+
+w
+
+w.on_click(f)
+
+# *NB: When callbacks are used in this way they are often called "events".*
+#
+# Did you know what you can create interactive apps in Jupyter with these widgets? Here's an example from [plotly](https://plot.ly/python/widget-app/):
+#
+# ![](https://cloud.githubusercontent.com/assets/12302455/16637308/4e476280-43ac-11e6-9fd3-ada2c9506ee1.gif)
+
+# ### Creating your own callback
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=680)
+
+from time import sleep
+
+
+def slow_calculation():
+    res = 0
+    for i in range(5):
+        res += i*i
+        sleep(1)
+    return res
+
+
+slow_calculation()
+
+
+def slow_calculation(cb=None):
+    res = 0
+    for i in range(5):
+        res += i*i
+        sleep(1)
+        if cb: cb(i)
+    return res
+
+
+def show_progress(epoch):
+    print(f"Awesome! We've finished epoch {epoch}!")
+
+
+slow_calculation(show_progress)
+
+# ### Lambdas and partials
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=811)
+
+slow_calculation(lambda o: print(f"Awesome! We've finished epoch {o}!"))
+
+
+def show_progress(exclamation, epoch):
+    print(f"{exclamation}! We've finished epoch {epoch}!")
+
+
+slow_calculation(lambda o: show_progress("OK I guess", o))
+
+
+def make_show_progress(exclamation):
+    _inner = lambda epoch: print(f"{exclamation}! We've finished epoch {epoch}!")
+    return _inner
+
+
+slow_calculation(make_show_progress("Nice!"))
+
+
+def make_show_progress(exclamation):
+    # Leading "_" is generally understood to be "private"
+    def _inner(epoch): print(f"{exclamation}! We've finished epoch {epoch}!")
+    return _inner
+
+
+slow_calculation(make_show_progress("Nice!"))
+
+f2 = make_show_progress("Terrific")
+
+slow_calculation(f2)
+
+slow_calculation(make_show_progress("Amazing"))
+
+from functools import partial
+
+slow_calculation(partial(show_progress, "OK I guess"))
+
+f2 = partial(show_progress, "OK I guess")
+
+
+# ### Callbacks as callable classes
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=1122)
+
+class ProgressShowingCallback():
+    def __init__(self, exclamation="Awesome"): self.exclamation = exclamation
+    def __call__(self, epoch): print(f"{self.exclamation}! We've finished epoch {epoch}!")
+
+
+cb = ProgressShowingCallback("Just super")
+
+slow_calculation(cb)
+
+
+# ### Multiple callback funcs; `*args` and `**kwargs`
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=1194)
+
+def f(*args, **kwargs): print(f"args: {args}; kwargs: {kwargs}")
+
+
+f(3, 'a', thing1="hello")
+
+
+# NB: We've been guilty of over-using kwargs in fastai - it's very convenient for the developer, but is annoying for the end-user unless care is taken to ensure docs show all kwargs too. kwargs can also hide bugs (because it might not tell you about a typo in a param name). In [R](https://www.r-project.org/) there's a very similar issue (R uses `...` for the same thing), and matplotlib uses kwargs a lot too.
+
+def slow_calculation(cb=None):
+    res = 0
+    for i in range(5):
+        if cb: cb.before_calc(i)
+        res += i*i
+        sleep(1)
+        if cb: cb.after_calc(i, val=res)
+    return res
+
+
+class PrintStepCallback():
+    def __init__(self): pass
+    def before_calc(self, *args, **kwargs): print(f"About to start")
+    def after_calc (self, *args, **kwargs): print(f"Done step")
+
+
+slow_calculation(PrintStepCallback())
+
+
+class PrintStatusCallback():
+    def __init__(self): pass
+    def before_calc(self, epoch, **kwargs): print(f"About to start: {epoch}")
+    def after_calc (self, epoch, val, **kwargs): print(f"After {epoch}: {val}")
+
+
+slow_calculation(PrintStatusCallback())
+
+
+# ### Modifying behavior
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=1454)
+
+def slow_calculation(cb=None):
+    res = 0
+    for i in range(5):
+        if cb and hasattr(cb,'before_calc'): cb.before_calc(i)
+        res += i*i
+        sleep(1)
+        if cb and hasattr(cb,'after_calc'):
+            if cb.after_calc(i, res):
+                print("stopping early")
+                break
+    return res
+
+
+class PrintAfterCallback():
+    def after_calc (self, epoch, val):
+        print(f"After {epoch}: {val}")
+        if val>10: return True
+
+
+slow_calculation(PrintAfterCallback())
+
+
+class SlowCalculator():
+    def __init__(self, cb=None): self.cb,self.res = cb,0
+    
+    def callback(self, cb_name, *args):
+        if not self.cb: return
+        cb = getattr(self.cb,cb_name, None)
+        if cb: return cb(self, *args)
+
+    def calc(self):
+        for i in range(5):
+            self.callback('before_calc', i)
+            self.res += i*i
+            sleep(1)
+            if self.callback('after_calc', i):
+                print("stopping early")
+                break
+
+
+class ModifyingCallback():
+    def after_calc (self, calc, epoch):
+        print(f"After {epoch}: {calc.res}")
+        if calc.res>10: return True
+        if calc.res<3: calc.res = calc.res*2
+
+
+calculator = SlowCalculator(ModifyingCallback())
+
+calculator.calc()
+calculator.res
+
+
+# ## `__dunder__` thingies
+
+# Anything that looks like `__this__` is, in some way, *special*. Python, or some library, can define some functions that they will call at certain documented times. For instance, when your class is setting up a new object, python will call `__init__`. These are defined as part of the python [data model](https://docs.python.org/3/reference/datamodel.html#object.__init__).
+#
+# For instance, if python sees `+`, then it will call the special method `__add__`. If you try to display an object in Jupyter (or lots of other places in Python) it will call `__repr__`.
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=1647)
+
+class SloppyAdder():
+    def __init__(self,o): self.o=o
+    def __add__(self,b): return SloppyAdder(self.o + b.o + 0.01)
+    def __repr__(self): return str(self.o)
+
+
+a = SloppyAdder(1)
+b = SloppyAdder(2)
+a+b
+
+# Special methods you should probably know about (see data model link above) are:
+#
+# - `__getitem__`
+# - `__getattr__`
+# - `__setattr__`
+# - `__del__`
+# - `__init__`
+# - `__new__`
+# - `__enter__`
+# - `__exit__`
+# - `__len__`
+# - `__repr__`
+# - `__str__`
+
+# ## Variance and stuff
+
+# ### Variance
+
+# Variance is the average of how far away each data point is from the mean. E.g.:
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=2133)
+
+t = torch.tensor([1.,2.,4.,18])
+
+m = t.mean(); m
+
+(t-m).mean()
+
+# Oops. We can't do that. Because by definition the positives and negatives cancel out. So we can fix that in one of (at least) two ways:
+
+(t-m).pow(2).mean()
+
+(t-m).abs().mean()
+
+# But the first of these is now a totally different scale, since we squared. So let's undo that at the end.
+
+(t-m).pow(2).mean().sqrt()
+
+# They're still different. Why?
+#
+# Note that we have one outlier (`18`). In the version where we square everything, it makes that much bigger than everything else.
+#
+# `(t-m).pow(2).mean()` is refered to as **variance**. It's a measure of how spread out the data is, and is particularly sensitive to outliers.
+#
+# When we take the sqrt of the variance, we get the **standard deviation**. Since it's on the same kind of scale as the original data, it's generally more interpretable. However, since `sqrt(1)==1`, it doesn't much matter which we use when talking about *unit variance* for initializing neural nets.
+#
+# `(t-m).abs().mean()` is referred to as the **mean absolute deviation**. It isn't used nearly as much as it deserves to be, because mathematicians don't like how awkward it is to work with. But that shouldn't stop us, because we have computers and stuff.
+#
+# Here's a useful thing to note about variance:
+
+(t-m).pow(2).mean(), (t*t).mean() - (m*m)
+
+# You can see why these are equal if you want to work thru the algebra. Or not.
+#
+# But, what's important here is that the latter is generally much easier to work with. In particular, you only have to track two things: the sum of the data, and the sum of squares of the data. Whereas in the first form you actually have to go thru all the data twice (once to calculate the mean, once to calculate the differences).
+#
+# Let's go steal the LaTeX from [Wikipedia](https://en.wikipedia.org/wiki/Variance):
+#
+# $$\operatorname{E}\left[X^2 \right] - \operatorname{E}[X]^2$$
+
+# ### Covariance and correlation
+
+# Here's how Wikipedia defines covariance:
+#
+# $$\operatorname{cov}(X,Y) = \operatorname{E}{\big[(X - \operatorname{E}[X])(Y - \operatorname{E}[Y])\big]}$$
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=2414)
+
+t
+
+# Let's see that in code. So now we need two vectors.
+
+# +
+# `u` is twice `t`, plus a bit of randomness
+u = t*2
+u *= torch.randn_like(t)/10+0.95
+
+plt.scatter(t, u);
+# -
+
+prod = (t-t.mean())*(u-u.mean()); prod
+
+prod.mean()
+
+v = torch.randn_like(t)
+plt.scatter(t, v);
+
+((t-t.mean())*(v-v.mean())).mean()
+
+# It's generally more conveniently defined like so:
+#
+# $$\operatorname{E}\left[X Y\right] - \operatorname{E}\left[X\right] \operatorname{E}\left[Y\right]$$
+
+cov = (t*v).mean() - t.mean()*v.mean(); cov
+
+# From now on, you're not allowed to look at an equation (or especially type it in LaTeX) without also typing it in Python and actually calculating some values. Ideally, you should also plot some values.
+#
+# Finally, here is the Pearson correlation coefficient:
+#
+# $$\rho_{X,Y}= \frac{\operatorname{cov}(X,Y)}{\sigma_X \sigma_Y}$$
+
+cov / (t.std() * v.std())
+
+
+# It's just a scaled version of the same thing. Question: *Why is it scaled by standard deviation, and not by variance or mean or something else?*
+
+# ## Softmax
+
+# Here's our final `logsoftmax` definition:
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=2674)
+
+def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()
+
+# which is:
+#
+# $$\hbox{logsoftmax(x)}_{i} = x_{i} - \log \sum_{j} e^{x_{j}}$$ 
+#
+# And our cross entropy loss is:
+# $$-\log(p_{i})$$
+
+# ## Browsing source code
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=1782)
+
+# - Jump to tag/symbol by with (with completions)
+# - Jump to current tag
+# - Jump to library tags
+# - Go back
+# - Search
+# - Outlining / folding
diff --git a/nbs/dl2/05b_early_stopping.ipynb b/nbs/dl2/05b_early_stopping.ipynb
index 0474440..40fe5a4 100644
--- ./nbs/dl2/05b_early_stopping.ipynb
+++ ./nbs/dl2/05b_early_stopping.ipynb
@@ -473,6 +473,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/05b_early_stopping.py b/nbs/dl2/05b_early_stopping.py
new file mode 100644
index 0000000..a702ac0
--- /dev/null
+++ ./nbs/dl2/05b_early_stopping.py
@@ -0,0 +1,268 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+#export
+from exp.nb_05 import *
+
+# [Jump_to notebook introduction in lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=3167)
+
+# ## Early stopping
+
+# ### Better callback cancellation
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=3230)
+
+x_train,y_train,x_valid,y_valid = get_data()
+train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)
+nh,bs = 50,512
+c = y_train.max().item()+1
+loss_func = F.cross_entropy
+
+data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)
+
+
+# +
+#export
+class Callback():
+    _order=0
+    def set_runner(self, run): self.run=run
+    def __getattr__(self, k): return getattr(self.run, k)
+    
+    @property
+    def name(self):
+        name = re.sub(r'Callback$', '', self.__class__.__name__)
+        return camel2snake(name or 'callback')
+    
+    def __call__(self, cb_name):
+        f = getattr(self, cb_name, None)
+        if f and f(): return True
+        return False
+
+class TrainEvalCallback(Callback):
+    def begin_fit(self):
+        self.run.n_epochs=0.
+        self.run.n_iter=0
+    
+    def after_batch(self):
+        if not self.in_train: return
+        self.run.n_epochs += 1./self.iters
+        self.run.n_iter   += 1
+        
+    def begin_epoch(self):
+        self.run.n_epochs=self.epoch
+        self.model.train()
+        self.run.in_train=True
+
+    def begin_validate(self):
+        self.model.eval()
+        self.run.in_train=False
+
+class CancelTrainException(Exception): pass
+class CancelEpochException(Exception): pass
+class CancelBatchException(Exception): pass
+
+
+# -
+
+#export
+class Runner():
+    def __init__(self, cbs=None, cb_funcs=None):
+        self.in_train = False
+        cbs = listify(cbs)
+        for cbf in listify(cb_funcs):
+            cb = cbf()
+            setattr(self, cb.name, cb)
+            cbs.append(cb)
+        self.stop,self.cbs = False,[TrainEvalCallback()]+cbs
+
+    @property
+    def opt(self):       return self.learn.opt
+    @property
+    def model(self):     return self.learn.model
+    @property
+    def loss_func(self): return self.learn.loss_func
+    @property
+    def data(self):      return self.learn.data
+
+    def one_batch(self, xb, yb):
+        try:
+            self.xb,self.yb = xb,yb
+            self('begin_batch')
+            self.pred = self.model(self.xb)
+            self('after_pred')
+            self.loss = self.loss_func(self.pred, self.yb)
+            self('after_loss')
+            if not self.in_train: return
+            self.loss.backward()
+            self('after_backward')
+            self.opt.step()
+            self('after_step')
+            self.opt.zero_grad()
+        except CancelBatchException: self('after_cancel_batch')
+        finally: self('after_batch')
+
+    def all_batches(self, dl):
+        self.iters = len(dl)
+        try:
+            for xb,yb in dl: self.one_batch(xb, yb)
+        except CancelEpochException: self('after_cancel_epoch')
+
+    def fit(self, epochs, learn):
+        self.epochs,self.learn,self.loss = epochs,learn,tensor(0.)
+
+        try:
+            for cb in self.cbs: cb.set_runner(self)
+            self('begin_fit')
+            for epoch in range(epochs):
+                self.epoch = epoch
+                if not self('begin_epoch'): self.all_batches(self.data.train_dl)
+
+                with torch.no_grad(): 
+                    if not self('begin_validate'): self.all_batches(self.data.valid_dl)
+                self('after_epoch')
+            
+        except CancelTrainException: self('after_cancel_train')
+        finally:
+            self('after_fit')
+            self.learn = None
+
+    def __call__(self, cb_name):
+        res = False
+        for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) or res
+        return res
+
+
+learn = create_learner(get_model, loss_func, data)
+
+
+class TestCallback(Callback):
+    _order=1
+    def after_step(self):
+        print(self.n_iter)
+        if self.n_iter>=10: raise CancelTrainException()
+
+
+run = Runner(cb_funcs=TestCallback)
+
+run.fit(3, learn)
+
+
+# ### Other callbacks
+
+# +
+#export
+class AvgStatsCallback(Callback):
+    def __init__(self, metrics):
+        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)
+        
+    def begin_epoch(self):
+        self.train_stats.reset()
+        self.valid_stats.reset()
+        
+    def after_loss(self):
+        stats = self.train_stats if self.in_train else self.valid_stats
+        with torch.no_grad(): stats.accumulate(self.run)
+    
+    def after_epoch(self):
+        print(self.train_stats)
+        print(self.valid_stats)
+        
+class Recorder(Callback):
+    def begin_fit(self):
+        self.lrs = [[] for _ in self.opt.param_groups]
+        self.losses = []
+
+    def after_batch(self):
+        if not self.in_train: return
+        for pg,lr in zip(self.opt.param_groups,self.lrs): lr.append(pg['lr'])
+        self.losses.append(self.loss.detach().cpu())        
+
+    def plot_lr  (self, pgid=-1): plt.plot(self.lrs[pgid])
+    def plot_loss(self, skip_last=0): plt.plot(self.losses[:len(self.losses)-skip_last])
+        
+    def plot(self, skip_last=0, pgid=-1):
+        losses = [o.item() for o in self.losses]
+        lrs    = self.lrs[pgid]
+        n = len(losses)-skip_last
+        plt.xscale('log')
+        plt.plot(lrs[:n], losses[:n])
+
+class ParamScheduler(Callback):
+    _order=1
+    def __init__(self, pname, sched_funcs): self.pname,self.sched_funcs = pname,sched_funcs
+        
+    def begin_fit(self):
+        if not isinstance(self.sched_funcs, (list,tuple)):
+            self.sched_funcs = [self.sched_funcs] * len(self.opt.param_groups)
+
+    def set_param(self):
+        assert len(self.opt.param_groups)==len(self.sched_funcs)
+        for pg,f in zip(self.opt.param_groups,self.sched_funcs):
+            pg[self.pname] = f(self.n_epochs/self.epochs)
+            
+    def begin_batch(self): 
+        if self.in_train: self.set_param()
+
+
+# -
+
+# ### LR Finder
+
+# NB: You may want to also add something that saves the model before running this, and loads it back after running - otherwise you'll lose your weights!
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=3545)
+
+class LR_Find(Callback):
+    _order=1
+    def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10):
+        self.max_iter,self.min_lr,self.max_lr = max_iter,min_lr,max_lr
+        self.best_loss = 1e9
+        
+    def begin_batch(self): 
+        if not self.in_train: return
+        pos = self.n_iter/self.max_iter
+        lr = self.min_lr * (self.max_lr/self.min_lr) ** pos
+        for pg in self.opt.param_groups: pg['lr'] = lr
+            
+    def after_step(self):
+        if self.n_iter>=self.max_iter or self.loss>self.best_loss*10:
+            raise CancelTrainException()
+        if self.loss < self.best_loss: self.best_loss = self.loss
+
+
+# NB: In fastai we also use exponential smoothing on the loss. For that reason we check for `best_loss*3` instead of `best_loss*10`.
+
+learn = create_learner(get_model, loss_func, data)
+
+run = Runner(cb_funcs=[LR_Find, Recorder])
+
+run.fit(2, learn)
+
+run.recorder.plot(skip_last=5)
+
+run.recorder.plot_lr()
+
+# ## Export
+
+# !python notebook2script.py 05b_early_stopping.ipynb
+
+
diff --git a/nbs/dl2/06_cuda_cnn_hooks_init.ipynb b/nbs/dl2/06_cuda_cnn_hooks_init.ipynb
index d69ab48..abea3ff 100644
--- ./nbs/dl2/06_cuda_cnn_hooks_init.ipynb
+++ ./nbs/dl2/06_cuda_cnn_hooks_init.ipynb
@@ -1856,6 +1856,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/06_cuda_cnn_hooks_init.py b/nbs/dl2/06_cuda_cnn_hooks_init.py
new file mode 100644
index 0000000..47515b3
--- /dev/null
+++ ./nbs/dl2/06_cuda_cnn_hooks_init.py
@@ -0,0 +1,614 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+#export
+from exp.nb_05b import *
+torch.set_num_threads(2)
+
+# ## ConvNet
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=3641)
+
+x_train,y_train,x_valid,y_valid = get_data()
+
+
+# Helper function to quickly normalize with the mean and standard deviation from our training set:
+
+#export
+def normalize_to(train, valid):
+    m,s = train.mean(),train.std()
+    return normalize(train, m, s), normalize(valid, m, s)
+
+
+x_train,x_valid = normalize_to(x_train,x_valid)
+train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)
+
+# Let's check it behaved properly.
+
+x_train.mean(),x_train.std()
+
+# +
+nh,bs = 50,512
+c = y_train.max().item()+1
+loss_func = F.cross_entropy
+
+data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)
+
+
+# -
+
+# To refactor layers, it's useful to have a `Lambda` layer that can take a basic function and convert it to a layer you can put in `nn.Sequential`.
+#
+# NB: if you use a Lambda layer with a lambda function, your model won't pickle so you won't be able to save it with PyTorch. So it's best to give a name to the function you're using inside your Lambda (like flatten below).
+
+# +
+#export
+class Lambda(nn.Module):
+    def __init__(self, func):
+        super().__init__()
+        self.func = func
+
+    def forward(self, x): return self.func(x)
+
+def flatten(x):      return x.view(x.shape[0], -1)
+
+
+# -
+
+# This one takes the flat vector of size `bs x 784` and puts it back as a batch of images of 28 by 28 pixels:
+
+def mnist_resize(x): return x.view(-1, 1, 28, 28)
+
+
+# We can now define a simple CNN.
+
+def get_cnn_model(data):
+    return nn.Sequential(
+        Lambda(mnist_resize),
+        nn.Conv2d( 1, 8, 5, padding=2,stride=2), nn.ReLU(), #14
+        nn.Conv2d( 8,16, 3, padding=1,stride=2), nn.ReLU(), # 7
+        nn.Conv2d(16,32, 3, padding=1,stride=2), nn.ReLU(), # 4
+        nn.Conv2d(32,32, 3, padding=1,stride=2), nn.ReLU(), # 2
+        nn.AdaptiveAvgPool2d(1),
+        Lambda(flatten),
+        nn.Linear(32,data.c)
+    )
+
+
+model = get_cnn_model(data)
+
+# Basic callbacks from the previous notebook:
+
+cbfs = [Recorder, partial(AvgStatsCallback,accuracy)]
+
+opt = optim.SGD(model.parameters(), lr=0.4)
+learn = Learner(model, opt, loss_func, data)
+run = Runner(cb_funcs=cbfs)
+
+# %time run.fit(1, learn)
+
+# ## CUDA
+
+# This took a long time to run, so it's time to use a GPU. A simple Callback can make sure the model, inputs and targets are all on the same device.
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=3799)
+
+# Somewhat more flexible way
+device = torch.device('cuda',0)
+
+
+class CudaCallback(Callback):
+    def __init__(self,device): self.device=device
+    def begin_fit(self): self.model.to(self.device)
+    def begin_batch(self): self.run.xb,self.run.yb = self.xb.to(self.device),self.yb.to(self.device)
+
+
+# Somewhat less flexible, but quite convenient
+torch.cuda.set_device(device)
+
+
+#export
+class CudaCallback(Callback):
+    def begin_fit(self): self.model.cuda()
+    def begin_batch(self): self.run.xb,self.run.yb = self.xb.cuda(),self.yb.cuda()
+
+
+cbfs.append(CudaCallback)
+
+model = get_cnn_model(data)
+
+opt = optim.SGD(model.parameters(), lr=0.4)
+learn = Learner(model, opt, loss_func, data)
+run = Runner(cb_funcs=cbfs)
+
+
+# %time run.fit(3, learn)
+
+# Now, that's definitely faster!
+
+# ## Refactor model
+
+# First we can regroup all the conv/relu in a single function:
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=3961)
+
+def conv2d(ni, nf, ks=3, stride=2):
+    return nn.Sequential(
+        nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride), nn.ReLU())
+
+
+# Another thing is that we can do the mnist resize in a batch transform, that we can do with a Callback.
+
+# +
+#export
+class BatchTransformXCallback(Callback):
+    _order=2
+    def __init__(self, tfm): self.tfm = tfm
+    def begin_batch(self): self.run.xb = self.tfm(self.xb)
+
+def view_tfm(*size):
+    def _inner(x): return x.view(*((-1,)+size))
+    return _inner
+
+
+# -
+
+mnist_view = view_tfm(1,28,28)
+cbfs.append(partial(BatchTransformXCallback, mnist_view))
+
+# With the `AdaptiveAvgPool`, this model can now work on any size input:
+
+nfs = [8,16,32,32]
+
+
+# +
+def get_cnn_layers(data, nfs):
+    nfs = [1] + nfs
+    return [
+        conv2d(nfs[i], nfs[i+1], 5 if i==0 else 3)
+        for i in range(len(nfs)-1)
+    ] + [nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c)]
+
+def get_cnn_model(data, nfs): return nn.Sequential(*get_cnn_layers(data, nfs))
+
+
+# -
+
+# And this helper function will quickly give us everything needed to run the training.
+
+#export
+def get_runner(model, data, lr=0.6, cbs=None, opt_func=None, loss_func = F.cross_entropy):
+    if opt_func is None: opt_func = optim.SGD
+    opt = opt_func(model.parameters(), lr=lr)
+    learn = Learner(model, opt, loss_func, data)
+    return learn, Runner(cb_funcs=listify(cbs))
+
+
+model = get_cnn_model(data, nfs)
+learn,run = get_runner(model, data, lr=0.4, cbs=cbfs)
+
+model
+
+run.fit(3, learn)
+
+
+# ## Hooks
+
+# ### Manual insertion
+
+# Let's say we want to do some telemetry, and want the mean and standard deviation of each activations in the model. First we can do it manually like this:
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=4353)
+
+class SequentialModel(nn.Module):
+    def __init__(self, *layers):
+        super().__init__()
+        self.layers = nn.ModuleList(layers)
+        self.act_means = [[] for _ in layers]
+        self.act_stds  = [[] for _ in layers]
+        
+    def __call__(self, x):
+        for i,l in enumerate(self.layers):
+            x = l(x)
+            self.act_means[i].append(x.data.mean())
+            self.act_stds [i].append(x.data.std ())
+        return x
+    
+    def __iter__(self): return iter(self.layers)
+
+
+model =  SequentialModel(*get_cnn_layers(data, nfs))
+learn,run = get_runner(model, data, lr=0.9, cbs=cbfs)
+
+run.fit(2, learn)
+
+# Now we can have a look at the means and stds of the activations at the beginning of training.
+
+for l in model.act_means: plt.plot(l)
+plt.legend(range(6));
+
+for l in model.act_stds: plt.plot(l)
+plt.legend(range(6));
+
+for l in model.act_means: plt.plot(l[:10])
+plt.legend(range(6));
+
+for l in model.act_stds: plt.plot(l[:10])
+plt.legend(range(6));
+
+# ### Pytorch hooks
+
+# Hooks are PyTorch object you can add to any nn.Module. A hook will be called when a layer, it is registered to, is executed during the forward pass (forward hook) or the backward pass (backward hook).
+#
+# Hooks don't require us to rewrite the model.
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=4693)
+
+model = get_cnn_model(data, nfs)
+learn,run = get_runner(model, data, lr=0.5, cbs=cbfs)
+
+act_means = [[] for _ in model]
+act_stds  = [[] for _ in model]
+
+
+# A hook is attached to a layer, and needs to have a function that takes three arguments: module, input, output. Here we store the mean and std of the output in the correct position of our list.
+
+def append_stats(i, mod, inp, outp):
+    act_means[i].append(outp.data.mean())
+    act_stds [i].append(outp.data.std())
+
+
+for i,m in enumerate(model): m.register_forward_hook(partial(append_stats, i))
+
+run.fit(1, learn)
+
+for o in act_means: plt.plot(o)
+plt.legend(range(5));
+
+
+# ### Hook class
+
+# We can refactor this in a Hook class. It's very important to remove the hooks when they are deleted, otherwise there will be references kept and the memory won't be properly released when your model is deleted.
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=4836)
+
+# +
+#export
+def children(m): return list(m.children())
+
+class Hook():
+    def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self))
+    def remove(self): self.hook.remove()
+    def __del__(self): self.remove()
+
+def append_stats(hook, mod, inp, outp):
+    if not hasattr(hook,'stats'): hook.stats = ([],[])
+    means,stds = hook.stats
+    means.append(outp.data.mean())
+    stds .append(outp.data.std())
+
+
+# -
+
+# NB: In fastai we use a `bool` param to choose whether to make it a forward or backward hook. In the above version we're only supporting forward hooks.
+
+model = get_cnn_model(data, nfs)
+learn,run = get_runner(model, data, lr=0.5, cbs=cbfs)
+
+hooks = [Hook(l, append_stats) for l in children(model[:4])]
+
+run.fit(1, learn)
+
+for h in hooks:
+    plt.plot(h.stats[0])
+    h.remove()
+plt.legend(range(4));
+
+
+# ### A Hooks class
+
+# Let's design our own class that can contain a list of objects. It will behave a bit like a numpy array in the sense that we can index into it via:
+# - a single index
+# - a slice (like 1:5)
+# - a list of indices
+# - a mask of indices (`[True,False,False,True,...]`)
+#
+# The `__iter__` method is there to be able to do things like `for x in ...`.
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=4972)
+
+#export
+class ListContainer():
+    def __init__(self, items): self.items = listify(items)
+    def __getitem__(self, idx):
+        if isinstance(idx, (int,slice)): return self.items[idx]
+        if isinstance(idx[0],bool):
+            assert len(idx)==len(self) # bool mask
+            return [o for m,o in zip(idx,self.items) if m]
+        return [self.items[i] for i in idx]
+    def __len__(self): return len(self.items)
+    def __iter__(self): return iter(self.items)
+    def __setitem__(self, i, o): self.items[i] = o
+    def __delitem__(self, i): del(self.items[i])
+    def __repr__(self):
+        res = f'{self.__class__.__name__} ({len(self)} items)\n{self.items[:10]}'
+        if len(self)>10: res = res[:-1]+ '...]'
+        return res
+
+
+ListContainer(range(10))
+
+ListContainer(range(100))
+
+t = ListContainer(range(10))
+t[[1,2]], t[[False]*8 + [True,False]]
+
+# We can use it to write a `Hooks` class that contains several hooks. We will also use it in the next notebook as a container for our objects in the data block API.
+
+# +
+#export
+from torch.nn import init
+
+class Hooks(ListContainer):
+    def __init__(self, ms, f): super().__init__([Hook(m, f) for m in ms])
+    def __enter__(self, *args): return self
+    def __exit__ (self, *args): self.remove()
+    def __del__(self): self.remove()
+
+    def __delitem__(self, i):
+        self[i].remove()
+        super().__delitem__(i)
+        
+    def remove(self):
+        for h in self: h.remove()
+
+
+# -
+
+model = get_cnn_model(data, nfs).cuda()
+learn,run = get_runner(model, data, lr=0.9, cbs=cbfs)
+
+hooks = Hooks(model, append_stats)
+hooks
+
+hooks.remove()
+
+x,y = next(iter(data.train_dl))
+x = mnist_resize(x).cuda()
+
+x.mean(),x.std()
+
+p = model[0](x)
+p.mean(),p.std()
+
+for l in model:
+    if isinstance(l, nn.Sequential):
+        init.kaiming_normal_(l[0].weight)
+        l[0].bias.data.zero_()
+
+p = model[0](x)
+p.mean(),p.std()
+
+# Having given an `__enter__` and `__exit__` method to our `Hooks` class, we can use it as a context manager. This makes sure that onces we are out of the `with` block, all the hooks have been removed and aren't there to pollute our memory.
+
+with Hooks(model, append_stats) as hooks:
+    run.fit(2, learn)
+    fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))
+    for h in hooks:
+        ms,ss = h.stats
+        ax0.plot(ms[:10])
+        ax1.plot(ss[:10])
+    plt.legend(range(6));
+    
+    fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))
+    for h in hooks:
+        ms,ss = h.stats
+        ax0.plot(ms)
+        ax1.plot(ss)
+    plt.legend(range(6));
+
+
+# ### Other statistics
+
+# Let's store more than the means and stds and plot histograms of our activations now.
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=5233)
+
+def append_stats(hook, mod, inp, outp):
+    if not hasattr(hook,'stats'): hook.stats = ([],[],[])
+    means,stds,hists = hook.stats
+    means.append(outp.data.mean().cpu())
+    stds .append(outp.data.std().cpu())
+    hists.append(outp.data.cpu().histc(40,0,10)) #histc isn't implemented on the GPU
+
+
+model = get_cnn_model(data, nfs).cuda()
+learn,run = get_runner(model, data, lr=0.9, cbs=cbfs)
+
+for l in model:
+    if isinstance(l, nn.Sequential):
+        init.kaiming_normal_(l[0].weight)
+        l[0].bias.data.zero_()
+
+with Hooks(model, append_stats) as hooks: run.fit(1, learn)
+
+
+# Thanks to @ste for initial version of histgram plotting code
+def get_hist(h): return torch.stack(h.stats[2]).t().float().log1p()
+
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=5582)
+
+fig,axes = plt.subplots(2,2, figsize=(15,6))
+for ax,h in zip(axes.flatten(), hooks[:4]):
+    ax.imshow(get_hist(h), origin='lower')
+    ax.axis('off')
+plt.tight_layout()
+
+
+# From the histograms, we can easily get more informations like the min or max of the activations
+
+def get_min(h):
+    h1 = torch.stack(h.stats[2]).t().float()
+    return h1[:2].sum(0)/h1.sum(0)
+
+
+fig,axes = plt.subplots(2,2, figsize=(15,6))
+for ax,h in zip(axes.flatten(), hooks[:4]):
+    ax.plot(get_min(h))
+    ax.set_ylim(0,1)
+plt.tight_layout()
+
+
+# ## Generalized ReLU
+
+# Now let's use our model with a generalized ReLU that can be shifted and with maximum value.
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=5390)
+
+# +
+#export
+def get_cnn_layers(data, nfs, layer, **kwargs):
+    nfs = [1] + nfs
+    return [layer(nfs[i], nfs[i+1], 5 if i==0 else 3, **kwargs)
+            for i in range(len(nfs)-1)] + [
+        nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c)]
+
+def conv_layer(ni, nf, ks=3, stride=2, **kwargs):
+    return nn.Sequential(
+        nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride), GeneralRelu(**kwargs))
+
+class GeneralRelu(nn.Module):
+    def __init__(self, leak=None, sub=None, maxv=None):
+        super().__init__()
+        self.leak,self.sub,self.maxv = leak,sub,maxv
+
+    def forward(self, x): 
+        x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)
+        if self.sub is not None: x.sub_(self.sub)
+        if self.maxv is not None: x.clamp_max_(self.maxv)
+        return x
+
+def init_cnn(m, uniform=False):
+    f = init.kaiming_uniform_ if uniform else init.kaiming_normal_
+    for l in m:
+        if isinstance(l, nn.Sequential):
+            f(l[0].weight, a=0.1)
+            l[0].bias.data.zero_()
+
+def get_cnn_model(data, nfs, layer, **kwargs):
+    return nn.Sequential(*get_cnn_layers(data, nfs, layer, **kwargs))
+
+
+# -
+
+def append_stats(hook, mod, inp, outp):
+    if not hasattr(hook,'stats'): hook.stats = ([],[],[])
+    means,stds,hists = hook.stats
+    means.append(outp.data.mean().cpu())
+    stds .append(outp.data.std().cpu())
+    hists.append(outp.data.cpu().histc(40,-7,7))
+
+
+model =  get_cnn_model(data, nfs, conv_layer, leak=0.1, sub=0.4, maxv=6.)
+init_cnn(model)
+learn,run = get_runner(model, data, lr=0.9, cbs=cbfs)
+
+with Hooks(model, append_stats) as hooks:
+    run.fit(1, learn)
+    fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))
+    for h in hooks:
+        ms,ss,hi = h.stats
+        ax0.plot(ms[:10])
+        ax1.plot(ss[:10])
+        h.remove()
+    plt.legend(range(5));
+    
+    fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))
+    for h in hooks:
+        ms,ss,hi = h.stats
+        ax0.plot(ms)
+        ax1.plot(ss)
+    plt.legend(range(5));
+
+fig,axes = plt.subplots(2,2, figsize=(15,6))
+for ax,h in zip(axes.flatten(), hooks[:4]):
+    ax.imshow(get_hist(h), origin='lower')
+    ax.axis('off')
+plt.tight_layout()
+
+
+def get_min(h):
+    h1 = torch.stack(h.stats[2]).t().float()
+    return h1[19:22].sum(0)/h1.sum(0)
+
+
+fig,axes = plt.subplots(2,2, figsize=(15,6))
+for ax,h in zip(axes.flatten(), hooks[:4]):
+    ax.plot(get_min(h))
+    ax.set_ylim(0,1)
+plt.tight_layout()
+
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=5705)
+
+#export
+def get_learn_run(nfs, data, lr, layer, cbs=None, opt_func=None, uniform=False, **kwargs):
+    model = get_cnn_model(data, nfs, layer, **kwargs)
+    init_cnn(model, uniform=uniform)
+    return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func)
+
+
+sched = combine_scheds([0.5, 0.5], [sched_cos(0.2, 1.), sched_cos(1., 0.1)]) 
+
+learn,run = get_learn_run(nfs, data, 1., conv_layer, cbs=cbfs+[partial(ParamScheduler,'lr', sched)])
+
+run.fit(8, learn)
+
+# Uniform init may provide more useful initial weights (normal distribution puts a lot of them at 0).
+
+learn,run = get_learn_run(nfs, data, 1., conv_layer, uniform=True,
+                          cbs=cbfs+[partial(ParamScheduler,'lr', sched)])
+
+run.fit(8, learn)
+
+# ## Export
+
+# Here's a handy way to export our module without needing to update the file name - after we define this, we can just use `nb_auto_export()` in the future (h/t Stas Bekman):
+
+#export
+from IPython.display import display, Javascript
+def nb_auto_export():
+    display(Javascript("""{
+const ip = IPython.notebook
+if (ip) {
+    ip.save_notebook()
+    console.log('a')
+    const s = `!python notebook2script.py ${ip.notebook_name}`
+    if (ip.kernel) { ip.kernel.execute(s) }
+}
+}"""))
+
+
+nb_auto_export()
+
+
diff --git a/nbs/dl2/07_batchnorm.ipynb b/nbs/dl2/07_batchnorm.ipynb
index e53803c..5cd6c96 100644
--- ./nbs/dl2/07_batchnorm.ipynb
+++ ./nbs/dl2/07_batchnorm.ipynb
@@ -1045,6 +1045,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/07_batchnorm.py b/nbs/dl2/07_batchnorm.py
new file mode 100644
index 0000000..0f4ea42
--- /dev/null
+++ ./nbs/dl2/07_batchnorm.py
@@ -0,0 +1,421 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+#export
+from exp.nb_06 import *
+
+# ## ConvNet
+
+# Let's get the data and training interface from where we left in the last notebook.
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=5899)
+
+# +
+x_train,y_train,x_valid,y_valid = get_data()
+
+x_train,x_valid = normalize_to(x_train,x_valid)
+train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)
+
+nh,bs = 50,512
+c = y_train.max().item()+1
+loss_func = F.cross_entropy
+
+data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)
+# -
+
+mnist_view = view_tfm(1,28,28)
+cbfs = [Recorder,
+        partial(AvgStatsCallback,accuracy),
+        CudaCallback,
+        partial(BatchTransformXCallback, mnist_view)]
+
+nfs = [8,16,32,64,64]
+
+learn,run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs)
+
+
+# %time run.fit(2, learn)
+
+# ## Batchnorm
+
+# ### Custom
+
+# Let's start by building our own `BatchNorm` layer from scratch.
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=6018)
+
+class BatchNorm(nn.Module):
+    def __init__(self, nf, mom=0.1, eps=1e-5):
+        super().__init__()
+        # NB: pytorch bn mom is opposite of what you'd expect
+        self.mom,self.eps = mom,eps
+        self.mults = nn.Parameter(torch.ones (nf,1,1))
+        self.adds  = nn.Parameter(torch.zeros(nf,1,1))
+        self.register_buffer('vars',  torch.ones(1,nf,1,1))
+        self.register_buffer('means', torch.zeros(1,nf,1,1))
+
+    def update_stats(self, x):
+        m = x.mean((0,2,3), keepdim=True)
+        v = x.var ((0,2,3), keepdim=True)
+        self.means.lerp_(m, self.mom)
+        self.vars.lerp_ (v, self.mom)
+        return m,v
+        
+    def forward(self, x):
+        if self.training:
+            with torch.no_grad(): m,v = self.update_stats(x)
+        else: m,v = self.means,self.vars
+        x = (x-m) / (v+self.eps).sqrt()
+        return x*self.mults + self.adds
+
+
+def conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs):
+    # No bias needed if using bn
+    layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=not bn),
+              GeneralRelu(**kwargs)]
+    if bn: layers.append(BatchNorm(nf))
+    return nn.Sequential(*layers)
+
+
+# +
+#export
+def init_cnn_(m, f):
+    if isinstance(m, nn.Conv2d):
+        f(m.weight, a=0.1)
+        if getattr(m, 'bias', None) is not None: m.bias.data.zero_()
+    for l in m.children(): init_cnn_(l, f)
+
+def init_cnn(m, uniform=False):
+    f = init.kaiming_uniform_ if uniform else init.kaiming_normal_
+    init_cnn_(m, f)
+
+def get_learn_run(nfs, data, lr, layer, cbs=None, opt_func=None, uniform=False, **kwargs):
+    model = get_cnn_model(data, nfs, layer, **kwargs)
+    init_cnn(model, uniform=uniform)
+    return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func)
+
+
+# -
+
+# We can then use it in training and see how it helps keep the activations means to 0 and the std to 1.
+
+learn,run = get_learn_run(nfs, data, 0.9, conv_layer, cbs=cbfs)
+
+with Hooks(learn.model, append_stats) as hooks:
+    run.fit(1, learn)
+    fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))
+    for h in hooks[:-1]:
+        ms,ss = h.stats
+        ax0.plot(ms[:10])
+        ax1.plot(ss[:10])
+        h.remove()
+    plt.legend(range(6));
+    
+    fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))
+    for h in hooks[:-1]:
+        ms,ss = h.stats
+        ax0.plot(ms)
+        ax1.plot(ss)
+
+learn,run = get_learn_run(nfs, data, 1.0, conv_layer, cbs=cbfs)
+
+
+# %time run.fit(3, learn)
+
+# ### Builtin batchnorm
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=6679)
+
+#export
+def conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs):
+    layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=not bn),
+              GeneralRelu(**kwargs)]
+    if bn: layers.append(nn.BatchNorm2d(nf, eps=1e-5, momentum=0.1))
+    return nn.Sequential(*layers)
+
+
+learn,run = get_learn_run(nfs, data, 1., conv_layer, cbs=cbfs)
+
+# %time run.fit(3, learn)
+
+# ### With scheduler
+
+# Now let's add the usual warm-up/annealing.
+
+sched = combine_scheds([0.3, 0.7], [sched_lin(0.6, 2.), sched_lin(2., 0.1)]) 
+
+learn,run = get_learn_run(nfs, data, 0.9, conv_layer, cbs=cbfs
+                          +[partial(ParamScheduler,'lr', sched)])
+
+run.fit(8, learn)
+
+
+# ## More norms
+
+# ### Layer norm
+
+# From [the paper](https://arxiv.org/abs/1607.06450): "*batch normalization cannot be applied to online learning tasks or to extremely large distributed models where the minibatches have to be small*".
+
+# General equation for a norm layer with learnable affine:
+#
+# $$y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta$$
+#
+# The difference with BatchNorm is
+# 1. we don't keep a moving average
+# 2. we don't average over the batches dimension but over the hidden dimension, so it's independent of the batch size
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=6717)
+
+class LayerNorm(nn.Module):
+    __constants__ = ['eps']
+    def __init__(self, eps=1e-5):
+        super().__init__()
+        self.eps = eps
+        self.mult = nn.Parameter(tensor(1.))
+        self.add  = nn.Parameter(tensor(0.))
+
+    def forward(self, x):
+        m = x.mean((1,2,3), keepdim=True)
+        v = x.var ((1,2,3), keepdim=True)
+        x = (x-m) / ((v+self.eps).sqrt())
+        return x*self.mult + self.add
+
+
+def conv_ln(ni, nf, ks=3, stride=2, bn=True, **kwargs):
+    layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=True),
+              GeneralRelu(**kwargs)]
+    if bn: layers.append(LayerNorm())
+    return nn.Sequential(*layers)
+
+
+learn,run = get_learn_run(nfs, data, 0.8, conv_ln, cbs=cbfs)
+
+
+# %time run.fit(3, learn)
+
+# *Thought experiment*: can this distinguish foggy days from sunny days (assuming you're using it before the first conv)?
+
+# ### Instance norm
+
+# From [the paper](https://arxiv.org/abs/1607.08022): 
+
+# The key difference between **contrast** and batch normalization is that the latter applies the normalization to a  whole batch of images instead for single ones:
+#
+# \begin{equation}\label{eq:bnorm}
+#     y_{tijk} =  \frac{x_{tijk} - \mu_{i}}{\sqrt{\sigma_i^2 + \epsilon}},
+#     \quad
+#     \mu_i = \frac{1}{HWT}\sum_{t=1}^T\sum_{l=1}^W \sum_{m=1}^H x_{tilm},
+#     \quad
+#     \sigma_i^2 = \frac{1}{HWT}\sum_{t=1}^T\sum_{l=1}^W \sum_{m=1}^H (x_{tilm} - mu_i)^2.
+# \end{equation}
+#
+# In order to combine the effects of instance-specific normalization and batch normalization, we propose to replace the latter by the *instance normalization* (also known as *contrast normalization*) layer:
+#
+# \begin{equation}\label{eq:inorm}
+#     y_{tijk} =  \frac{x_{tijk} - \mu_{ti}}{\sqrt{\sigma_{ti}^2 + \epsilon}},
+#     \quad
+#     \mu_{ti} = \frac{1}{HW}\sum_{l=1}^W \sum_{m=1}^H x_{tilm},
+#     \quad
+#     \sigma_{ti}^2 = \frac{1}{HW}\sum_{l=1}^W \sum_{m=1}^H (x_{tilm} - mu_{ti})^2.
+# \end{equation}
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=7114)
+
+class InstanceNorm(nn.Module):
+    __constants__ = ['eps']
+    def __init__(self, nf, eps=1e-0):
+        super().__init__()
+        self.eps = eps
+        self.mults = nn.Parameter(torch.ones (nf,1,1))
+        self.adds  = nn.Parameter(torch.zeros(nf,1,1))
+
+    def forward(self, x):
+        m = x.mean((2,3), keepdim=True)
+        v = x.var ((2,3), keepdim=True)
+        res = (x-m) / ((v+self.eps).sqrt())
+        return res*self.mults + self.adds
+
+
+def conv_in(ni, nf, ks=3, stride=2, bn=True, **kwargs):
+    layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=True),
+              GeneralRelu(**kwargs)]
+    if bn: layers.append(InstanceNorm(nf))
+    return nn.Sequential(*layers)
+
+
+learn,run = get_learn_run(nfs, data, 0.1, conv_in, cbs=cbfs)
+
+# %time run.fit(3, learn)
+
+# *Question*: why can't this classify anything?
+
+# Lost in all those norms? The authors from the [group norm paper](https://arxiv.org/pdf/1803.08494.pdf) have you covered:
+#
+# ![Various norms](images/norms.png)
+
+# ### Group norm
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=7213)
+
+# *From the PyTorch docs:*
+
+# `GroupNorm(num_groups, num_channels, eps=1e-5, affine=True)`
+#
+# The input channels are separated into `num_groups` groups, each containing
+# ``num_channels / num_groups`` channels. The mean and standard-deviation are calculated
+# separately over the each group. $\gamma$ and $\beta$ are learnable
+# per-channel affine transform parameter vectorss of size `num_channels` if
+# `affine` is ``True``.
+#
+# This layer uses statistics computed from input data in both training and
+# evaluation modes.
+#
+# Args:
+# -    num_groups (int): number of groups to separate the channels into
+# -    num_channels (int): number of channels expected in input
+# -    eps: a value added to the denominator for numerical stability. Default: 1e-5
+# -    affine: a boolean value that when set to ``True``, this module
+#         has learnable per-channel affine parameters initialized to ones (for weights)
+#         and zeros (for biases). Default: ``True``.
+#
+# Shape:
+# - Input: `(N, num_channels, *)`
+# - Output: `(N, num_channels, *)` (same shape as input)
+#
+# Examples::
+#
+#     >>> input = torch.randn(20, 6, 10, 10)
+#     >>> # Separate 6 channels into 3 groups
+#     >>> m = nn.GroupNorm(3, 6)
+#     >>> # Separate 6 channels into 6 groups (equivalent with InstanceNorm)
+#     >>> m = nn.GroupNorm(6, 6)
+#     >>> # Put all 6 channels into a single group (equivalent with LayerNorm)
+#     >>> m = nn.GroupNorm(1, 6)
+#     >>> # Activating the module
+#     >>> output = m(input)
+
+# ## Fix small batch sizes
+
+# ### What's the problem?
+
+# When we compute the statistics (mean and std) for a BatchNorm Layer on a small batch, it is possible that we get a standard deviation very close to 0. because there aren't many samples (the variance of one thing is 0. since it's equal to its mean).
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=7304)
+
+data = DataBunch(*get_dls(train_ds, valid_ds, 2), c)
+
+
+def conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs):
+    layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=not bn),
+              GeneralRelu(**kwargs)]
+    if bn: layers.append(nn.BatchNorm2d(nf, eps=1e-5, momentum=0.1))
+    return nn.Sequential(*layers)
+
+
+learn,run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs)
+
+
+# %time run.fit(1, learn)
+
+# ### Running Batch Norm
+
+# To solve this problem we introduce a Running BatchNorm that uses smoother running mean and variance for the mean and std.
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=7516)
+
+class RunningBatchNorm(nn.Module):
+    def __init__(self, nf, mom=0.1, eps=1e-5):
+        super().__init__()
+        self.mom,self.eps = mom,eps
+        self.mults = nn.Parameter(torch.ones (nf,1,1))
+        self.adds = nn.Parameter(torch.zeros(nf,1,1))
+        self.register_buffer('sums', torch.zeros(1,nf,1,1))
+        self.register_buffer('sqrs', torch.zeros(1,nf,1,1))
+        self.register_buffer('batch', tensor(0.))
+        self.register_buffer('count', tensor(0.))
+        self.register_buffer('step', tensor(0.))
+        self.register_buffer('dbias', tensor(0.))
+
+    def update_stats(self, x):
+        bs,nc,*_ = x.shape
+        self.sums.detach_()
+        self.sqrs.detach_()
+        dims = (0,2,3)
+        s = x.sum(dims, keepdim=True)
+        ss = (x*x).sum(dims, keepdim=True)
+        c = self.count.new_tensor(x.numel()/nc)
+        mom1 = 1 - (1-self.mom)/math.sqrt(bs-1)
+        self.mom1 = self.dbias.new_tensor(mom1)
+        self.sums.lerp_(s, self.mom1)
+        self.sqrs.lerp_(ss, self.mom1)
+        self.count.lerp_(c, self.mom1)
+        self.dbias = self.dbias*(1-self.mom1) + self.mom1
+        self.batch += bs
+        self.step += 1
+
+    def forward(self, x):
+        if self.training: self.update_stats(x)
+        sums = self.sums
+        sqrs = self.sqrs
+        c = self.count
+        if self.step<100:
+            sums = sums / self.dbias
+            sqrs = sqrs / self.dbias
+            c    = c    / self.dbias
+        means = sums/c
+        vars = (sqrs/c).sub_(means*means)
+        if bool(self.batch < 20): vars.clamp_min_(0.01)
+        x = (x-means).div_((vars.add_(self.eps)).sqrt())
+        return x.mul_(self.mults).add_(self.adds)
+
+
+def conv_rbn(ni, nf, ks=3, stride=2, bn=True, **kwargs):
+    layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=not bn),
+              GeneralRelu(**kwargs)]
+    if bn: layers.append(RunningBatchNorm(nf))
+    return nn.Sequential(*layers)
+
+
+learn,run = get_learn_run(nfs, data, 0.4, conv_rbn, cbs=cbfs)
+
+# %time run.fit(1, learn)
+
+# This solves the small batch size issue!
+
+# ### What can we do in a single epoch?
+
+# Now let's see with a decent batch size what result we can get.
+
+# [Jump_to lesson 10 video](https://course.fast.ai/videos/?lesson=10&t=8068)
+
+data = DataBunch(*get_dls(train_ds, valid_ds, 32), c)
+
+learn,run = get_learn_run(nfs, data, 0.9, conv_rbn, cbs=cbfs
+                          +[partial(ParamScheduler,'lr', sched_lin(1., 0.2))])
+
+# %time run.fit(1, learn)
+
+# ## Export
+
+nb_auto_export()
+
+
diff --git a/nbs/dl2/07a_lsuv.ipynb b/nbs/dl2/07a_lsuv.ipynb
index a33390e..85f684d 100644
--- ./nbs/dl2/07a_lsuv.ipynb
+++ ./nbs/dl2/07a_lsuv.ipynb
@@ -452,6 +452,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/07a_lsuv.py b/nbs/dl2/07a_lsuv.py
new file mode 100644
index 0000000..6c62774
--- /dev/null
+++ ./nbs/dl2/07a_lsuv.py
@@ -0,0 +1,163 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+#export
+from exp.nb_07 import *
+
+# ## Layerwise Sequential Unit Variance (LSUV)
+
+# Getting the MNIST data and a CNN
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=235)
+
+# +
+x_train,y_train,x_valid,y_valid = get_data()
+
+x_train,x_valid = normalize_to(x_train,x_valid)
+train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)
+
+nh,bs = 50,512
+c = y_train.max().item()+1
+loss_func = F.cross_entropy
+
+data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)
+# -
+
+mnist_view = view_tfm(1,28,28)
+cbfs = [Recorder,
+        partial(AvgStatsCallback,accuracy),
+        CudaCallback,
+        partial(BatchTransformXCallback, mnist_view)]
+
+nfs = [8,16,32,64,64]
+
+
+class ConvLayer(nn.Module):
+    def __init__(self, ni, nf, ks=3, stride=2, sub=0., **kwargs):
+        super().__init__()
+        self.conv = nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=True)
+        self.relu = GeneralRelu(sub=sub, **kwargs)
+    
+    def forward(self, x): return self.relu(self.conv(x))
+    
+    @property
+    def bias(self): return -self.relu.sub
+    @bias.setter
+    def bias(self,v): self.relu.sub = -v
+    @property
+    def weight(self): return self.conv.weight
+
+
+learn,run = get_learn_run(nfs, data, 0.6, ConvLayer, cbs=cbfs)
+
+# Now we're going to look at the paper [All You Need is a Good Init](https://arxiv.org/pdf/1511.06422.pdf), which introduces *Layer-wise Sequential Unit-Variance* (*LSUV*). We initialize our neural net with the usual technique, then we pass a batch through the model and check the outputs of the linear and convolutional layers. We can then rescale the weights according to the actual variance we observe on the activations, and subtract the mean we observe from the initial bias. That way we will have activations that stay normalized.
+#
+# We repeat this process until we are satisfied with the mean/variance we observe.
+#
+# Let's start by looking at a baseline:
+
+run.fit(2, learn)
+
+# Now we recreate our model and we'll try again with LSUV. Hopefully, we'll get better results!
+
+learn,run = get_learn_run(nfs, data, 0.6, ConvLayer, cbs=cbfs)
+
+
+# Helper function to get one batch of a given dataloader, with the callbacks called to preprocess it.
+
+#export
+def get_batch(dl, run):
+    run.xb,run.yb = next(iter(dl))
+    for cb in run.cbs: cb.set_runner(run)
+    run('begin_batch')
+    return run.xb,run.yb
+
+
+xb,yb = get_batch(data.train_dl, run)
+
+
+# We only want the outputs of convolutional or linear layers. To find them, we need a recursive function. We can use `sum(list, [])` to concatenate the lists the function finds (`sum` applies the + operate between the elements of the list you pass it, beginning with the initial state in the second argument).
+
+# +
+#export
+def find_modules(m, cond):
+    if cond(m): return [m]
+    return sum([find_modules(o,cond) for o in m.children()], [])
+
+def is_lin_layer(l):
+    lin_layers = (nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.Linear, nn.ReLU)
+    return isinstance(l, lin_layers)
+
+
+# -
+
+mods = find_modules(learn.model, lambda o: isinstance(o,ConvLayer))
+
+mods
+
+
+# This is a helper function to grab the mean and std of the output of a hooked layer.
+
+def append_stat(hook, mod, inp, outp):
+    d = outp.data
+    hook.mean,hook.std = d.mean().item(),d.std().item()
+
+
+mdl = learn.model.cuda()
+
+# So now we can look at the mean and std of the conv layers of our model.
+
+with Hooks(mods, append_stat) as hooks:
+    mdl(xb)
+    for hook in hooks: print(hook.mean,hook.std)
+
+
+# We first adjust the bias terms to make the means 0, then we adjust the standard deviations to make the stds 1 (with a threshold of 1e-3). The `mdl(xb) is not None` clause is just there to pass `xb` through `mdl` and compute all the activations so that the hooks get updated. 
+
+#export
+def lsuv_module(m, xb):
+    h = Hook(m, append_stat)
+
+    while mdl(xb) is not None and abs(h.mean)  > 1e-3: m.bias -= h.mean
+    while mdl(xb) is not None and abs(h.std-1) > 1e-3: m.weight.data /= h.std
+
+    h.remove()
+    return h.mean,h.std
+
+
+# We execute that initialization on all the conv layers in order:
+
+for m in mods: print(lsuv_module(m, xb))
+
+# Note that the mean doesn't exactly stay at 0. since we change the standard deviation after by scaling the weight.
+
+# Then training is beginning on better grounds.
+
+# %time run.fit(2, learn)
+
+# LSUV is particularly useful for more complex and deeper architectures that are hard to initialize to get unit variance at the last layer.
+
+# ## Export
+
+# !python notebook2script.py 07a_lsuv.ipynb
+
+
diff --git a/nbs/dl2/08_data_block.ipynb b/nbs/dl2/08_data_block.ipynb
index 31289d9..98ba5ab 100644
--- ./nbs/dl2/08_data_block.ipynb
+++ ./nbs/dl2/08_data_block.ipynb
@@ -1865,6 +1865,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/08_data_block.py b/nbs/dl2/08_data_block.py
new file mode 100644
index 0000000..2415a13
--- /dev/null
+++ ./nbs/dl2/08_data_block.py
@@ -0,0 +1,593 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Data block API foundations
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+#export
+from exp.nb_07a import *
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=600)
+
+datasets.URLs.IMAGENETTE_160
+
+# ## Image ItemList
+
+# Previously we were reading in to RAM the whole MNIST dataset at once, loading it as a pickle file. We can't do that for datasets larger than our RAM capacity, so instead we leave the images on disk and just grab the ones we need for each mini-batch as we use them.
+#
+# Let's use the [imagenette dataset](https://github.com/fastai/imagenette/blob/master/README.md) and build the data blocks we need along the way.
+
+# ### Get images
+
+path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)
+path
+
+# To be able to look at what's inside a directory from a notebook, we add the `.ls` method to `Path` with a monkey-patch.
+
+#export
+import PIL,os,mimetypes
+Path.ls = lambda x: list(x.iterdir())
+
+path.ls()
+
+(path/'val').ls()
+
+# Let's have a look inside a class folder (the first class is tench):
+
+path_tench = path/'val'/'n01440764'
+
+img_fn = path_tench.ls()[0]
+img_fn
+
+img = PIL.Image.open(img_fn)
+img
+
+plt.imshow(img)
+
+import numpy
+imga = numpy.array(img)
+
+imga.shape
+
+imga[:10,:10,0]
+
+# Just in case there are other files in the directory (models, texts...) we want to keep only the images. Let's not write it out by hand, but instead use what's already on our computer (the MIME types database).
+
+#export
+image_extensions = set(k for k,v in mimetypes.types_map.items() if v.startswith('image/'))
+
+' '.join(image_extensions)
+
+
+#export
+def setify(o): return o if isinstance(o,set) else set(listify(o))
+
+
+test_eq(setify('aa'), {'aa'})
+test_eq(setify(['aa',1]), {'aa',1})
+test_eq(setify(None), set())
+test_eq(setify(1), {1})
+test_eq(setify({1}), {1})
+
+
+# Now let's walk through the directories and grab all the images. The first private function grabs all the images inside a given directory and the second one walks (potentially recursively) through all the folder in `path`.
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=1325)
+
+#export
+def _get_files(p, fs, extensions=None):
+    p = Path(p)
+    res = [p/f for f in fs if not f.startswith('.')
+           and ((not extensions) or f'.{f.split(".")[-1].lower()}' in extensions)]
+    return res
+
+
+t = [o.name for o in os.scandir(path_tench)]
+t = _get_files(path, t, extensions=image_extensions)
+t[:3]
+
+
+#export
+def get_files(path, extensions=None, recurse=False, include=None):
+    path = Path(path)
+    extensions = setify(extensions)
+    extensions = {e.lower() for e in extensions}
+    if recurse:
+        res = []
+        for i,(p,d,f) in enumerate(os.walk(path)): # returns (dirpath, dirnames, filenames)
+            if include is not None and i==0: d[:] = [o for o in d if o in include]
+            else:                            d[:] = [o for o in d if not o.startswith('.')]
+            res += _get_files(p, f, extensions)
+        return res
+    else:
+        f = [o.name for o in os.scandir(path) if o.is_file()]
+        return _get_files(path, f, extensions)
+
+
+get_files(path_tench, image_extensions)[:3]
+
+# We need the recurse argument when we start from `path` since the pictures are two level below in directories.
+
+get_files(path, image_extensions, recurse=True)[:3]
+
+all_fns = get_files(path, image_extensions, recurse=True)
+len(all_fns)
+
+
+# Imagenet is 100 times bigger than imagenette, so we need this to be fast.
+
+# %timeit -n 10 get_files(path, image_extensions, recurse=True)
+
+# ## Prepare for modeling
+
+# What we need to do:
+#
+# - Get files
+# - Split validation set
+#   - random%, folder name, csv, ...
+# - Label: 
+#   - folder name, file name/re, csv, ...
+# - Transform per image (optional)
+# - Transform to tensor
+# - DataLoader
+# - Transform per batch (optional)
+# - DataBunch
+# - Add test set (optional)
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=1728)
+
+# ### Get files
+
+# We use the `ListContainer` class from notebook 06 to store our objects in an `ItemList`. The `get` method will need to be subclassed to explain how to access an element (open an image for instance), then the private `_get` method can allow us to apply any additional transform to it.
+#
+# `new` will be used in conjunction with `__getitem__` (that works for one index or a list of indices) to create training and validation set from a single stream when we split the data.
+
+# +
+#export
+def compose(x, funcs, *args, order_key='_order', **kwargs):
+    key = lambda o: getattr(o, order_key, 0)
+    for f in sorted(listify(funcs), key=key): x = f(x, **kwargs)
+    return x
+
+class ItemList(ListContainer):
+    def __init__(self, items, path='.', tfms=None):
+        super().__init__(items)
+        self.path,self.tfms = Path(path),tfms
+
+    def __repr__(self): return f'{super().__repr__()}\nPath: {self.path}'
+    
+    def new(self, items, cls=None):
+        if cls is None: cls=self.__class__
+        return cls(items, self.path, tfms=self.tfms)
+    
+    def  get(self, i): return i
+    def _get(self, i): return compose(self.get(i), self.tfms)
+    
+    def __getitem__(self, idx):
+        res = super().__getitem__(idx)
+        if isinstance(res,list): return [self._get(o) for o in res]
+        return self._get(res)
+
+class ImageList(ItemList):
+    @classmethod
+    def from_files(cls, path, extensions=None, recurse=True, include=None, **kwargs):
+        if extensions is None: extensions = image_extensions
+        return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs)
+    
+    def get(self, fn): return PIL.Image.open(fn)
+
+
+# -
+
+# Transforms aren't only used for data augmentation. To allow total flexibility, `ImageList` returns the raw PIL image. The first thing is to convert it to 'RGB' (or something else).
+#
+# Transforms only need to be functions that take an element of the `ItemList` and transform it. If they need state, they can be defined as a class. Also, having them as a class allows to define an `_order` attribute (default 0) that is used to sort the transforms.
+
+# +
+#export
+class Transform(): _order=0
+
+class MakeRGB(Transform):
+    def __call__(self, item): return item.convert('RGB')
+
+def make_rgb(item): return item.convert('RGB')
+
+
+# -
+
+il = ImageList.from_files(path, tfms=make_rgb)
+
+il
+
+img = il[0]; img
+
+# We can also index with a range or a list of integers:
+
+il[:1]
+
+# ### Split validation set
+
+# Here, we need to split the files between those in the folder train and those in the folder val.
+
+fn = il.items[0]; fn
+
+# Since our filenames are `path` object, we can find the directory of the file with `.parent`. We need to go back two folders before since the last folders are the class names.
+
+fn.parent.parent.name
+
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=2175)
+
+# +
+#export
+def grandparent_splitter(fn, valid_name='valid', train_name='train'):
+    gp = fn.parent.parent.name
+    return True if gp==valid_name else False if gp==train_name else None
+
+def split_by_func(items, f):
+    mask = [f(o) for o in items]
+    # `None` values will be filtered out
+    f = [o for o,m in zip(items,mask) if m==False]
+    t = [o for o,m in zip(items,mask) if m==True ]
+    return f,t
+
+
+# -
+
+splitter = partial(grandparent_splitter, valid_name='val')
+
+# %time train,valid = split_by_func(il, splitter)
+
+len(train),len(valid)
+
+
+# Now that we can split our data, let's create the class that will contain it. It just needs two `ItemList` to be initialized, and we create a shortcut to all the unknown attributes by trying to grab them in the `train` `ItemList`. 
+
+#export
+class SplitData():
+    def __init__(self, train, valid): self.train,self.valid = train,valid
+        
+    def __getattr__(self,k): return getattr(self.train,k)
+    #This is needed if we want to pickle SplitData and be able to load it back without recursion errors
+    def __setstate__(self,data:Any): self.__dict__.update(data) 
+    
+    @classmethod
+    def split_by_func(cls, il, f):
+        lists = map(il.new, split_by_func(il.items, f))
+        return cls(*lists)
+
+    def __repr__(self): return f'{self.__class__.__name__}\nTrain: {self.train}\nValid: {self.valid}\n'
+
+
+sd = SplitData.split_by_func(il, splitter); sd
+
+# ### Labeling
+
+# Labeling has to be done *after* splitting, because it uses *training* set information to apply to the *validation* set, using a *Processor*.
+#
+# A *Processor* is a transformation that is applied to all the inputs once at initialization, with some *state* computed on the training set that is then applied without modification on the validation set (and maybe the test set or at inference time on a single item). For instance, it could be **processing texts** to **tokenize**, then **numericalize** them. In that case we want the validation set to be numericalized with exactly the same vocabulary as the training set.
+#
+# Another example is in **tabular data**, where we **fill missing values** with (for instance) the median computed on the training set. That statistic is stored in the inner state of the *Processor* and applied on the validation set.
+#
+# In our case, we want to **convert label strings to numbers** in a consistent and reproducible way. So we create a list of possible labels in the training set, and then convert our labels to numbers based on this *vocab*.
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=2368)
+
+# +
+#export
+from collections import OrderedDict
+
+def uniqueify(x, sort=False):
+    res = list(OrderedDict.fromkeys(x).keys())
+    if sort: res.sort()
+    return res
+
+
+# -
+
+# First, let's define the processor. We also define a `ProcessedItemList` with an `obj` method that can get the unprocessed items: for instance a processed label will be an index between 0 and the number of classes - 1, the corresponding `obj` will be the name of the class. The first one is needed by the model for the training, but the second one is better for displaying the objects.
+
+# +
+#export
+class Processor(): 
+    def process(self, items): return items
+
+class CategoryProcessor(Processor):
+    def __init__(self): self.vocab=None
+    
+    def __call__(self, items):
+        #The vocab is defined on the first use.
+        if self.vocab is None:
+            self.vocab = uniqueify(items)
+            self.otoi  = {v:k for k,v in enumerate(self.vocab)}
+        return [self.proc1(o) for o in items]
+    def proc1(self, item):  return self.otoi[item]
+    
+    def deprocess(self, idxs):
+        assert self.vocab is not None
+        return [self.deproc1(idx) for idx in idxs]
+    def deproc1(self, idx): return self.vocab[idx]
+
+
+# -
+
+# Here we label according to the folders of the images, so simply `fn.parent.name`. We label the training set first with a newly created `CategoryProcessor` so that it computes its inner `vocab` on that set. Then we label the validation set using the same processor, which means it uses the same `vocab`. The end result is another `SplitData` object.
+
+# +
+#export
+def parent_labeler(fn): return fn.parent.name
+
+def _label_by_func(ds, f, cls=ItemList): return cls([f(o) for o in ds.items], path=ds.path)
+
+#This is a slightly different from what was seen during the lesson,
+#   we'll discuss the changes in lesson 11
+class LabeledData():
+    def process(self, il, proc): return il.new(compose(il.items, proc))
+
+    def __init__(self, x, y, proc_x=None, proc_y=None):
+        self.x,self.y = self.process(x, proc_x),self.process(y, proc_y)
+        self.proc_x,self.proc_y = proc_x,proc_y
+        
+    def __repr__(self): return f'{self.__class__.__name__}\nx: {self.x}\ny: {self.y}\n'
+    def __getitem__(self,idx): return self.x[idx],self.y[idx]
+    def __len__(self): return len(self.x)
+    
+    def x_obj(self, idx): return self.obj(self.x, idx, self.proc_x)
+    def y_obj(self, idx): return self.obj(self.y, idx, self.proc_y)
+    
+    def obj(self, items, idx, procs):
+        isint = isinstance(idx, int) or (isinstance(idx,torch.LongTensor) and not idx.ndim)
+        item = items[idx]
+        for proc in reversed(listify(procs)):
+            item = proc.deproc1(item) if isint else proc.deprocess(item)
+        return item
+
+    @classmethod
+    def label_by_func(cls, il, f, proc_x=None, proc_y=None):
+        return cls(il, _label_by_func(il, f), proc_x=proc_x, proc_y=proc_y)
+
+def label_by_func(sd, f, proc_x=None, proc_y=None):
+    train = LabeledData.label_by_func(sd.train, f, proc_x=proc_x, proc_y=proc_y)
+    valid = LabeledData.label_by_func(sd.valid, f, proc_x=proc_x, proc_y=proc_y)
+    return SplitData(train,valid)
+
+
+# -
+
+ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())
+
+assert ll.train.proc_y is ll.valid.proc_y
+
+ll.train.y
+
+ll.train.y.items[0], ll.train.y_obj(0), ll.train.y_obj(slice(2))
+
+ll
+
+# ### Transform to tensor
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=3044)
+
+ll.train[0]
+
+ll.train[0][0]
+
+# To be able to put all our images in a batch, we need them to have all the same size. We can do this easily in PIL.
+
+ll.train[0][0].resize((128,128))
+
+
+# The first transform resizes to a given size, then we convert the image to a by tensor before converting it to float and dividing by 255. We will investigate data augmentation transforms at length in notebook 10.
+
+# +
+#export
+class ResizeFixed(Transform):
+    _order=10
+    def __init__(self,size):
+        if isinstance(size,int): size=(size,size)
+        self.size = size
+        
+    def __call__(self, item): return item.resize(self.size, PIL.Image.BILINEAR)
+
+def to_byte_tensor(item):
+    res = torch.ByteTensor(torch.ByteStorage.from_buffer(item.tobytes()))
+    w,h = item.size
+    return res.view(h,w,-1).permute(2,0,1)
+to_byte_tensor._order=20
+
+def to_float_tensor(item): return item.float().div_(255.)
+to_float_tensor._order=30
+
+# +
+tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor]
+
+il = ImageList.from_files(path, tfms=tfms)
+sd = SplitData.split_by_func(il, splitter)
+ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())
+
+
+# -
+
+# Here is a little convenience function to show an image from the corresponding tensor.
+
+#export
+def show_image(im, figsize=(3,3)):
+    plt.figure(figsize=figsize)
+    plt.axis('off')
+    plt.imshow(im.permute(1,2,0))
+
+
+x,y = ll.train[0]
+x.shape
+
+show_image(x)
+
+# ## Modeling
+
+# ### DataBunch
+
+# Now we are ready to put our datasets together in a `DataBunch`.
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=3226)
+
+bs=64
+
+train_dl,valid_dl = get_dls(ll.train,ll.valid,bs, num_workers=4)
+
+x,y = next(iter(train_dl))
+
+x.shape
+
+# We can still see the images in a batch and get the corresponding classes.
+
+show_image(x[0])
+ll.train.proc_y.vocab[y[0]]
+
+y
+
+
+# We change a little bit our `DataBunch` to add a few attributes: `c_in` (for channel in) and `c_out` (for channel out) instead of just `c`. This will help when we need to build our model.
+
+#export
+class DataBunch():
+    def __init__(self, train_dl, valid_dl, c_in=None, c_out=None):
+        self.train_dl,self.valid_dl,self.c_in,self.c_out = train_dl,valid_dl,c_in,c_out
+
+    @property
+    def train_ds(self): return self.train_dl.dataset
+
+    @property
+    def valid_ds(self): return self.valid_dl.dataset
+
+
+# Then we define a function that goes directly from the `SplitData` to a `DataBunch`.
+
+# +
+#export
+def databunchify(sd, bs, c_in=None, c_out=None, **kwargs):
+    dls = get_dls(sd.train, sd.valid, bs, **kwargs)
+    return DataBunch(*dls, c_in=c_in, c_out=c_out)
+
+SplitData.to_databunch = databunchify
+# -
+
+# This gives us the full summary on how to grab our data and put it in a `DataBunch`:
+
+# +
+path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)
+tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor]
+
+il = ImageList.from_files(path, tfms=tfms)
+sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='val'))
+ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())
+data = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=4)
+# -
+
+# ### Model
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=3360)
+
+cbfs = [partial(AvgStatsCallback,accuracy),
+        CudaCallback]
+
+# We will normalize with the statistics from a batch.
+
+m,s = x.mean((0,2,3)).cuda(),x.std((0,2,3)).cuda()
+m,s
+
+
+# +
+#export
+def normalize_chan(x, mean, std):
+    return (x-mean[...,None,None]) / std[...,None,None]
+
+_m = tensor([0.47, 0.48, 0.45])
+_s = tensor([0.29, 0.28, 0.30])
+norm_imagenette = partial(normalize_chan, mean=_m.cuda(), std=_s.cuda())
+# -
+
+cbfs.append(partial(BatchTransformXCallback, norm_imagenette))
+
+nfs = [64,64,128,256]
+
+# We build our model using [Bag of Tricks for Image Classification with Convolutional Neural Networks](https://arxiv.org/abs/1812.01187), in particular: we don't use a big conv 7x7 at first but three 3x3 convs, and don't go directly from 3 channels to 64 but progressively add those.
+
+# +
+#export
+import math
+def prev_pow_2(x): return 2**math.floor(math.log2(x))
+
+def get_cnn_layers(data, nfs, layer, **kwargs):
+    def f(ni, nf, stride=2): return layer(ni, nf, 3, stride=stride, **kwargs)
+    l1 = data.c_in
+    l2 = prev_pow_2(l1*3*3)
+    layers =  [f(l1  , l2  , stride=1),
+               f(l2  , l2*2, stride=2),
+               f(l2*2, l2*4, stride=2)]
+    nfs = [l2*4] + nfs
+    layers += [f(nfs[i], nfs[i+1]) for i in range(len(nfs)-1)]
+    layers += [nn.AdaptiveAvgPool2d(1), Lambda(flatten), 
+               nn.Linear(nfs[-1], data.c_out)]
+    return layers
+
+def get_cnn_model(data, nfs, layer, **kwargs):
+    return nn.Sequential(*get_cnn_layers(data, nfs, layer, **kwargs))
+
+def get_learn_run(nfs, data, lr, layer, cbs=None, opt_func=None, **kwargs):
+    model = get_cnn_model(data, nfs, layer, **kwargs)
+    init_cnn(model)
+    return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func)
+
+
+# -
+
+sched = combine_scheds([0.3,0.7], cos_1cycle_anneal(0.1,0.3,0.05))
+
+learn,run = get_learn_run(nfs, data, 0.2, conv_layer, cbs=cbfs+[
+    partial(ParamScheduler, 'lr', sched)
+])
+
+
+# Let's have a look at our model using Hooks. We print the layers and the shapes of their outputs.
+
+#export
+def model_summary(run, learn, data, find_all=False):
+    xb,yb = get_batch(data.valid_dl, run)
+    device = next(learn.model.parameters()).device#Model may not be on the GPU yet
+    xb,yb = xb.to(device),yb.to(device)
+    mods = find_modules(learn.model, is_lin_layer) if find_all else learn.model.children()
+    f = lambda hook,mod,inp,out: print(f"{mod}\n{out.shape}\n")
+    with Hooks(mods, f) as hooks: learn.model(xb)
+
+
+model_summary(run, learn, data)
+
+# And we can train the model:
+
+# %time run.fit(5, learn)
+
+# The [leaderboard](https://github.com/fastai/imagenette/blob/master/README.md) as this notebook is written has ~85% accuracy for 5 epochs at 128px size, so we're definitely on the right track!
+
+# ## Export
+
+# !python notebook2script.py 08_data_block.ipynb
+
+
diff --git a/nbs/dl2/09_optimizers.ipynb b/nbs/dl2/09_optimizers.ipynb
index 573dc02..72ee916 100644
--- ./nbs/dl2/09_optimizers.ipynb
+++ ./nbs/dl2/09_optimizers.ipynb
@@ -1317,6 +1317,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/09_optimizers.py b/nbs/dl2/09_optimizers.py
new file mode 100644
index 0000000..a28081e
--- /dev/null
+++ ./nbs/dl2/09_optimizers.py
@@ -0,0 +1,562 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Optimizer tweaks
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+#export
+from exp.nb_08 import *
+
+# ## Imagenette data
+
+# We grab the data from the previous notebook.
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=3917) 
+
+path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)
+
+# +
+tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor]
+bs=128
+
+il = ImageList.from_files(path, tfms=tfms)
+sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='val'))
+ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())
+data = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=4)
+# -
+
+# Then a model:
+
+nfs = [32,64,128,256]
+
+cbfs = [partial(AvgStatsCallback,accuracy), CudaCallback,
+        partial(BatchTransformXCallback, norm_imagenette)]
+
+# This is the baseline of training with vanilla SGD.
+
+learn,run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs)
+
+run.fit(1, learn)
+
+
+# ## Refining the optimizer
+
+# In PyTorch, the base optimizer in `torch.optim` is just a dictionary that stores the hyper-parameters and references to the parameters of the model we want to train in parameter groups (different groups can have different learning rates/momentum/weight decay... which is what lets us do discriminative learning rates).
+#
+# It contains a method `step` that will update our parameters with the gradients and a method `zero_grad` to detach and zero the gradients of all our parameters.
+#
+# We build the equivalent from scratch, only ours will be more flexible. In our implementation, the step function loops over all the parameters to execute the step using stepper functions that we have to provide when initializing the optimizer.
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=4074)
+
+class Optimizer():
+    def __init__(self, params, steppers, **defaults):
+        # might be a generator
+        self.param_groups = list(params)
+        # ensure params is a list of lists
+        if not isinstance(self.param_groups[0], list): self.param_groups = [self.param_groups]
+        self.hypers = [{**defaults} for p in self.param_groups]
+        self.steppers = listify(steppers)
+
+    def grad_params(self):
+        return [(p,hyper) for pg,hyper in zip(self.param_groups,self.hypers)
+            for p in pg if p.grad is not None]
+
+    def zero_grad(self):
+        for p,hyper in self.grad_params():
+            p.grad.detach_()
+            p.grad.zero_()
+
+    def step(self):
+        for p,hyper in self.grad_params(): compose(p, self.steppers, **hyper)
+
+
+# To do basic SGD, this what a step looks like:
+
+#export
+def sgd_step(p, lr, **kwargs):
+    p.data.add_(-lr, p.grad.data)
+    return p
+
+
+opt_func = partial(Optimizer, steppers=[sgd_step])
+
+
+# Now that we have changed the optimizer, we will need to adjust the callbacks that were using properties from the PyTorch optimizer: in particular the hyper-parameters are in the list of dictionaries `opt.hypers` (PyTorch has everything in the the list of param groups).
+
+# +
+#export
+class Recorder(Callback):
+    def begin_fit(self): self.lrs,self.losses = [],[]
+
+    def after_batch(self):
+        if not self.in_train: return
+        self.lrs.append(self.opt.hypers[-1]['lr'])
+        self.losses.append(self.loss.detach().cpu())        
+
+    def plot_lr  (self): plt.plot(self.lrs)
+    def plot_loss(self): plt.plot(self.losses)
+        
+    def plot(self, skip_last=0):
+        losses = [o.item() for o in self.losses]
+        n = len(losses)-skip_last
+        plt.xscale('log')
+        plt.plot(self.lrs[:n], losses[:n])
+
+class ParamScheduler(Callback):
+    _order=1
+    def __init__(self, pname, sched_funcs):
+        self.pname,self.sched_funcs = pname,listify(sched_funcs)
+
+    def begin_batch(self): 
+        if not self.in_train: return
+        fs = self.sched_funcs
+        if len(fs)==1: fs = fs*len(self.opt.param_groups)
+        pos = self.n_epochs/self.epochs
+        for f,h in zip(fs,self.opt.hypers): h[self.pname] = f(pos)
+            
+class LR_Find(Callback):
+    _order=1
+    def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10):
+        self.max_iter,self.min_lr,self.max_lr = max_iter,min_lr,max_lr
+        self.best_loss = 1e9
+        
+    def begin_batch(self): 
+        if not self.in_train: return
+        pos = self.n_iter/self.max_iter
+        lr = self.min_lr * (self.max_lr/self.min_lr) ** pos
+        for pg in self.opt.hypers: pg['lr'] = lr
+            
+    def after_step(self):
+        if self.n_iter>=self.max_iter or self.loss>self.best_loss*10:
+            raise CancelTrainException()
+        if self.loss < self.best_loss: self.best_loss = self.loss
+
+
+# -
+
+# So let's check we didn't break anything and that recorder and param scheduler work properly.
+
+sched = combine_scheds([0.3, 0.7], [sched_cos(0.3, 0.6), sched_cos(0.6, 0.2)]) 
+
+cbfs = [partial(AvgStatsCallback,accuracy),
+        CudaCallback, Recorder,
+        partial(ParamScheduler, 'lr', sched)]
+
+learn,run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs, opt_func=opt_func)
+
+# %time run.fit(1, learn)
+
+run.recorder.plot_loss()
+
+run.recorder.plot_lr()
+
+
+# ## Weight decay
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=4623)
+
+# By letting our model learn high parameters, it might fit all the data points in the training set with an over-complex function that has very sharp changes, which will lead to overfitting.
+#
+# <img src="images/overfit.png" alt="Fitting vs over-fitting" width="600">
+#
+# Weight decay comes from the idea of L2 regularization, which consists in adding to your loss function the sum of all the weights squared. Why do that? Because when we compute the gradients, it will add a contribution to them that will encourage the weights to be as small as possible.
+
+# Limiting our weights from growing too much is going to hinder the training of the model, but it will yield to a state where it generalizes better. Going back to the theory a little bit, weight decay (or just `wd`) is a parameter that controls that sum of squares we add to our loss:
+# ``` python
+# loss_with_wd = loss + (wd/2) * (weights**2).sum()
+# ```
+#
+# In practice though, it would be very inefficient (and maybe numerically unstable) to compute that big sum and add it to the loss. If you remember a little bit of high school math, the derivative of `p**2` with respect to `p` is `2*p`. So adding that big sum to our loss is exactly the same as doing:
+# ``` python
+# weight.grad += wd * weight
+# ```
+#
+# for every weight in our model, which in the case of vanilla SGD is equivalent to updating the parameters with:
+# ``` python
+# weight = weight - lr*(weight.grad + wd*weight)
+# ```
+#
+# This technique is called "weight decay", as each weight is decayed by a factor `lr * wd`, as it's shown in this last formula.
+#
+# This only works for standard SGD, as we have seen that with momentum, RMSProp and Adam, the update has some additional formulas around the gradient. In those cases, the formula that comes from L2 regularization:
+# ``` python
+# weight.grad += wd * weight
+# ```
+# is different than weight decay
+# ``` python
+# new_weight = weight - lr * weight.grad - lr * wd * weight
+# ```
+#
+# Most libraries use the first one, but as it was pointed out in [Decoupled Weight Regularization](https://arxiv.org/pdf/1711.05101.pdf) by Ilya Loshchilov and Frank Hutter, it is better to use the second one with the Adam optimizer, which is why fastai made it its default.
+
+# Weight decay is subtracting `lr*wd*weight` from the weights. We need this function to have an attribute `_defaults` so that we are sure there is an hyper-parameter of the same name in our `Optimizer`.
+
+#export
+def weight_decay(p, lr, wd, **kwargs):
+    p.data.mul_(1 - lr*wd)
+    return p
+weight_decay._defaults = dict(wd=0.)
+
+
+# L2 regularization is adding `wd*weight` to the gradients.
+
+#export
+def l2_reg(p, lr, wd, **kwargs):
+    p.grad.data.add_(wd, p.data)
+    return p
+l2_reg._defaults = dict(wd=0.)
+
+
+# Let's allow steppers to add to our `defaults` (which are the default values of all the hyper-parameters). This helper function adds in `dest` the key/values it finds while going through `os` and applying `f` when they was no `key` of the same name.
+
+# +
+#export
+def maybe_update(os, dest, f):
+    for o in os:
+        for k,v in f(o).items():
+            if k not in dest: dest[k] = v
+
+def get_defaults(d): return getattr(d,'_defaults',{})
+
+
+# -
+
+# This is the same as before, we just take the default values of the steppers when none are provided in the kwargs.
+
+#export
+class Optimizer():
+    def __init__(self, params, steppers, **defaults):
+        self.steppers = listify(steppers)
+        maybe_update(self.steppers, defaults, get_defaults)
+        # might be a generator
+        self.param_groups = list(params)
+        # ensure params is a list of lists
+        if not isinstance(self.param_groups[0], list): self.param_groups = [self.param_groups]
+        self.hypers = [{**defaults} for p in self.param_groups]
+
+    def grad_params(self):
+        return [(p,hyper) for pg,hyper in zip(self.param_groups,self.hypers)
+            for p in pg if p.grad is not None]
+
+    def zero_grad(self):
+        for p,hyper in self.grad_params():
+            p.grad.detach_()
+            p.grad.zero_()
+
+    def step(self):
+        for p,hyper in self.grad_params(): compose(p, self.steppers, **hyper)
+
+
+#export 
+sgd_opt = partial(Optimizer, steppers=[weight_decay, sgd_step])
+
+learn,run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs, opt_func=sgd_opt)
+
+# Before trying to train, let's check the behavior works as intended: when we don't provide a value for `wd`, we pull the corresponding default from `weight_decay`.
+
+model = learn.model
+
+opt = sgd_opt(model.parameters(), lr=0.1)
+test_eq(opt.hypers[0]['wd'], 0.)
+test_eq(opt.hypers[0]['lr'], 0.1)
+
+# But if we provide a value, it overrides the default.
+
+opt = sgd_opt(model.parameters(), lr=0.1, wd=1e-4)
+test_eq(opt.hypers[0]['wd'], 1e-4)
+test_eq(opt.hypers[0]['lr'], 0.1)
+
+# Now let's fit.
+
+cbfs = [partial(AvgStatsCallback,accuracy), CudaCallback]
+
+learn,run = get_learn_run(nfs, data, 0.3, conv_layer, cbs=cbfs, opt_func=partial(sgd_opt, wd=0.01))
+
+run.fit(1, learn)
+
+
+# This is already better than the baseline!
+
+# ## With momentum
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=4872)
+
+# Momentum requires to add some state. We need to save the moving average of the gradients to be able to do the step and store this inside the optimizer state. To do this, we introduce statistics. Statistics are object with two methods:
+# - `init_state`, that returns the initial state (a tensor of 0. for the moving average of gradients)
+# - `update`, that updates the state with the new gradient value
+#
+# We also read the `_defaults` values of those objects, to allow them to provide default values to hyper-parameters.
+
+#export
+class StatefulOptimizer(Optimizer):
+    def __init__(self, params, steppers, stats=None, **defaults): 
+        self.stats = listify(stats)
+        maybe_update(self.stats, defaults, get_defaults)
+        super().__init__(params, steppers, **defaults)
+        self.state = {}
+        
+    def step(self):
+        for p,hyper in self.grad_params():
+            if p not in self.state:
+                #Create a state for p and call all the statistics to initialize it.
+                self.state[p] = {}
+                maybe_update(self.stats, self.state[p], lambda o: o.init_state(p))
+            state = self.state[p]
+            for stat in self.stats: state = stat.update(p, state, **hyper)
+            compose(p, self.steppers, **state, **hyper)
+            self.state[p] = state
+
+
+#export
+class Stat():
+    _defaults = {}
+    def init_state(self, p): raise NotImplementedError
+    def update(self, p, state, **kwargs): raise NotImplementedError    
+
+
+# Here is an example of `Stat`:
+
+class AverageGrad(Stat):
+    _defaults = dict(mom=0.9)
+
+    def init_state(self, p): return {'grad_avg': torch.zeros_like(p.grad.data)}
+    def update(self, p, state, mom, **kwargs):
+        state['grad_avg'].mul_(mom).add_(p.grad.data)
+        return state
+
+
+# Then we add the momentum step (instead of using the gradients to perform the step, we use the average).
+
+#export
+def momentum_step(p, lr, grad_avg, **kwargs):
+    p.data.add_(-lr, grad_avg)
+    return p
+
+
+sgd_mom_opt = partial(StatefulOptimizer, steppers=[momentum_step,weight_decay],
+                  stats=AverageGrad(), wd=0.01)
+
+learn,run = get_learn_run(nfs, data, 0.3, conv_layer, cbs=cbfs, opt_func=sgd_mom_opt)
+
+run.fit(1, learn)
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=5115) for discussion about weight decay interaction with batch normalisation
+
+# ### Momentum experiments
+
+# What does momentum do to the gradients exactly? Let's do some plots to find out!
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=5487)
+
+x = torch.linspace(-4, 4, 200)
+y = torch.randn(200) + 0.3
+betas = [0.5, 0.7, 0.9, 0.99]
+
+
+def plot_mom(f):
+    _,axs = plt.subplots(2,2, figsize=(12,8))
+    for beta,ax in zip(betas, axs.flatten()):
+        ax.plot(y, linestyle='None', marker='.')
+        avg,res = None,[]
+        for i,yi in enumerate(y):
+            avg,p = f(avg, beta, yi, i)
+            res.append(p)
+        ax.plot(res, color='red')
+        ax.set_title(f'beta={beta}')
+
+
+# This is the regular momentum.
+
+def mom1(avg, beta, yi, i): 
+    if avg is None: avg=yi
+    res = beta*avg + yi
+    return res,res
+plot_mom(mom1)
+
+
+# As we can see, with a too high value, it may go way too high with no way to change its course.
+#
+# Another way to smooth noisy data is to do an exponentially weighted moving average. In this case, there is a dampening of (1-beta) in front of the new value, which is less trusted than the current average. We'll define `lin_comb` (*linear combination*) to make this easier (note that in the lesson this was named `ewma`).
+
+#export
+def lin_comb(v1, v2, beta): return beta*v1 + (1-beta)*v2
+
+
+def mom2(avg, beta, yi, i):
+    if avg is None: avg=yi
+    avg = lin_comb(avg, yi, beta)
+    return avg, avg
+plot_mom(mom2)
+
+# We can see it gets to a zero-constant when the data is purely random. If the data has a certain shape, it will get that shape (with some delay for high beta).
+
+y = 1 - (x/3) ** 2 + torch.randn(200) * 0.1
+
+y[0]=0.5
+
+plot_mom(mom2)
+
+
+# Debiasing is here to correct the wrong information we may have in the very first batch. The debias term corresponds to the sum of the coefficient in our moving average. At the time step i, our average is:
+#
+# $\begin{align*}
+# avg_{i} &= \beta\ avg_{i-1} + (1-\beta)\ v_{i} = \beta\ (\beta\ avg_{i-2} + (1-\beta)\ v_{i-1}) + (1-\beta)\ v_{i} \\
+# &= \beta^{2}\ avg_{i-2} + (1-\beta)\ \beta\ v_{i-1} + (1-\beta)\ v_{i} \\
+# &= \beta^{3}\ avg_{i-3} + (1-\beta)\ \beta^{2}\ v_{i-2} + (1-\beta)\ \beta\ v_{i-1} + (1-\beta)\ v_{i} \\
+# &\vdots \\
+# &= (1-\beta)\ \beta^{i}\ v_{0} + (1-\beta)\ \beta^{i-1}\ v_{1} + \cdots + (1-\beta)\ \beta^{2}\ v_{i-2} + (1-\beta)\ \beta\  v_{i-1} + (1-\beta)\ v_{i}
+# \end{align*}$
+#
+# and so the sum of the coefficients is
+#
+# $\begin{align*}
+# S &=(1-\beta)\ \beta^{i} + (1-\beta)\ \beta^{i-1} + \cdots + (1-\beta)\ \beta^{2} + (1-\beta)\ \beta + (1-\beta) \\
+# &= (\beta^{i} - \beta^{i+1}) + (\beta^{i-1} - \beta^{i}) + \cdots + (\beta^{2} - \beta^{3}) + (\beta - \beta^{2}) + (1-\beta) \\
+# &= 1 - \beta^{i+1}
+# \end{align*}$
+#
+# since all the other terms cancel out each other.
+#
+# By dividing by this term, we make our moving average a true average (in the sense that all the coefficients we used for the average sum up to 1).
+
+def mom3(avg, beta, yi, i):
+    if avg is None: avg=0
+    avg = lin_comb(avg, yi, beta)
+    return avg, avg/(1-beta**(i+1))
+plot_mom(mom3)
+
+
+# ## Adam and friends
+
+# In Adam, we use the gradient averages but with dampening (not like in SGD with momentum), so let's add this to the `AverageGrad` class.
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=5889)
+
+#export
+class AverageGrad(Stat):
+    _defaults = dict(mom=0.9)
+    
+    def __init__(self, dampening:bool=False): self.dampening=dampening
+    def init_state(self, p): return {'grad_avg': torch.zeros_like(p.grad.data)}
+    def update(self, p, state, mom, **kwargs):
+        state['mom_damp'] = 1-mom if self.dampening else 1.
+        state['grad_avg'].mul_(mom).add_(state['mom_damp'], p.grad.data)
+        return state
+
+
+# We also need to track the moving average of the gradients squared.
+
+#export
+class AverageSqrGrad(Stat):
+    _defaults = dict(sqr_mom=0.99)
+    
+    def __init__(self, dampening:bool=True): self.dampening=dampening
+    def init_state(self, p): return {'sqr_avg': torch.zeros_like(p.grad.data)}
+    def update(self, p, state, sqr_mom, **kwargs):
+        state['sqr_damp'] = 1-sqr_mom if self.dampening else 1.
+        state['sqr_avg'].mul_(sqr_mom).addcmul_(state['sqr_damp'], p.grad.data, p.grad.data)
+        return state
+
+
+# We will also need the number of steps done during training for the debiasing.
+
+#export
+class StepCount(Stat):
+    def init_state(self, p): return {'step': 0}
+    def update(self, p, state, **kwargs):
+        state['step'] += 1
+        return state
+
+
+# This helper function computes the debias term. If we dampening, `damp = 1 - mom` and we get the same result as before. If we don't use dampening, (`damp = 1`) we will need to divide by `1 - mom` because that term is missing everywhere.
+
+#export
+def debias(mom, damp, step): return damp * (1 - mom**step) / (1-mom)
+
+
+# Then the Adam step is just the following:
+
+#export
+def adam_step(p, lr, mom, mom_damp, step, sqr_mom, sqr_damp, grad_avg, sqr_avg, eps, **kwargs):
+    debias1 = debias(mom,     mom_damp, step)
+    debias2 = debias(sqr_mom, sqr_damp, step)
+    p.data.addcdiv_(-lr / debias1, grad_avg, (sqr_avg/debias2).sqrt() + eps)
+    return p
+adam_step._defaults = dict(eps=1e-5)
+
+
+#export
+def adam_opt(xtra_step=None, **kwargs):
+    return partial(StatefulOptimizer, steppers=[adam_step,weight_decay]+listify(xtra_step),
+                   stats=[AverageGrad(dampening=True), AverageSqrGrad(), StepCount()], **kwargs)
+
+
+learn,run = get_learn_run(nfs, data, 0.001, conv_layer, cbs=cbfs, opt_func=adam_opt())
+
+run.fit(3, learn)
+
+
+# ## LAMB
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=6038)
+
+# It's then super easy to implement a new optimizer. This is LAMB from a [very recent paper](https://arxiv.org/pdf/1904.00962.pdf):
+#
+# $\begin{align}
+# g_{t}^{l} &= \nabla L(w_{t-1}^{l}, x_{t}) \\
+# m_{t}^{l} &= \beta_{1} m_{t-1}^{l} + (1-\beta_{1}) g_{t}^{l} \\
+# v_{t}^{l} &= \beta_{2} v_{t-1}^{l} + (1-\beta_{2}) g_{t}^{l} \odot g_{t}^{l} \\
+# m_{t}^{l} &= m_{t}^{l} / (1 - \beta_{1}^{t}) \\
+# v_{t}^{l} &= v_{t}^{l} / (1 - \beta_{2}^{t}) \\
+# r_{1} &= \|w_{t-1}^{l}\|_{2} \\
+# s_{t}^{l} &= \frac{m_{t}^{l}}{\sqrt{v_{t}^{l} + \epsilon}} + \lambda w_{t-1}^{l} \\ 
+# r_{2} &= \| s_{t}^{l} \|_{2} \\
+# \eta^{l} &= \eta * r_{1}/r_{2} \\ 
+# w_{t}^{l} &= w_{t}^{l-1} - \eta_{l} * s_{t}^{l} \\
+# \end{align}$
+
+def lamb_step(p, lr, mom, mom_damp, step, sqr_mom, sqr_damp, grad_avg, sqr_avg, eps, wd, **kwargs):
+    debias1 = debias(mom,     mom_damp, step)
+    debias2 = debias(sqr_mom, sqr_damp, step)
+    r1 = p.data.pow(2).mean().sqrt()
+    step = (grad_avg/debias1) / ((sqr_avg/debias2).sqrt()+eps) + wd*p.data
+    r2 = step.pow(2).mean().sqrt()
+    p.data.add_(-lr * min(r1/r2,10), step)
+    return p
+lamb_step._defaults = dict(eps=1e-6, wd=0.)
+
+lamb = partial(StatefulOptimizer, steppers=lamb_step, stats=[AverageGrad(dampening=True), AverageSqrGrad(), StepCount()])
+
+learn,run = get_learn_run(nfs, data, 0.003, conv_layer, cbs=cbfs, opt_func=lamb)
+
+run.fit(3, learn)
+
+# Other recent variants of optimizers:
+# - [Large Batch Training of Convolutional Networks](https://arxiv.org/abs/1708.03888) (LARS also uses weight statistics, not just gradient statistics. Can you add that to this class?)
+# - [Adafactor: Adaptive Learning Rates with Sublinear Memory Cost](https://arxiv.org/abs/1804.04235) (Adafactor combines stats over multiple sets of axes)
+# - [Adaptive Gradient Methods with Dynamic Bound of Learning Rate](https://arxiv.org/abs/1902.09843)
+
+# ## Export
+
+# !python notebook2script.py 09_optimizers.ipynb
+
+
diff --git a/nbs/dl2/09b_learner.ipynb b/nbs/dl2/09b_learner.ipynb
index faa8068..a1de51d 100644
--- ./nbs/dl2/09b_learner.ipynb
+++ ./nbs/dl2/09b_learner.ipynb
@@ -422,6 +422,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/09b_learner.py b/nbs/dl2/09b_learner.py
new file mode 100644
index 0000000..14d8d38
--- /dev/null
+++ ./nbs/dl2/09b_learner.py
@@ -0,0 +1,204 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Let's kill off `Runner`
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+#export
+from exp.nb_09 import *
+
+AvgStats
+
+# ## Imagenette data
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=6571)
+
+path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)
+
+# +
+tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor]
+bs=64
+
+il = ImageList.from_files(path, tfms=tfms)
+sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='val'))
+ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())
+data = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=4)
+# -
+
+cbfs = [partial(AvgStatsCallback,accuracy),
+        CudaCallback,
+        partial(BatchTransformXCallback, norm_imagenette)]
+
+nfs = [32]*4
+
+
+# Having a Runner is great but not essential when the `Learner` already has everything needed in its state. We implement everything inside it directly instead of building a second object.
+
+# ##### In Lesson 12 Jeremy Howard revisited material in the cell below  [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=65)
+
+# +
+#export
+def param_getter(m): return m.parameters()
+
+class Learner():
+    def __init__(self, model, data, loss_func, opt_func=sgd_opt, lr=1e-2, splitter=param_getter,
+                 cbs=None, cb_funcs=None):
+        self.model,self.data,self.loss_func,self.opt_func,self.lr,self.splitter = model,data,loss_func,opt_func,lr,splitter
+        self.in_train,self.logger,self.opt = False,print,None
+        
+        # NB: Things marked "NEW" are covered in lesson 12
+        # NEW: avoid need for set_runner
+        self.cbs = []
+        self.add_cb(TrainEvalCallback())
+        self.add_cbs(cbs)
+        self.add_cbs(cbf() for cbf in listify(cb_funcs))
+
+    def add_cbs(self, cbs):
+        for cb in listify(cbs): self.add_cb(cb)
+            
+    def add_cb(self, cb):
+        cb.set_runner(self)
+        setattr(self, cb.name, cb)
+        self.cbs.append(cb)
+
+    def remove_cbs(self, cbs):
+        for cb in listify(cbs): self.cbs.remove(cb)
+            
+    def one_batch(self, i, xb, yb):
+        try:
+            self.iter = i
+            self.xb,self.yb = xb,yb;                        self('begin_batch')
+            self.pred = self.model(self.xb);                self('after_pred')
+            self.loss = self.loss_func(self.pred, self.yb); self('after_loss')
+            if not self.in_train: return
+            self.loss.backward();                           self('after_backward')
+            self.opt.step();                                self('after_step')
+            self.opt.zero_grad()
+        except CancelBatchException:                        self('after_cancel_batch')
+        finally:                                            self('after_batch')
+
+    def all_batches(self):
+        self.iters = len(self.dl)
+        try:
+            for i,(xb,yb) in enumerate(self.dl): self.one_batch(i, xb, yb)
+        except CancelEpochException: self('after_cancel_epoch')
+
+    def do_begin_fit(self, epochs):
+        self.epochs,self.loss = epochs,tensor(0.)
+        self('begin_fit')
+
+    def do_begin_epoch(self, epoch):
+        self.epoch,self.dl = epoch,self.data.train_dl
+        return self('begin_epoch')
+
+    def fit(self, epochs, cbs=None, reset_opt=False):
+        # NEW: pass callbacks to fit() and have them removed when done
+        self.add_cbs(cbs)
+        # NEW: create optimizer on fit(), optionally replacing existing
+        if reset_opt or not self.opt: self.opt = self.opt_func(self.splitter(self.model), lr=self.lr)
+            
+        try:
+            self.do_begin_fit(epochs)
+            for epoch in range(epochs):
+                if not self.do_begin_epoch(epoch): self.all_batches()
+
+                with torch.no_grad(): 
+                    self.dl = self.data.valid_dl
+                    if not self('begin_validate'): self.all_batches()
+                self('after_epoch')
+            
+        except CancelTrainException: self('after_cancel_train')
+        finally:
+            self('after_fit')
+            self.remove_cbs(cbs)
+
+    ALL_CBS = {'begin_batch', 'after_pred', 'after_loss', 'after_backward', 'after_step',
+        'after_cancel_batch', 'after_batch', 'after_cancel_epoch', 'begin_fit',
+        'begin_epoch', 'begin_validate', 'after_epoch',
+        'after_cancel_train', 'after_fit'}
+    
+    def __call__(self, cb_name):
+        res = False
+        assert cb_name in self.ALL_CBS
+        for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) and res
+        return res
+
+
+# -
+
+#export
+class AvgStatsCallback(Callback):
+    def __init__(self, metrics):
+        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)
+        
+    def begin_epoch(self):
+        self.train_stats.reset()
+        self.valid_stats.reset()
+        
+    def after_loss(self):
+        stats = self.train_stats if self.in_train else self.valid_stats
+        with torch.no_grad(): stats.accumulate(self.run)
+    
+    def after_epoch(self):
+        #We use the logger function of the `Learner` here, it can be customized to write in a file or in a progress bar
+        self.logger(self.train_stats)
+        self.logger(self.valid_stats) 
+
+
+cbfs = [partial(AvgStatsCallback,accuracy),
+        CudaCallback,
+        partial(BatchTransformXCallback, norm_imagenette)]
+
+
+#export
+def get_learner(nfs, data, lr, layer, loss_func=F.cross_entropy,
+                cb_funcs=None, opt_func=sgd_opt, **kwargs):
+    model = get_cnn_model(data, nfs, layer, **kwargs)
+    init_cnn(model)
+    return Learner(model, data, loss_func, lr=lr, cb_funcs=cb_funcs, opt_func=opt_func)
+
+
+learn = get_learner(nfs, data, 0.4, conv_layer, cb_funcs=cbfs)
+
+# %time learn.fit(1)
+
+# ## Check everything works
+
+# Let's check our previous callbacks still work.
+
+cbfs += [Recorder]
+
+learn = get_learner(nfs, data, 0.4, conv_layer, cb_funcs=cbfs)
+
+phases = combine_scheds([0.3, 0.7], cos_1cycle_anneal(0.2, 0.6, 0.2))
+sched = ParamScheduler('lr', phases)
+
+learn.fit(1, sched)
+
+learn.recorder.plot_lr()
+
+learn.recorder.plot_loss()
+
+# ## Export
+
+!./notebook2script.py 09b_learner.ipynb
+
+
diff --git a/nbs/dl2/09c_add_progress_bar.ipynb b/nbs/dl2/09c_add_progress_bar.ipynb
index 8bada8e..0190b65 100644
--- ./nbs/dl2/09c_add_progress_bar.ipynb
+++ ./nbs/dl2/09c_add_progress_bar.ipynb
@@ -301,6 +301,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/09c_add_progress_bar.py b/nbs/dl2/09c_add_progress_bar.py
new file mode 100644
index 0000000..06c3c12
--- /dev/null
+++ ./nbs/dl2/09c_add_progress_bar.py
@@ -0,0 +1,119 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Adding progress bars to Learner
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+#export
+from exp.nb_09b import *
+import time
+from fastprogress.fastprogress import master_bar, progress_bar
+from fastprogress.fastprogress import format_time
+
+# One thing has been missing all this time, and as fun as it is to stare at a blank screen waiting for the results, it's nicer to have some tool to track progress.
+
+# ## Imagenette data
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=6743)
+
+path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)
+
+# +
+tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor]
+bs = 64
+
+il = ImageList.from_files(path, tfms=tfms)
+sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='val'))
+ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())
+data = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=4)
+# -
+
+nfs = [32]*4
+
+
+# We rewrite the `AvgStatsCallback` to add a line with the names of the things measured and keep track of the time per epoch.
+
+# export 
+class AvgStatsCallback(Callback):
+    def __init__(self, metrics):
+        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)
+    
+    def begin_fit(self):
+        met_names = ['loss'] + [m.__name__ for m in self.train_stats.metrics]
+        names = ['epoch'] + [f'train_{n}' for n in met_names] + [
+            f'valid_{n}' for n in met_names] + ['time']
+        self.logger(names)
+    
+    def begin_epoch(self):
+        self.train_stats.reset()
+        self.valid_stats.reset()
+        self.start_time = time.time()
+        
+    def after_loss(self):
+        stats = self.train_stats if self.in_train else self.valid_stats
+        with torch.no_grad(): stats.accumulate(self.run)
+    
+    def after_epoch(self):
+        stats = [str(self.epoch)] 
+        for o in [self.train_stats, self.valid_stats]:
+            stats += [f'{v:.6f}' for v in o.avg_stats] 
+        stats += [format_time(time.time() - self.start_time)]
+        self.logger(stats)
+
+
+# Then we add the progress bars... with a Callback of course! `master_bar` handles the count over the epochs while its child `progress_bar` is looping over all the batches. We just create one at the beginning or each epoch/validation phase, and update it at the end of each batch. By changing the logger of the `Learner` to the `write` function of the master bar, everything is automatically written there.
+#
+# Note: this requires fastprogress v0.1.21 or later. 
+
+# export 
+class ProgressCallback(Callback):
+    _order=-1
+    def begin_fit(self):
+        self.mbar = master_bar(range(self.epochs))
+        self.mbar.on_iter_begin()
+        self.run.logger = partial(self.mbar.write, table=True)
+        
+    def after_fit(self): self.mbar.on_iter_end()
+    def after_batch(self): self.pb.update(self.iter)
+    def begin_epoch   (self): self.set_pb()
+    def begin_validate(self): self.set_pb()
+        
+    def set_pb(self):
+        self.pb = progress_bar(self.dl, parent=self.mbar)
+        self.mbar.update(self.epoch)
+
+
+# By making the progress bar a callback, you can easily choose if you want to have them shown or not.
+
+cbfs = [partial(AvgStatsCallback,accuracy),
+        CudaCallback,
+        ProgressCallback,
+        partial(BatchTransformXCallback, norm_imagenette)]
+
+learn = get_learner(nfs, data, 0.4, conv_layer, cb_funcs=cbfs)
+
+learn.fit(2)
+
+# ## Export
+
+!./notebook2script.py 09c_add_progress_bar.ipynb
+
+
diff --git a/nbs/dl2/10_augmentation.ipynb b/nbs/dl2/10_augmentation.ipynb
index b06d393..1fe498d 100644
--- ./nbs/dl2/10_augmentation.ipynb
+++ ./nbs/dl2/10_augmentation.ipynb
@@ -2336,6 +2336,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/10_augmentation.py b/nbs/dl2/10_augmentation.py
new file mode 100644
index 0000000..c58f333
--- /dev/null
+++ ./nbs/dl2/10_augmentation.py
@@ -0,0 +1,648 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Data augmentation
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+#export
+from exp.nb_09c import *
+
+# ## PIL transforms
+
+# We start with PIL transforms to resize all our images to the same size. Then, when they are in a batch, we can apply data augmentation to all of them at the same time on the GPU. We have already seen the basics of resizing and putting on the GPU in 08, but we'll look more into it now.
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=6882)
+
+# ### View images
+
+#export
+make_rgb._order=0
+
+path = datasets.untar_data(datasets.URLs.IMAGENETTE)
+tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor]
+
+
+def get_il(tfms): return ImageList.from_files(path, tfms=tfms)
+
+
+il = get_il(tfms)
+
+show_image(il[0])
+
+img = PIL.Image.open(il.items[0])
+
+img
+
+img.getpixel((1,1))
+
+import numpy as np
+
+# %timeit -n 10 a = np.array(PIL.Image.open(il.items[0]))
+
+# Be careful of resampling methods, you can quickly lose some textures!
+
+img.resize((128,128), resample=PIL.Image.ANTIALIAS)
+
+img.resize((128,128), resample=PIL.Image.BILINEAR)
+
+img.resize((128,128), resample=PIL.Image.NEAREST)
+
+img.resize((256,256), resample=PIL.Image.BICUBIC).resize((128,128), resample=PIL.Image.NEAREST)
+
+# %timeit img.resize((224,224), resample=PIL.Image.BICUBIC)
+
+# %timeit img.resize((224,224), resample=PIL.Image.BILINEAR)
+
+# %timeit -n 10 img.resize((224,224), resample=PIL.Image.NEAREST)
+
+# ### Flip
+
+# Flip can be done with PIL very fast.
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=7092)
+
+#export
+import random
+
+
+def pil_random_flip(x):
+    return x.transpose(PIL.Image.FLIP_LEFT_RIGHT) if random.random()<0.5 else x
+
+
+il1 = get_il(tfms)
+il1.items = [il1.items[0]]*64
+dl = DataLoader(il1, 8)
+
+x = next(iter(dl))
+
+
+# Here is a convenience function to look at images in a batch.
+
+# +
+#export
+def show_image(im, ax=None, figsize=(3,3)):
+    if ax is None: _,ax = plt.subplots(1, 1, figsize=figsize)
+    ax.axis('off')
+    ax.imshow(im.permute(1,2,0))
+
+def show_batch(x, c=4, r=None, figsize=None):
+    n = len(x)
+    if r is None: r = int(math.ceil(n/c))
+    if figsize is None: figsize=(c*3,r*3)
+    fig,axes = plt.subplots(r,c, figsize=figsize)
+    for xi,ax in zip(x,axes.flat): show_image(xi, ax)
+
+
+# -
+
+# Without data augmentation:
+
+show_batch(x)
+
+# With random flip:
+
+il1.tfms.append(pil_random_flip)
+
+x = next(iter(dl))
+show_batch(x)
+
+
+# We can also make that transform a class so it's easier to set the value of the parameter `p`. As seen before, it also allows us to set the `_order` attribute.
+
+class PilRandomFlip(Transform):
+    _order=11
+    def __init__(self, p=0.5): self.p=p
+    def __call__(self, x):
+        return x.transpose(PIL.Image.FLIP_LEFT_RIGHT) if random.random()<self.p else x
+
+
+# +
+#export
+class PilTransform(Transform): _order=11
+
+class PilRandomFlip(PilTransform):
+    def __init__(self, p=0.5): self.p=p
+    def __call__(self, x):
+        return x.transpose(PIL.Image.FLIP_LEFT_RIGHT) if random.random()<self.p else x
+
+
+# -
+
+del(il1.tfms[-1])
+il1.tfms.append(PilRandomFlip(0.8))
+
+x = next(iter(dl))
+show_batch(x)
+
+# PIL can also do the whole dihedral group of transformations (random horizontal flip, random vertical flip and the four 90 degrees rotation) with the `transpose` method. Here are the codes of a few transformations:
+
+PIL.Image.FLIP_LEFT_RIGHT,PIL.Image.ROTATE_270,PIL.Image.TRANSVERSE
+
+# Be careful that `img.transpose(0)` is already one transform, so doing nothing requires a separate case, then we have 7 different transformations.
+
+img = PIL.Image.open(il.items[0])
+img = img.resize((128,128), resample=PIL.Image.NEAREST)
+_, axs = plt.subplots(2, 4, figsize=(12, 6))
+for i,ax in enumerate(axs.flatten()):
+    if i==0: ax.imshow(img)
+    else:    ax.imshow(img.transpose(i-1))
+    ax.axis('off')
+
+
+# And we can implement it like this:
+
+#export
+class PilRandomDihedral(PilTransform):
+    def __init__(self, p=0.75): self.p=p*7/8 #Little hack to get the 1/8 identity dihedral transform taken into account.
+    def __call__(self, x):
+        if random.random()>self.p: return x
+        return x.transpose(random.randint(0,6))
+
+
+del(il1.tfms[-1])
+il1.tfms.append(PilRandomDihedral())
+
+show_batch(next(iter(dl)))
+
+# ### Random crop
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=7422)
+
+img = PIL.Image.open(il.items[0])
+img.size
+
+# To crop an image with PIL we have to specify the top/left and bottom/right corner in this format: (left, top, right, bottom). We won't just crop the size we want, but first crop the section we want of the image and then apply a resize. In what follows, we call the first one the `crop_size`.
+
+img.crop((60,60,320,320)).resize((128,128), resample=PIL.Image.BILINEAR)
+
+cnr2 = (60,60,320,320)
+resample = PIL.Image.BILINEAR
+
+# This is pretty fast in PIL:
+
+# %timeit -n 10 img.crop(cnr2).resize((128,128), resample=resample)
+
+# Our time budget: aim for 5 mins per batch for imagenet on 8 GPUs. 1.25m images in imagenet. So on one GPU per minute that's `1250000/8/5 == 31250`, or 520 per second. Assuming 4 cores per GPU, then we want ~125 images per second - so try to stay <10ms per image. Here we have time to do more things. For instance, we can do the crop and resize in the same call to `transform`, which will give a smoother result.
+
+img.transform((128,128), PIL.Image.EXTENT, cnr2, resample=resample)
+
+# %timeit -n 10 img.transform((128,128), PIL.Image.EXTENT, cnr2, resample=resample)
+
+# It's a little bit slower but still fast enough for our purpose, so we will use this. We then define a general crop transform and two subclasses: one to crop at the center (for validation) and one to randomly crop. Each time, the subclass only implements the way to get the four corners passed to PIL.
+
+# +
+#export
+from random import randint
+
+def process_sz(sz):
+    sz = listify(sz)
+    return tuple(sz if len(sz)==2 else [sz[0],sz[0]])
+
+def default_crop_size(w,h): return [w,w] if w < h else [h,h]
+
+class GeneralCrop(PilTransform):
+    def __init__(self, size, crop_size=None, resample=PIL.Image.BILINEAR): 
+        self.resample,self.size = resample,process_sz(size)
+        self.crop_size = None if crop_size is None else process_sz(crop_size)
+        
+    def default_crop_size(self, w,h): return default_crop_size(w,h)
+
+    def __call__(self, x):
+        csize = self.default_crop_size(*x.size) if self.crop_size is None else self.crop_size
+        return x.transform(self.size, PIL.Image.EXTENT, self.get_corners(*x.size, *csize), resample=self.resample)
+    
+    def get_corners(self, w, h): return (0,0,w,h)
+
+class CenterCrop(GeneralCrop):
+    def __init__(self, size, scale=1.14, resample=PIL.Image.BILINEAR):
+        super().__init__(size, resample=resample)
+        self.scale = scale
+        
+    def default_crop_size(self, w,h): return [w/self.scale,h/self.scale]
+    
+    def get_corners(self, w, h, wc, hc):
+        return ((w-wc)//2, (h-hc)//2, (w-wc)//2+wc, (h-hc)//2+hc)
+
+
+# -
+
+il1.tfms = [make_rgb, CenterCrop(128), to_byte_tensor, to_float_tensor]
+
+show_batch(next(iter(dl)))
+
+
+# ### RandomResizeCrop
+
+# This is the usual data augmentation used on ImageNet (introduced [here](https://arxiv.org/pdf/1409.4842.pdf)) that consists of selecting 8 to 100% of the image area and a scale between 3/4 and 4/3 as a crop, then resizing it to the desired size. It combines some zoom and a bit of squishing at a very low computational cost.
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=7629)
+
+# export
+class RandomResizedCrop(GeneralCrop):
+    def __init__(self, size, scale=(0.08,1.0), ratio=(3./4., 4./3.), resample=PIL.Image.BILINEAR):
+        super().__init__(size, resample=resample)
+        self.scale,self.ratio = scale,ratio
+    
+    def get_corners(self, w, h, wc, hc):
+        area = w*h
+        #Tries 10 times to get a proper crop inside the image.
+        for attempt in range(10):
+            area = random.uniform(*self.scale) * area
+            ratio = math.exp(random.uniform(math.log(self.ratio[0]), math.log(self.ratio[1])))
+            new_w = int(round(math.sqrt(area * ratio)))
+            new_h = int(round(math.sqrt(area / ratio)))
+            if new_w <= w and new_h <= h:
+                left = random.randint(0, w - new_w)
+                top  = random.randint(0, h - new_h)
+                return (left, top, left + new_w, top + new_h)
+        
+        # Fallback to squish
+        if   w/h < self.ratio[0]: size = (w, int(w/self.ratio[0]))
+        elif w/h > self.ratio[1]: size = (int(h*self.ratio[1]), h)
+        else:                     size = (w, h)
+        return ((w-size[0])//2, (h-size[1])//2, (w+size[0])//2, (h+size[1])//2)
+
+
+il1.tfms = [make_rgb, RandomResizedCrop(128), to_byte_tensor, to_float_tensor]
+
+show_batch(next(iter(dl)))
+
+# ### Perspective warping
+
+# To do perspective warping, we map the corners of the image to new points: for instance, if we want to tilt the image so that the top looks closer to us, the top/left corner needs to be shifted to the right and the top/right to the left. To avoid squishing, the bottom/left corner needs to be shifted to the left and the bottom/right corner to the right. For instance, if we have an image with corners in:
+# ```
+# (60,60,60,280,280,280,280,60)
+# ```
+# (top/left, bottom/left, bottom/right, top/right) then a warped version is:
+# ```
+# (90,60,30,280,310,280,250,60)
+# ```
+# PIL can do this for us but it requires 8 coefficients we need to calculate. The math isn't the most important here, as we've done it for you. We need to solve this [equation](https://web.archive.org/web/20150222120106/xenia.media.mit.edu/~cwren/interpolator/). The equation solver is called `torch.solve` in PyTorch.
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=7751)
+
+# +
+# export
+from torch import FloatTensor,LongTensor
+
+def find_coeffs(orig_pts, targ_pts):
+    matrix = []
+    #The equations we'll need to solve.
+    for p1, p2 in zip(targ_pts, orig_pts):
+        matrix.append([p1[0], p1[1], 1, 0, 0, 0, -p2[0]*p1[0], -p2[0]*p1[1]])
+        matrix.append([0, 0, 0, p1[0], p1[1], 1, -p2[1]*p1[0], -p2[1]*p1[1]])
+
+    A = FloatTensor(matrix)
+    B = FloatTensor(orig_pts).view(8, 1)
+    #The 8 scalars we seek are solution of AX = B
+    return list(torch.solve(B,A)[0][:,0])
+
+
+# -
+
+# export
+def warp(img, size, src_coords, resample=PIL.Image.BILINEAR):
+    w,h = size
+    targ_coords = ((0,0),(0,h),(w,h),(w,0))
+    c = find_coeffs(src_coords,targ_coords)
+    res = img.transform(size, PIL.Image.PERSPECTIVE, list(c), resample=resample)
+    return res
+
+
+targ = ((0,0),(0,128),(128,128),(128,0))
+src  = ((90,60),(30,280),(310,280),(250,60))
+
+c = find_coeffs(src, targ)
+img.transform((128,128), PIL.Image.PERSPECTIVE, list(c), resample=resample)
+
+# %timeit -n 10 warp(img, (128,128), src)
+
+# %timeit -n 10 warp(img, (128,128), src, resample=PIL.Image.NEAREST)
+
+warp(img, (64,64), src, resample=PIL.Image.BICUBIC)
+
+warp(img, (64,64), src, resample=PIL.Image.NEAREST)
+
+
+# export
+def uniform(a,b): return a + (b-a) * random.random()
+
+
+# We can add a transform to do this perspective warping automatically with the rand resize and crop.
+
+class PilTiltRandomCrop(PilTransform):
+    def __init__(self, size, crop_size=None, magnitude=0., resample=PIL.Image.NEAREST): 
+        self.resample,self.size,self.magnitude = resample,process_sz(size),magnitude
+        self.crop_size = None if crop_size is None else process_sz(crop_size)
+        
+    def __call__(self, x):
+        csize = default_crop_size(*x.size) if self.crop_size is None else self.crop_size
+        up_t,lr_t = uniform(-self.magnitude, self.magnitude),uniform(-self.magnitude, self.magnitude)
+        left,top = randint(0,x.size[0]-csize[0]),randint(0,x.size[1]-csize[1])
+        src_corners = tensor([[-up_t, -lr_t], [up_t, 1+lr_t], [1-up_t, 1-lr_t], [1+up_t, lr_t]])
+        src_corners = src_corners * tensor(csize).float() + tensor([left,top]).float()
+        src_corners = tuple([(int(o[0].item()), int(o[1].item())) for o in src_corners])
+        return warp(x, self.size, src_corners, resample=self.resample)
+
+
+il1.tfms = [make_rgb, PilTiltRandomCrop(128, magnitude=0.1), to_byte_tensor, to_float_tensor]
+
+x = next(iter(dl))
+show_batch(x)
+
+
+# Problem is that black padding appears as soon as our target points are outside of the image, so we have to limit the magnitude if we want to avoid that.
+
+# export
+class PilTiltRandomCrop(PilTransform):
+    def __init__(self, size, crop_size=None, magnitude=0., resample=PIL.Image.BILINEAR): 
+        self.resample,self.size,self.magnitude = resample,process_sz(size),magnitude
+        self.crop_size = None if crop_size is None else process_sz(crop_size)
+        
+    def __call__(self, x):
+        csize = default_crop_size(*x.size) if self.crop_size is None else self.crop_size
+        left,top = randint(0,x.size[0]-csize[0]),randint(0,x.size[1]-csize[1])
+        top_magn = min(self.magnitude, left/csize[0], (x.size[0]-left)/csize[0]-1)
+        lr_magn  = min(self.magnitude, top /csize[1], (x.size[1]-top) /csize[1]-1)
+        up_t,lr_t = uniform(-top_magn, top_magn),uniform(-lr_magn, lr_magn)
+        src_corners = tensor([[-up_t, -lr_t], [up_t, 1+lr_t], [1-up_t, 1-lr_t], [1+up_t, lr_t]])
+        src_corners = src_corners * tensor(csize).float() + tensor([left,top]).float()
+        src_corners = tuple([(int(o[0].item()), int(o[1].item())) for o in src_corners])
+        return warp(x, self.size, src_corners, resample=self.resample)
+
+
+il1.tfms = [make_rgb, PilTiltRandomCrop(128, 200, magnitude=0.2), to_byte_tensor, to_float_tensor]
+
+x = next(iter(dl))
+show_batch(x)
+
+# ### Faster tensor creation
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=7971)
+
+[(o._order,o) for o in sorted(tfms, key=operator.attrgetter('_order'))]
+
+# +
+#export
+import numpy as np
+
+def np_to_float(x): return torch.from_numpy(np.array(x, dtype=np.float32, copy=False)).permute(2,0,1).contiguous()/255.
+np_to_float._order = 30
+# -
+
+# It is actually faster to combine `to_float_tensor` and `to_byte_tensor` in one transform using numpy.
+
+# %timeit -n 10 to_float_tensor(to_byte_tensor(img))
+
+# %timeit -n 10 np_to_float(img)
+
+# ## Batch data augmentation
+
+# You can write your own augmentation for your domain's data types, and have them run on the GPU, by using regular PyTorch tensor operations. Here's an example for images. The key is to do them on a whole batch at a time. Nearly all PyTorch operations can be done batch-wise.
+
+# Once we have resized our images so that we can batch them together, we can apply more data augmentation on a batch level. For the affine/coord transforms, we proceed like this:
+# 1. generate a grid map of the size of our batch (bs x height x width x 2) that contains the coordinates of a grid of size height x width (this will be the final size of the image, and doesn't have to be the same as the current size in the batch)
+# 2. apply the affine transforms (which is a matrix multiplication) and the coord transforms to that grid map
+# 3. interpolate the values of the final pixels we want from the initial images in the batch, according to the transformed grid map
+#
+# For 1. and 3. there are PyTorch functions: `F.affine_grid` and `F.grid_sample`. `F.affine_grid` can even combine 1 and 2 if we just want to do an affine transformation.
+
+# [Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=8029)
+
+# ### Step 1: generate the grid
+
+il1.tfms = [make_rgb, PilTiltRandomCrop(128, magnitude=0.2), to_byte_tensor, to_float_tensor]
+
+dl = DataLoader(il1, 64)
+
+x = next(iter(dl))
+
+from torch import FloatTensor
+
+
+def affine_grid_cpu(size):
+    N, C, H, W = size
+    grid = FloatTensor(N, H, W, 2)
+    linear_points = torch.linspace(-1, 1, W) if W > 1 else tensor([-1])
+    grid[:, :, :, 0] = torch.ger(torch.ones(H), linear_points).expand_as(grid[:, :, :, 0])
+    linear_points = torch.linspace(-1, 1, H) if H > 1 else tensor([-1])
+    grid[:, :, :, 1] = torch.ger(linear_points, torch.ones(W)).expand_as(grid[:, :, :, 1])
+    return grid
+
+
+grid = affine_grid_cpu(x.size())
+
+grid.shape
+
+grid[0,:5,:5]
+
+# %timeit -n 10 grid = affine_grid_cpu(x.size())
+
+# Coords in the grid go from -1, to 1 (PyTorch convention).
+
+# PyTorch version is slower on the CPU but optimized to go very fast on the GPU
+
+m = tensor([[1., 0., 0.], [0., 1., 0.]])
+theta = m.expand(x.size(0), 2, 3)
+
+theta.shape
+
+
+# %timeit -n 10 grid = F.affine_grid(theta, x.size())
+
+# %timeit -n 10 grid = F.affine_grid(theta.cuda(), x.size())
+
+# So we write our own version that dispatches on the CPU with our function and uses PyTorch's on the GPU.
+
+def affine_grid(x, size):
+    size = (size,size) if isinstance(size, int) else tuple(size)
+    size = (x.size(0),x.size(1)) + size
+    if x.device.type == 'cpu': return affine_grid_cpu(size) 
+    m = tensor([[1., 0., 0.], [0., 1., 0.]], device=x.device)
+    return F.affine_grid(m.expand(x.size(0), 2, 3), size)
+
+
+grid = affine_grid(x, 128)
+
+# ### Step 2: Affine multiplication
+
+# In 2D an affine transformation has the form y = Ax + b where A is a 2x2 matrix and b a vector with 2 coordinates. It's usually represented by the 3x3 matrix
+# ```
+# A[0,0]  A[0,1]  b[0]
+# A[1,0]  A[1,1]  b[1]
+#    0       0     1
+# ```
+# because then the composition of two affine transforms can be computed with the matrix product of their 3x3 representations.
+
+from torch import stack,zeros_like,ones_like
+
+
+# The matrix for a rotation that has an angle of `theta` is:
+# ```
+# cos(theta) -sin(theta) 0
+# sin(theta)  cos(theta) 0
+# 0           0          1
+# ```
+# Here we have to apply the reciprocal of a regular rotation (exercise: find why!) so we use this matrix:
+# ```
+#  cos(theta) sin(theta) 0
+# -sin(theta) cos(theta) 0
+#  0          0          1
+# ```
+# then we draw a different `theta` for each version of the image in the batch to return a batch of rotation matrices (size `bs x 3 x 3`).
+
+def rotation_matrix(thetas):
+    thetas.mul_(math.pi/180)
+    rows = [stack([thetas.cos(),             thetas.sin(),             torch.zeros_like(thetas)], dim=1),
+            stack([-thetas.sin(),            thetas.cos(),             torch.zeros_like(thetas)], dim=1),
+            stack([torch.zeros_like(thetas), torch.zeros_like(thetas), torch.ones_like(thetas)], dim=1)]
+    return stack(rows, dim=1)
+
+
+thetas = torch.empty(x.size(0)).uniform_(-30,30)
+
+thetas[:5]
+
+m = rotation_matrix(thetas)
+
+m.shape, m[:,None].shape, grid.shape
+
+grid.view(64,-1,2).shape
+
+# We have to apply our rotation to every point in the grid. The matrix a is given by the first two rows and two columns of `m` and the vector `b` is the first two coefficients of the last column. Of course we have to deal with the fact that here `m` is  a batch of matrices.
+
+a = m[:,:2,:2]
+b = m[:, 2:,:2]
+tfm_grid = (grid.view(64,-1,2) @ a + b).view(64, 128, 128, 2)
+
+# We can also do this without the `view` by using broadcasting. 
+
+# %timeit -n 10 tfm_grid = grid @ m[:,None,:2,:2] + m[:,2,:2][:,None,None]
+
+# %timeit -n 10 tfm_grid = torch.einsum('bijk,bkl->bijl', grid, m[:,:2,:2]) + m[:,2,:2][:,None,None]
+
+# %timeit -n 10 tfm_grid = torch.matmul(grid, m[:,:2,:2].unsqueeze(1)) + m[:,2,:2][:,None,None]
+
+# %timeit -n 10 tfm_grid = (torch.bmm(grid.view(64,-1,2), m[:,:2,:2]) + m[:,2,:2][:,None]).view(-1, 128, 128, 2)
+
+# And on the GPU
+
+grid = grid.cuda()
+m = m.cuda()
+
+# %timeit -n 10 tfm_grid = grid @ m[:,None,:2,:2] + m[:,2,:2][:,None,None]
+
+# %timeit -n 10 tfm_grid = torch.einsum('bijk,bkl->bijl', grid, m[:,:2,:2]) + m[:,2,:2][:,None,None]
+
+# %timeit -n 10 tfm_grid = torch.matmul(grid, m[:,:2,:2].unsqueeze(1)) + m[:,2,:2][:,None,None]
+
+# %timeit -n 10 tfm_grid = (torch.bmm(grid.view(64,-1,2), m[:,:2,:2]) + m[:,2,:2][:,None]).view(-1, 128, 128, 2)
+
+# ### Step 3: interpolate
+
+# Since `bmm` is always the fastest, we use this one for the matrix multiplication.
+
+tfm_grid = torch.bmm(grid.view(64,-1,2), m[:,:2,:2]).view(-1, 128, 128, 2)
+
+# The interpolation to find our coordinates back is done by `grid_sample`.
+
+tfm_x = F.grid_sample(x, tfm_grid.cpu())
+
+show_batch(tfm_x, r=2)
+
+# It takes a `padding_mode` argument.
+
+tfm_x = F.grid_sample(x, tfm_grid.cpu(), padding_mode='reflection')
+
+show_batch(tfm_x, r=2)
+
+
+# ### Timing
+
+# Let's look at the speed now!
+
+def rotate_batch(x, size, degrees):
+    grid = affine_grid(x, size)
+    thetas = x.new(x.size(0)).uniform_(-degrees,degrees)
+    m = rotation_matrix(thetas)
+    tfm_grid = grid @ m[:,:2,:2].unsqueeze(1) + m[:,2,:2][:,None,None]
+    return F.grid_sample(x, tfm_grid)
+
+
+show_batch(rotate_batch(x, 128, 30), r=2)
+
+# %timeit -n 10 tfm_x = rotate_batch(x, 128, 30)
+
+# %timeit -n 10 tfm_x = rotate_batch(x.cuda(), 128, 30)
+
+# Not bad for 64 rotations!
+
+# ### Jit version
+
+# But we can be even faster!
+
+from torch import Tensor
+
+# +
+from torch.jit import script
+
+@script
+def rotate_batch(x:Tensor, size:int, degrees:float) -> Tensor:
+    sz = (x.size(0),x.size(1)) + (size,size)
+    idm = torch.zeros(2,3, device=x.device)
+    idm[0,0] = 1.
+    idm[1,1] = 1.
+    grid = F.affine_grid(idm.expand(x.size(0), 2, 3), sz)
+    thetas = torch.zeros(x.size(0), device=x.device).uniform_(-degrees,degrees)
+    m = rotation_matrix(thetas)
+    tfm_grid = torch.matmul(grid, m[:,:2,:2].unsqueeze(1)) + m[:,2,:2].unsqueeze(1).unsqueeze(2)
+    return F.grid_sample(x, tfm_grid)
+
+
+# -
+
+m = tensor([[1., 0., 0.], [0., 1., 0.]], device=x.device)
+
+
+# %timeit -n 10 tfm_x = rotate_batch(x.cuda(), 128, 30)
+
+# The speed of this depends a lot on what card you have. On a V100 it is generally about 3x faster than non-JIT (as at April 2019) although PyTorch JIT is rapidly improving.
+
+# ### affine multiplication with `affine_grid`
+
+# And even faster if we give the matrix rotation to `affine_grid`.
+
+def rotate_batch(x, size, degrees):
+    size = (size,size) if isinstance(size, int) else tuple(size)
+    size = (x.size(0),x.size(1)) + size
+    thetas = x.new(x.size(0)).uniform_(-degrees,degrees)
+    m = rotation_matrix(thetas)
+    grid = F.affine_grid(m[:,:2], size)
+    return F.grid_sample(x.cuda(), grid)
+
+
+# %timeit -n 10 tfm_x = rotate_batch(x.cuda(), 128, 30)
+
+# ## Export
+
+!./notebook2script.py 10_augmentation.ipynb
+
+
diff --git a/nbs/dl2/10b_mixup_label_smoothing.ipynb b/nbs/dl2/10b_mixup_label_smoothing.ipynb
index 974da20..b1bed92 100644
--- ./nbs/dl2/10b_mixup_label_smoothing.ipynb
+++ ./nbs/dl2/10b_mixup_label_smoothing.ipynb
@@ -522,6 +522,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/10b_mixup_label_smoothing.py b/nbs/dl2/10b_mixup_label_smoothing.py
new file mode 100644
index 0000000..07aa798
--- /dev/null
+++ ./nbs/dl2/10b_mixup_label_smoothing.py
@@ -0,0 +1,259 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Mixup / Label smoothing
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+#export
+from exp.nb_10 import *
+
+path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)
+
+# +
+tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor]
+bs = 64
+
+il = ImageList.from_files(path, tfms=tfms)
+sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='val'))
+ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())
+data = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=4)
+# -
+
+# ## Mixup
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=226)
+
+# ### What is mixup?
+#
+# As the name kind of suggests, the authors of the [mixup article](https://arxiv.org/abs/1710.09412) propose to train the model on a mix of the pictures of the training set. Let's say we're on CIFAR10 for instance, then instead of feeding the model the raw images, we take two (which could be in the same class or not) and do a linear combination of them: in terms of tensor it's
+# ``` python
+# new_image = t * image1 + (1-t) * image2
+# ```
+# where t is a float between 0 and 1. Then the target we assign to that image is the same combination of the original targets:
+# ``` python
+# new_target = t * target1 + (1-t) * target2
+# ```
+# assuming your targets are one-hot encoded (which isn't the case in pytorch usually). And that's as simple as this.
+
+img1 = PIL.Image.open(ll.train.x.items[0])
+img1
+
+img2 = PIL.Image.open(ll.train.x.items[4000])
+img2
+
+mixed_up = ll.train.x[0] * 0.3 + ll.train.x[4000] * 0.7
+plt.imshow(mixed_up.permute(1,2,0));
+
+# French horn or tench? The right answer is 70% french horn and 30% tench ;)
+
+# ### Implementation
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=490)
+
+# The implementation relies on something called the *beta distribution* which in turns uses something which Jeremy still finds mildly terrifying called the *gamma function*. To get over his fears, Jeremy reminds himself that *gamma* is just a factorial function that (kinda) interpolates nice and smoothly to non-integers too. How it does that exactly isn't important...
+
+# PyTorch has a log-gamma but not a gamma, so we'll create one
+Γ = lambda x: x.lgamma().exp()
+
+# NB: If you see math symbols you don't know you can google them like this: [Γ function](https://www.google.com/search?q=Γ+function).
+#
+# If you're not used to typing unicode symbols, on Mac type <kbd>ctrl</kbd>-<kbd>cmd</kbd>-<kbd>space</kbd> to bring up a searchable emoji box. On Linux you can use the [compose key](https://help.ubuntu.com/community/ComposeKey). On Windows you can also use a compose key, but you first need to install [WinCompose](https://github.com/samhocevar/wincompose). By default the <kbd>compose</kbd> key is the right-hand <kbd>Alt</kbd> key.
+#
+# You can search for symbol names in WinCompose. The greek letters are generally <kbd>compose</kbd>-<kbd>\*</kbd>-<kbd>letter</kbd> (where *letter* is, for instance, <kbd>a</kbd> to get greek α alpha).
+
+facts = [math.factorial(i) for i in range(7)]
+
+plt.plot(range(7), facts, 'ro')
+plt.plot(torch.linspace(0,6), Γ(torch.linspace(0,6)+1))
+plt.legend(['factorial','Γ']);
+
+torch.linspace(0,0.9,10)
+
+# In the original article, the authors suggested three things:
+#  1. Create two separate dataloaders and draw a batch from each at every iteration to mix them up
+#  2. Draw a t value following a beta distribution with a parameter α (0.4 is suggested in their article)
+#  3. Mix up the two batches with the same value t.
+#  4. Use one-hot encoded targets
+#
+# Why the beta distribution with the same parameters α? Well it looks like this:
+
+_,axs = plt.subplots(1,2, figsize=(12,4))
+x = torch.linspace(0,1, 100)
+for α,ax in zip([0.1,0.8], axs):
+    α = tensor(α)
+#     y = (x.pow(α-1) * (1-x).pow(α-1)) / (gamma_func(α ** 2) / gamma_func(α))
+    y = (x**(α-1) * (1-x)**(α-1)) / (Γ(α)**2 / Γ(2*α))
+    ax.plot(x,y)
+    ax.set_title(f"α={α:.1}")
+
+
+# With a low `α`, we pick values close to 0. and 1. with a high probability, and the values in the middle  all have the same kind of probability. With a greater `α`, 0. and 1. get a lower probability .
+
+# While the approach above works very well, it's not the fastest way we can do this. The main point that slows down this process is wanting two different batches at every iteration (which means loading twice the amount of images and applying to them the other data augmentation function). To avoid this slow down, we can be a little smarter and mixup a batch with a shuffled version of itself (this way the images mixed up are still different). This was a trick suggested in the MixUp paper.
+#
+# Then pytorch was very careful to avoid one-hot encoding targets when it could, so it seems a bit of a drag to undo this. Fortunately for us, if the loss is a classic cross-entropy, we have
+# ```python
+# loss(output, new_target) = t * loss(output, target1) + (1-t) * loss(output, target2)
+# ```
+# so we won't one-hot encode anything and just compute those two losses then do the linear combination.
+#
+# Using the same parameter t for the whole batch also seemed a bit inefficient. In our experiments, we noticed that the model can train faster if we draw a different t for every image in the batch (both options get to the same result in terms of accuracy, it's just that one arrives there more slowly).
+# The last trick we have to apply with this is that there can be some duplicates with this strategy: let's say or shuffle say to mix image0 with image1 then image1 with image0, and that we draw t=0.1 for the first, and t=0.9 for the second. Then
+# ```python
+# image0 * 0.1 + shuffle0 * (1-0.1) = image0 * 0.1 + image1 * 0.9
+# image1 * 0.9 + shuffle1 * (1-0.9) = image1 * 0.9 + image0 * 0.1
+# ```
+# will be the same. Of course, we have to be a bit unlucky but in practice, we saw there was a drop in accuracy by using this without removing those near-duplicates. To avoid them, the tricks is to replace the vector of parameters we drew by
+# ``` python
+# t = max(t, 1-t)
+# ```
+# The beta distribution with the two parameters equal is symmetric in any case, and this way we insure that the biggest coefficient is always near the first image (the non-shuffled batch).
+#
+
+# In `Mixup` we have handle loss functions that have an attribute `reduction` (like `nn.CrossEntropy()`). To deal with the `reduction=None` with various types of loss function without modifying the actual loss function outside of the scope we need to perform those operations with no reduction, we create a context manager:
+
+#export
+class NoneReduce():
+    def __init__(self, loss_func): 
+        self.loss_func,self.old_red = loss_func,None
+        
+    def __enter__(self):
+        if hasattr(self.loss_func, 'reduction'):
+            self.old_red = getattr(self.loss_func, 'reduction')
+            setattr(self.loss_func, 'reduction', 'none')
+            return self.loss_func
+        else: return partial(self.loss_func, reduction='none')
+        
+    def __exit__(self, type, value, traceback):
+        if self.old_red is not None: setattr(self.loss_func, 'reduction', self.old_red)    
+
+
+# Then we can use it in `MixUp`:
+
+# +
+#export
+from torch.distributions.beta import Beta
+
+def unsqueeze(input, dims):
+    for dim in listify(dims): input = torch.unsqueeze(input, dim)
+    return input
+
+def reduce_loss(loss, reduction='mean'):
+    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss    
+
+
+# -
+
+#export
+class MixUp(Callback):
+    _order = 90 #Runs after normalization and cuda
+    def __init__(self, α:float=0.4): self.distrib = Beta(tensor([α]), tensor([α]))
+    
+    def begin_fit(self): self.old_loss_func,self.run.loss_func = self.run.loss_func,self.loss_func
+    
+    def begin_batch(self):
+        if not self.in_train: return #Only mixup things during training
+        λ = self.distrib.sample((self.yb.size(0),)).squeeze().to(self.xb.device)
+        λ = torch.stack([λ, 1-λ], 1)
+        self.λ = unsqueeze(λ.max(1)[0], (1,2,3))
+        shuffle = torch.randperm(self.yb.size(0)).to(self.xb.device)
+        xb1,self.yb1 = self.xb[shuffle],self.yb[shuffle]
+        self.run.xb = lin_comb(self.xb, xb1, self.λ)
+        
+    def after_fit(self): self.run.loss_func = self.old_loss_func
+    
+    def loss_func(self, pred, yb):
+        if not self.in_train: return self.old_loss_func(pred, yb)
+        with NoneReduce(self.old_loss_func) as loss_func:
+            loss1 = loss_func(pred, yb)
+            loss2 = loss_func(pred, self.yb1)
+        loss = lin_comb(loss1, loss2, self.λ)
+        return reduce_loss(loss, getattr(self.old_loss_func, 'reduction', 'mean'))
+
+
+nfs = [32,64,128,256,512]
+
+
+def get_learner(nfs, data, lr, layer, loss_func=F.cross_entropy,
+                cb_funcs=None, opt_func=optim.SGD, **kwargs):
+    model = get_cnn_model(data, nfs, layer, **kwargs)
+    init_cnn(model)
+    return Learner(model, data, loss_func, lr=lr, cb_funcs=cb_funcs, opt_func=opt_func)
+
+
+cbfs = [partial(AvgStatsCallback,accuracy),
+        CudaCallback, 
+        ProgressCallback,
+        partial(BatchTransformXCallback, norm_imagenette),
+        MixUp]
+
+learn = get_learner(nfs, data, 0.4, conv_layer, cb_funcs=cbfs)
+
+learn.fit(1)
+
+
+# Questions: How does softmax interact with all this? Should we jump straight from mixup to inference?
+
+# ## Label smoothing
+
+# Another regularization technique that's often used is label smoothing. It's designed to make the model a little bit less certain of it's decision by changing a little bit its target: instead of wanting to predict 1 for the correct class and 0 for all the others, we ask it to predict `1-ε` for the correct class and `ε` for all the others, with `ε` a (small) positive number and N the number of classes. This can be written as:
+#
+# $$loss = (1-ε) ce(i) + ε \sum ce(j) / N$$
+#
+# where `ce(x)` is cross-entropy of `x` (i.e. $-\log(p_{x})$), and `i` is the correct class. This can be coded in a loss function:
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=1121)
+
+#export
+class LabelSmoothingCrossEntropy(nn.Module):
+    def __init__(self, ε:float=0.1, reduction='mean'):
+        super().__init__()
+        self.ε,self.reduction = ε,reduction
+    
+    def forward(self, output, target):
+        c = output.size()[-1]
+        log_preds = F.log_softmax(output, dim=-1)
+        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)
+        nll = F.nll_loss(log_preds, target, reduction=self.reduction)
+        return lin_comb(loss/c, nll, self.ε)
+
+
+# Note: we implement the various reduction attributes so that it plays nicely with MixUp after.
+
+cbfs = [partial(AvgStatsCallback,accuracy),
+        CudaCallback,
+        ProgressCallback,
+        partial(BatchTransformXCallback, norm_imagenette)]
+
+learn = get_learner(nfs, data, 0.4, conv_layer, cb_funcs=cbfs, loss_func=LabelSmoothingCrossEntropy())
+
+learn.fit(1)
+
+# And we can check our loss function `reduction` attribute hasn't changed outside of the training loop:
+
+assert learn.loss_func.reduction == 'mean'
+
+# ## Export
+
+!./notebook2script.py 10b_mixup_label_smoothing.ipynb
+
+
diff --git a/nbs/dl2/10c_fp16.ipynb b/nbs/dl2/10c_fp16.ipynb
index 996aae4..bf071f2 100644
--- ./nbs/dl2/10c_fp16.ipynb
+++ ./nbs/dl2/10c_fp16.ipynb
@@ -1022,6 +1022,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/10c_fp16.py b/nbs/dl2/10c_fp16.py
new file mode 100644
index 0000000..957d75a
--- /dev/null
+++ ./nbs/dl2/10c_fp16.py
@@ -0,0 +1,499 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Training in mixed precision
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+#export
+from exp.nb_10b import *
+
+# ## A little bit of theory
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=1318)
+
+# Continuing the documentation on the fastai_v1 development here is a brief piece about mixed precision training. A very nice and clear introduction to it is [this video from NVIDIA](http://on-demand.gputechconf.com/gtc/2018/video/S81012/).
+#
+# ### What's half precision?
+# In neural nets, all the computations are usually done in single precision, which means all the floats in all the arrays that represent inputs, activations, weights... are 32-bit floats (FP32 in the rest of this post). An idea to reduce memory usage (and avoid those annoying cuda errors) has been to try and do the same thing in half-precision, which means using 16-bits floats (or FP16 in the rest of this post). By definition, they take half the space in RAM, and in theory could allow you to double the size of your model and double your batch size.
+#
+# Another very nice feature is that NVIDIA developed its latest GPUs (the Volta generation) to take fully advantage of half-precision tensors. Basically, if you give half-precision tensors to those, they'll stack them so that each core can do more operations at the same time, and theoretically gives an 8x speed-up (sadly, just in theory).
+#
+# So training at half precision is better for your memory usage, way faster if you have a Volta GPU (still a tiny bit faster if you don't since the computations are easiest). How do we do it? Super easily in pytorch, we just have to put .half() everywhere: on the inputs of our model and all the parameters. Problem is that you usually won't see the same accuracy in the end (so it happens sometimes) because half-precision is... well... not as precise ;).
+#
+# ### Problems with half-precision:
+# To understand the problems with half precision, let's look briefly at what an FP16 looks like (more information [here](https://en.wikipedia.org/wiki/Half-precision_floating-point_format)).
+#
+# ![half float](images/half.png)
+#
+# The sign bit gives us +1 or -1, then we have 5 bits to code an exponent between -14 and 15, while the fraction part has the remaining 10 bits. Compared to FP32, we have a smaller range of possible values (2e-14 to 2e15 roughly, compared to 2e-126 to 2e127 for FP32) but also a smaller *offset*.
+#
+# For instance, between 1 and 2, the FP16 format only represents the number 1, 1+2e-10, 1+2*2e-10... which means that 1 + 0.0001 = 1 in half precision. That's what will cause a certain numbers of problems, specifically three that can occur and mess up your training.
+# 1. The weight update is imprecise: inside your optimizer, you basically do w = w - lr * w.grad for each weight of your network. The problem in performing this operation in half precision is that very often, w.grad is several orders of magnitude below w, and the learning rate is also small. The situation where w=1 and lr*w.grad is 0.0001 (or lower) is therefore very common, but the update doesn't do anything in those cases.
+# 2. Your gradients can underflow. In FP16, your gradients can easily be replaced by 0 because they are too low.
+# 3. Your activations or loss can overflow. The opposite problem from the gradients: it's easier to hit nan (or infinity) in FP16 precision, and your training might more easily diverge.
+#
+# ### The solution: mixed precision training
+#
+# To address those three problems, we don't fully train in FP16 precision. As the name mixed training implies, some of the operations will be done in FP16, others in FP32. This is mainly to take care of the first problem listed above. For the next two there are additional tricks.
+#
+# The main idea is that we want to do the forward pass and the gradient computation in half precision (to go fast) but the update in single precision (to be more precise). It's okay if w and grad are both half floats, but when we do the operation w = w - lr * grad, we need to compute it in FP32. That way our 1 + 0.0001 is going to be 1.0001. 
+#
+# This is why we keep a copy of the weights in FP32 (called master model). Then, our training loop will look like:
+# 1. compute the output with the FP16 model, then the loss
+# 2. back-propagate the gradients in half-precision.
+# 3. copy the gradients in FP32 precision
+# 4. do the update on the master model (in FP32 precision)
+# 5. copy the master model in the FP16 model.
+#
+# Note that we lose precision during step 5, and that the 1.0001 in one of the weights will go back to 1. But if the next update corresponds to add 0.0001 again, since the optimizer step is done on the master model, the 1.0001 will become 1.0002 and if we eventually go like this up to 1.0005, the FP16 model will be able to tell the difference.
+#
+# That takes care of problem 1. For the second problem, we use something called gradient scaling: to avoid the gradients getting zeroed by the FP16 precision, we multiply the loss by a scale factor (scale=512 for instance). That way we can push the gradients to the right in the next figure, and have them not become zero.
+#
+# ![half float representation](images/half_representation.png)
+#
+# Of course we don't want those 512-scaled gradients to be in the weight update, so after converting them into FP32, we can divide them by this scale factor (once they have no risks of becoming 0). This changes the loop to:
+# 1. compute the output with the FP16 model, then the loss.
+# 2. multiply the loss by scale then back-propagate the gradients in half-precision.
+# 3. copy the gradients in FP32 precision then divide them by scale.
+# 4. do the update on the master model (in FP32 precision).
+# 5. copy the master model in the FP16 model.
+#
+# For the last problem, the tricks offered by NVIDIA are to leave the batchnorm layers in single precision (they don't have many weights so it's not a big memory challenge) and compute the loss in single precision (which means converting the last output of the model in single precision before passing it to the loss).
+#
+# ![Mixed precision training](images/Mixed_precision.jpeg)
+#
+# Implementing all of this in the new callback system is surprisingly easy, let's dig into this!
+
+# ## Util functions
+
+# Before going in the main `Callback` we will need some helper functions. We will refactor using the [APEX library](https://github.com/NVIDIA/apex) util functions. The python-only build is enough for what we will use here if you don't manage to do the CUDA/C++ installation.
+
+# export 
+import apex.fp16_utils as fp16
+
+# ### Converting the model to FP16
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=1425)
+
+# We will need a function to convert all the layers of the model to FP16 precision except the BatchNorm-like layers (since those need to be done in FP32 precision to be stable). We do this in two steps: first we convert the model to FP16, then we loop over all the layers and put them back to FP32 if they are a BatchNorm layer.
+
+bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)
+
+
+def bn_to_float(model):
+    if isinstance(model, bn_types): model.float()
+    for child in model.children():  bn_to_float(child)
+    return model
+
+
+def model_to_half(model):
+    model = model.half()
+    return bn_to_float(model)
+
+
+# Let's test this:
+
+model = nn.Sequential(nn.Linear(10,30), nn.BatchNorm1d(30), nn.Linear(30,2)).cuda()
+model = model_to_half(model)
+
+
+def check_weights(model):
+    for i,t in enumerate([torch.float16, torch.float32, torch.float16]):
+        assert model[i].weight.dtype == t
+        assert model[i].bias.dtype   == t
+
+
+check_weights(model)
+
+# In Apex, the function that does this for us is `convert_network`. We can use it to put the model in FP16 or back to FP32.
+
+model = nn.Sequential(nn.Linear(10,30), nn.BatchNorm1d(30), nn.Linear(30,2)).cuda()
+model = fp16.convert_network(model, torch.float16)
+check_weights(model)
+
+# ### Creating the master copy of the parameters
+
+# From our model parameters (mostly in FP16), we'll want to create a copy in FP32 (master parameters) that we will use for the step in the optimizer. Optionally, we concatenate all the parameters to do one flat big tensor, which can make that step a little bit faster.
+
+# +
+from torch.nn.utils import parameters_to_vector
+
+def get_master(model, flat_master=False):
+    model_params = [param for param in model.parameters() if param.requires_grad]
+    if flat_master:
+        master_param = parameters_to_vector([param.data.float() for param in model_params])
+        master_param = torch.nn.Parameter(master_param, requires_grad=True)
+        if master_param.grad is None: master_param.grad = master_param.new(*master_param.size())
+        return model_params, [master_param]
+    else:
+        master_params = [param.clone().float().detach() for param in model_params]
+        for param in master_params: param.requires_grad_(True)
+        return model_params, master_params
+
+
+# -
+
+# The util function from Apex to do this is `prep_param_lists`.
+
+model_p,master_p = get_master(model)
+model_p1,master_p1 = fp16.prep_param_lists(model)
+
+
+def same_lists(ps1, ps2):
+    assert len(ps1) == len(ps2)
+    for (p1,p2) in zip(ps1,ps2): 
+        assert p1.requires_grad == p2.requires_grad
+        assert torch.allclose(p1.data.float(), p2.data.float())
+
+
+same_lists(model_p,model_p1)
+same_lists(model_p,master_p)
+same_lists(master_p,master_p1)
+same_lists(model_p1,master_p1)
+
+# We can't use flat_master when there is a mix of FP32 and FP16 parameters (like batchnorm here).
+
+model1 = nn.Sequential(nn.Linear(10,30), nn.Linear(30,2)).cuda()
+model1 = fp16.convert_network(model1, torch.float16)
+
+model_p,master_p = get_master(model1, flat_master=True)
+model_p1,master_p1 = fp16.prep_param_lists(model1, flat_master=True)
+
+same_lists(model_p,model_p1)
+same_lists(master_p,master_p1)
+
+assert len(master_p[0]) == 10*30 + 30 + 30*2 + 2
+assert len(master_p1[0]) == 10*30 + 30 + 30*2 + 2
+
+
+# The thing is that we don't always want all the parameters of our model in the same parameter group, because we might:
+# - want to do transfer learning and freeze some layers
+# - apply discriminative learning rates
+# - don't apply weight decay to some layers (like BatchNorm) or the bias terms
+#
+# So we actually need a function that splits the parameters of an optimizer (and not a model) according to the right parameter groups.
+
+def get_master(opt, flat_master=False):
+    model_params = [[param for param in pg if param.requires_grad] for pg in opt.param_groups]
+    if flat_master:
+        master_params = []
+        for pg in model_params:
+            mp = parameters_to_vector([param.data.float() for param in pg])
+            mp = torch.nn.Parameter(mp, requires_grad=True)
+            if mp.grad is None: mp.grad = mp.new(*mp.size())
+            master_params.append(mp)
+    else:
+        master_params = [[param.clone().float().detach() for param in pg] for pg in model_params]
+        for pg in master_params:
+            for param in pg: param.requires_grad_(True)
+    return model_params, master_params
+
+
+# ### Copy the gradients from model params to master params
+
+# After the backward pass, all gradients must be copied to the master params before the optimizer step can be done in FP32. We need a function for that (with a bit of adjustement if we have flat master).
+
+def to_master_grads(model_params, master_params, flat_master:bool=False)->None:
+    if flat_master:
+        if master_params[0].grad is None: master_params[0].grad = master_params[0].data.new(*master_params[0].data.size())
+        master_params[0].grad.data.copy_(parameters_to_vector([p.grad.data.float() for p in model_params]))
+    else:
+        for model, master in zip(model_params, master_params):
+            if model.grad is not None:
+                if master.grad is None: master.grad = master.data.new(*master.data.size())
+                master.grad.data.copy_(model.grad.data)
+            else: master.grad = None
+
+
+# The corresponding function in the Apex utils is `model_grads_to_master_grads`.
+
+x = torch.randn(20,10).half().cuda()
+z = model(x)
+loss = F.cross_entropy(z, torch.randint(0, 2, (20,)).cuda())
+loss.backward()
+
+to_master_grads(model_p, master_p)
+
+
+def check_grads(m1, m2):
+    for p1,p2 in zip(m1,m2): 
+        if p1.grad is None: assert p2.grad is None
+        else: assert torch.allclose(p1.grad.data, p2.grad.data) 
+
+
+check_grads(model_p, master_p)
+
+fp16.model_grads_to_master_grads(model_p, master_p)
+
+check_grads(model_p, master_p)
+
+# ### Copy the master params to the model params
+
+# After the step, we need to copy back the master parameters to the model parameters for the next update.
+
+# +
+from torch._utils import _unflatten_dense_tensors
+
+def to_model_params(model_params, master_params, flat_master:bool=False)->None:
+    if flat_master:
+        for model, master in zip(model_params, _unflatten_dense_tensors(master_params[0].data, model_params)):
+            model.data.copy_(master)
+    else:
+        for model, master in zip(model_params, master_params): model.data.copy_(master.data)
+
+
+# -
+
+# The corresponding function in Apex is `master_params_to_model_params`.
+
+# ### But we need to handle param groups
+
+# The thing is that we don't always want all the parameters of our model in the same parameter group, because we might:
+# - want to do transfer learning and freeze some layers
+# - apply discriminative learning rates
+# - don't apply weight decay to some layers (like BatchNorm) or the bias terms
+#
+# So we actually need a function that splits the parameters of an optimizer (and not a model) according to the right parameter groups and the following functions need to handle lists of lists of parameters (one list of each param group in `model_pgs` and `master_pgs`)
+
+# export 
+def get_master(opt, flat_master=False):
+    model_pgs = [[param for param in pg if param.requires_grad] for pg in opt.param_groups]
+    if flat_master:
+        master_pgs = []
+        for pg in model_pgs:
+            mp = parameters_to_vector([param.data.float() for param in pg])
+            mp = torch.nn.Parameter(mp, requires_grad=True)
+            if mp.grad is None: mp.grad = mp.new(*mp.size())
+            master_pgs.append([mp])
+    else:
+        master_pgs = [[param.clone().float().detach() for param in pg] for pg in model_pgs]
+        for pg in master_pgs:
+            for param in pg: param.requires_grad_(True)
+    return model_pgs, master_pgs
+
+
+# export 
+def to_master_grads(model_pgs, master_pgs, flat_master:bool=False)->None:
+    for (model_params,master_params) in zip(model_pgs,master_pgs):
+        fp16.model_grads_to_master_grads(model_params, master_params, flat_master=flat_master)
+
+
+# export 
+def to_model_params(model_pgs, master_pgs, flat_master:bool=False)->None:
+    for (model_params,master_params) in zip(model_pgs,master_pgs):
+        fp16.master_params_to_model_params(model_params, master_params, flat_master=flat_master)
+
+
+# ## The main Callback
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=1452)
+
+class MixedPrecision(Callback):
+    _order = 99
+    def __init__(self, loss_scale=512, flat_master=False):
+        assert torch.backends.cudnn.enabled, "Mixed precision training requires cudnn."
+        self.loss_scale,self.flat_master = loss_scale,flat_master
+
+    def begin_fit(self):
+        self.run.model = fp16.convert_network(self.model, dtype=torch.float16)
+        self.model_pgs, self.master_pgs = get_master(self.opt, self.flat_master)
+        #Changes the optimizer so that the optimization step is done in FP32.
+        self.run.opt.param_groups = self.master_pgs #Put those param groups inside our runner.
+        
+    def after_fit(self): self.model.float()
+
+    def begin_batch(self): self.run.xb = self.run.xb.half() #Put the inputs to half precision
+    def after_pred(self):  self.run.pred = self.run.pred.float() #Compute the loss in FP32
+    def after_loss(self):  self.run.loss *= self.loss_scale #Loss scaling to avoid gradient underflow
+
+    def after_backward(self):
+        #Copy the gradients to master and unscale
+        to_master_grads(self.model_pgs, self.master_pgs, self.flat_master)
+        for master_params in self.master_pgs:
+            for param in master_params:
+                if param.grad is not None: param.grad.div_(self.loss_scale)
+
+    def after_step(self):
+        #Zero the gradients of the model since the optimizer is disconnected.
+        self.model.zero_grad()
+        #Update the params from master to model.
+        to_model_params(self.model_pgs, self.master_pgs, self.flat_master)
+
+
+# Now let's test this on Imagenette
+
+path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)
+
+# +
+tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor]
+bs = 64
+
+il = ImageList.from_files(path, tfms=tfms)
+sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='val'))
+ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())
+data = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=4)
+# -
+
+nfs = [32,64,128,256,512]
+
+
+def get_learner(nfs, data, lr, layer, loss_func=F.cross_entropy,
+                cb_funcs=None, opt_func=adam_opt(), **kwargs):
+    model = get_cnn_model(data, nfs, layer, **kwargs)
+    init_cnn(model)
+    return Learner(model, data, loss_func, lr=lr, cb_funcs=cb_funcs, opt_func=opt_func)
+
+
+# Training without mixed precision
+
+cbfs = [partial(AvgStatsCallback,accuracy),
+        ProgressCallback,
+        CudaCallback,
+        partial(BatchTransformXCallback, norm_imagenette)]
+
+learn = get_learner(nfs, data, 1e-2, conv_layer, cb_funcs=cbfs)
+
+learn.fit(1)
+
+# Training with mixed precision
+
+cbfs = [partial(AvgStatsCallback,accuracy),
+        CudaCallback,
+        ProgressCallback,
+        partial(BatchTransformXCallback, norm_imagenette),
+        MixedPrecision]
+
+learn = get_learner(nfs, data, 1e-2, conv_layer, cb_funcs=cbfs)
+
+learn.fit(1)
+
+test_eq(next(learn.model.parameters()).type(), 'torch.cuda.FloatTensor')
+
+
+# ## Dynamic loss scaling
+
+# The only annoying thing with the previous implementation of mixed precision training is that it introduces one new hyper-parameter to tune, the value of the loss scaling. Fortunately for us, there is a way around this. We want the loss scaling to be as high as possible so that our gradients can use the whole range of representation, so let's first try a really high value. In all likelihood, this will cause our gradients or our loss to overflow, and we will try again with half that big value, and again, until we get to the largest loss scale possible that doesn't make our gradients overflow.
+#
+# This value will be perfectly fitted to our model and can continue to be dynamically adjusted as the training goes, if it's still too high, by just halving it each time we overflow. After a while though, training will converge and gradients will start to get smaller, so we also need a mechanism to get this dynamic loss scale larger if it's safe to do so. The strategy used in the Apex library is to multiply the loss scale by 2 each time we had a given number of iterations without overflowing.
+#
+# To check if the gradients have overflowed, we check their sum (computed in FP32). If one term is nan, the sum will be nan. Interestingly, on the GPU, it's faster than checking `torch.isnan`:
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=1472)
+
+# export 
+def test_overflow(x):
+    s = float(x.float().sum())
+    return (s == float('inf') or s == float('-inf') or s != s)
+
+
+x = torch.randn(512,1024).cuda()
+
+test_overflow(x)
+
+x[123,145] = float('inf')
+test_overflow(x)
+
+
+# %timeit test_overflow(x)
+
+# %timeit torch.isnan(x).any().item()
+
+# So we can use it in the following function that checks for gradient overflow:
+
+# export 
+def grad_overflow(param_groups):
+    for group in param_groups:
+        for p in group:
+            if p.grad is not None:
+                s = float(p.grad.data.float().sum())
+                if s == float('inf') or s == float('-inf') or s != s: return True
+    return False
+
+
+# And now we can write a new version of the `Callback` that handles dynamic loss scaling.
+
+# export 
+class MixedPrecision(Callback):
+    _order = 99
+    def __init__(self, loss_scale=512, flat_master=False, dynamic=True, max_loss_scale=2.**24, div_factor=2.,
+                 scale_wait=500):
+        assert torch.backends.cudnn.enabled, "Mixed precision training requires cudnn."
+        self.flat_master,self.dynamic,self.max_loss_scale = flat_master,dynamic,max_loss_scale
+        self.div_factor,self.scale_wait = div_factor,scale_wait
+        self.loss_scale = max_loss_scale if dynamic else loss_scale
+
+    def begin_fit(self):
+        self.run.model = fp16.convert_network(self.model, dtype=torch.float16)
+        self.model_pgs, self.master_pgs = get_master(self.opt, self.flat_master)
+        #Changes the optimizer so that the optimization step is done in FP32.
+        self.run.opt.param_groups = self.master_pgs #Put those param groups inside our runner.
+        if self.dynamic: self.count = 0
+
+    def begin_batch(self): self.run.xb = self.run.xb.half() #Put the inputs to half precision
+    def after_pred(self):  self.run.pred = self.run.pred.float() #Compute the loss in FP32
+    def after_loss(self):  
+        if self.in_train: self.run.loss *= self.loss_scale #Loss scaling to avoid gradient underflow
+
+    def after_backward(self):
+        #First, check for an overflow
+        if self.dynamic and grad_overflow(self.model_pgs):
+            #Divide the loss scale by div_factor, zero the grad (after_step will be skipped)
+            self.loss_scale /= self.div_factor
+            self.model.zero_grad()
+            return True #skip step and zero_grad
+        #Copy the gradients to master and unscale
+        to_master_grads(self.model_pgs, self.master_pgs, self.flat_master)
+        for master_params in self.master_pgs:
+            for param in master_params:
+                if param.grad is not None: param.grad.div_(self.loss_scale)
+        #Check if it's been long enough without overflow
+        if self.dynamic:
+            self.count += 1
+            if self.count == self.scale_wait:
+                self.count = 0
+                self.loss_scale *= self.div_factor
+
+    def after_step(self):
+        #Zero the gradients of the model since the optimizer is disconnected.
+        self.model.zero_grad()
+        #Update the params from master to model.
+        to_model_params(self.model_pgs, self.master_pgs, self.flat_master)
+
+
+cbfs = [partial(AvgStatsCallback,accuracy),
+        CudaCallback,
+        ProgressCallback,
+        partial(BatchTransformXCallback, norm_imagenette),
+        MixedPrecision]
+
+learn = get_learner(nfs, data, 1e-2, conv_layer, cb_funcs=cbfs)
+
+learn.fit(1)
+
+# The loss scale used is way higher than our previous number:
+
+learn.cbs[-1].loss_scale
+
+# ## Export
+
+!./notebook2script.py 10c_fp16.ipynb
+
+
diff --git a/nbs/dl2/11_train_imagenette.ipynb b/nbs/dl2/11_train_imagenette.ipynb
index 3480188..08b41f4 100644
--- ./nbs/dl2/11_train_imagenette.ipynb
+++ ./nbs/dl2/11_train_imagenette.ipynb
@@ -643,6 +643,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/11_train_imagenette.py b/nbs/dl2/11_train_imagenette.py
new file mode 100644
index 0000000..4755c8c
--- /dev/null
+++ ./nbs/dl2/11_train_imagenette.py
@@ -0,0 +1,244 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+#export
+from exp.nb_10c import *
+
+# ## Imagenet(te) training
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=1681)
+
+path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)
+
+# +
+size = 128
+tfms = [make_rgb, RandomResizedCrop(size, scale=(0.35,1)), np_to_float, PilRandomFlip()]
+
+bs = 64
+
+il = ImageList.from_files(path, tfms=tfms)
+sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='val'))
+ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())
+
+ll.valid.x.tfms = [make_rgb, CenterCrop(size), np_to_float]
+
+data = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=8)
+
+
+# -
+
+# ## XResNet
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=1701)
+
+# +
+#export
+def noop(x): return x
+
+class Flatten(nn.Module):
+    def forward(self, x): return x.view(x.size(0), -1)
+
+def conv(ni, nf, ks=3, stride=1, bias=False):
+    return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)
+
+
+# +
+#export
+act_fn = nn.ReLU(inplace=True)
+
+def init_cnn(m):
+    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)
+    if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight)
+    for l in m.children(): init_cnn(l)
+
+def conv_layer(ni, nf, ks=3, stride=1, zero_bn=False, act=True):
+    bn = nn.BatchNorm2d(nf)
+    nn.init.constant_(bn.weight, 0. if zero_bn else 1.)
+    layers = [conv(ni, nf, ks, stride=stride), bn]
+    if act: layers.append(act_fn)
+    return nn.Sequential(*layers)
+
+
+# -
+
+#export
+class ResBlock(nn.Module):
+    def __init__(self, expansion, ni, nh, stride=1):
+        super().__init__()
+        nf,ni = nh*expansion,ni*expansion
+        layers  = [conv_layer(ni, nh, 3, stride=stride),
+                   conv_layer(nh, nf, 3, zero_bn=True, act=False)
+        ] if expansion == 1 else [
+                   conv_layer(ni, nh, 1),
+                   conv_layer(nh, nh, 3, stride=stride),
+                   conv_layer(nh, nf, 1, zero_bn=True, act=False)
+        ]
+        self.convs = nn.Sequential(*layers)
+        self.idconv = noop if ni==nf else conv_layer(ni, nf, 1, act=False)
+        self.pool = noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)
+
+    def forward(self, x): return act_fn(self.convs(x) + self.idconv(self.pool(x)))
+
+
+#export
+class XResNet(nn.Sequential):
+    @classmethod
+    def create(cls, expansion, layers, c_in=3, c_out=1000):
+        nfs = [c_in, (c_in+1)*8, 64, 64]
+        stem = [conv_layer(nfs[i], nfs[i+1], stride=2 if i==0 else 1)
+            for i in range(3)]
+
+        nfs = [64//expansion,64,128,256,512]
+        res_layers = [cls._make_layer(expansion, nfs[i], nfs[i+1],
+                                      n_blocks=l, stride=1 if i==0 else 2)
+                  for i,l in enumerate(layers)]
+        res = cls(
+            *stem,
+            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
+            *res_layers,
+            nn.AdaptiveAvgPool2d(1), Flatten(),
+            nn.Linear(nfs[-1]*expansion, c_out),
+        )
+        init_cnn(res)
+        return res
+
+    @staticmethod
+    def _make_layer(expansion, ni, nf, n_blocks, stride):
+        return nn.Sequential(
+            *[ResBlock(expansion, ni if i==0 else nf, nf, stride if i==0 else 1)
+              for i in range(n_blocks)])
+
+
+#export
+def xresnet18 (**kwargs): return XResNet.create(1, [2, 2,  2, 2], **kwargs)
+def xresnet34 (**kwargs): return XResNet.create(1, [3, 4,  6, 3], **kwargs)
+def xresnet50 (**kwargs): return XResNet.create(4, [3, 4,  6, 3], **kwargs)
+def xresnet101(**kwargs): return XResNet.create(4, [3, 4, 23, 3], **kwargs)
+def xresnet152(**kwargs): return XResNet.create(4, [3, 8, 36, 3], **kwargs)
+
+
+# ## Train
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=2515)
+
+cbfs = [partial(AvgStatsCallback,accuracy), ProgressCallback, CudaCallback,
+        partial(BatchTransformXCallback, norm_imagenette),
+#         partial(MixUp, alpha=0.2)
+       ]
+
+loss_func = LabelSmoothingCrossEntropy()
+arch = partial(xresnet18, c_out=10)
+opt_func = adam_opt(mom=0.9, mom_sqr=0.99, eps=1e-6, wd=1e-2)
+
+
+#export
+def get_batch(dl, learn):
+    learn.xb,learn.yb = next(iter(dl))
+    learn.do_begin_fit(0)
+    learn('begin_batch')
+    learn('after_fit')
+    return learn.xb,learn.yb
+
+
+# We need to replace the old `model_summary` since it used to take a `Runner`.
+
+# export
+def model_summary(model, data, find_all=False, print_mod=False):
+    xb,yb = get_batch(data.valid_dl, learn)
+    mods = find_modules(model, is_lin_layer) if find_all else model.children()
+    f = lambda hook,mod,inp,out: print(f"====\n{mod}\n" if print_mod else "", out.shape)
+    with Hooks(mods, f) as hooks: learn.model(xb)
+
+
+learn = Learner(arch(), data, loss_func, lr=1, cb_funcs=cbfs, opt_func=opt_func)
+
+learn.model = learn.model.cuda()
+model_summary(learn.model, data, print_mod=False)
+
+arch = partial(xresnet34, c_out=10)
+
+learn = Learner(arch(), data, loss_func, lr=1, cb_funcs=cbfs, opt_func=opt_func)
+
+learn.fit(1, cbs=[LR_Find(), Recorder()])
+
+learn.recorder.plot(3)
+
+
+#export
+def create_phases(phases):
+    phases = listify(phases)
+    return phases + [1-sum(phases)]
+
+
+print(create_phases(0.3))
+print(create_phases([0.3,0.2]))
+
+lr = 1e-2
+pct_start = 0.5
+phases = create_phases(pct_start)
+sched_lr  = combine_scheds(phases, cos_1cycle_anneal(lr/10., lr, lr/1e5))
+sched_mom = combine_scheds(phases, cos_1cycle_anneal(0.95, 0.85, 0.95))
+
+cbsched = [
+    ParamScheduler('lr', sched_lr),
+    ParamScheduler('mom', sched_mom)]
+
+learn = Learner(arch(), data, loss_func, lr=lr, cb_funcs=cbfs, opt_func=opt_func)
+
+learn.fit(5, cbs=cbsched)
+
+
+# ## cnn_learner
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=2711)
+
+#export
+def cnn_learner(arch, data, loss_func, opt_func, c_in=None, c_out=None,
+                lr=1e-2, cuda=True, norm=None, progress=True, mixup=0, xtra_cb=None, **kwargs):
+    cbfs = [partial(AvgStatsCallback,accuracy)]+listify(xtra_cb)
+    if progress: cbfs.append(ProgressCallback)
+    if cuda:     cbfs.append(CudaCallback)
+    if norm:     cbfs.append(partial(BatchTransformXCallback, norm))
+    if mixup:    cbfs.append(partial(MixUp, mixup))
+    arch_args = {}
+    if not c_in : c_in  = data.c_in
+    if not c_out: c_out = data.c_out
+    if c_in:  arch_args['c_in' ]=c_in
+    if c_out: arch_args['c_out']=c_out
+    return Learner(arch(**arch_args), data, loss_func, opt_func=opt_func, lr=lr, cb_funcs=cbfs, **kwargs)
+
+
+learn = cnn_learner(xresnet34, data, loss_func, opt_func, norm=norm_imagenette)
+
+learn.fit(5, cbsched)
+
+# ## Imagenet
+
+# You can see all this put together in the fastai [imagenet training script](https://github.com/fastai/fastai/blob/master/examples/train_imagenet.py). It's the same as what we've seen so far, except it also handles multi-GPU training. So how well does this work?
+#
+# We trained for 60 epochs, and got an error of 5.9%, compared to the official PyTorch resnet which gets 7.5% error in 90 epochs! Our xresnet 50 training even surpasses standard resnet 152, which trains for 50% more epochs and has 3x as many layers.
+
+# ## Export
+
+!./notebook2script.py 11_train_imagenette.ipynb
+
+
diff --git a/nbs/dl2/11a_transfer_learning.ipynb b/nbs/dl2/11a_transfer_learning.ipynb
index 3c285fd..0ba9321 100644
--- ./nbs/dl2/11a_transfer_learning.ipynb
+++ ./nbs/dl2/11a_transfer_learning.ipynb
@@ -924,6 +924,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/11a_transfer_learning.py b/nbs/dl2/11a_transfer_learning.py
new file mode 100644
index 0000000..0c23857
--- /dev/null
+++ ./nbs/dl2/11a_transfer_learning.py
@@ -0,0 +1,326 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+#export
+from exp.nb_11 import *
+
+# ## Serializing the model
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=2920)
+
+path = datasets.untar_data(datasets.URLs.IMAGEWOOF_160)
+
+# +
+size = 128
+bs = 64
+
+tfms = [make_rgb, RandomResizedCrop(size, scale=(0.35,1)), np_to_float, PilRandomFlip()]
+val_tfms = [make_rgb, CenterCrop(size), np_to_float]
+il = ImageList.from_files(path, tfms=tfms)
+sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='val'))
+ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())
+ll.valid.x.tfms = val_tfms
+data = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=8)
+# -
+
+len(il)
+
+loss_func = LabelSmoothingCrossEntropy()
+opt_func = adam_opt(mom=0.9, mom_sqr=0.99, eps=1e-6, wd=1e-2)
+
+learn = cnn_learner(xresnet18, data, loss_func, opt_func, norm=norm_imagenette)
+
+
+def sched_1cycle(lr, pct_start=0.3, mom_start=0.95, mom_mid=0.85, mom_end=0.95):
+    phases = create_phases(pct_start)
+    sched_lr  = combine_scheds(phases, cos_1cycle_anneal(lr/10., lr, lr/1e5))
+    sched_mom = combine_scheds(phases, cos_1cycle_anneal(mom_start, mom_mid, mom_end))
+    return [ParamScheduler('lr', sched_lr),
+            ParamScheduler('mom', sched_mom)]
+
+
+lr = 3e-3
+pct_start = 0.5
+cbsched = sched_1cycle(lr, pct_start)
+
+learn.fit(40, cbsched)
+
+st = learn.model.state_dict()
+
+type(st)
+
+', '.join(st.keys())
+
+st['10.bias']
+
+mdl_path = path/'models'
+mdl_path.mkdir(exist_ok=True)
+
+# It's also possible to save the whole model, including the architecture, but it gets quite fiddly and we don't recommend it. Instead, just save the parameters, and recreate the model directly.
+
+torch.save(st, mdl_path/'iw5')
+
+# ## Pets
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=3127)
+
+pets = datasets.untar_data(datasets.URLs.PETS)
+
+pets.ls()
+
+pets_path = pets/'images'
+
+il = ImageList.from_files(pets_path, tfms=tfms)
+
+il
+
+
+#export
+def random_splitter(fn, p_valid): return random.random() < p_valid
+
+
+random.seed(42)
+
+sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1))
+
+sd
+
+n = il.items[0].name; n
+
+re.findall(r'^(.*)_\d+.jpg$', n)[0]
+
+
+def pet_labeler(fn): return re.findall(r'^(.*)_\d+.jpg$', fn.name)[0]
+
+
+proc = CategoryProcessor()
+
+ll = label_by_func(sd, pet_labeler, proc_y=proc)
+
+', '.join(proc.vocab)
+
+ll.valid.x.tfms = val_tfms
+
+c_out = len(proc.vocab)
+
+data = ll.to_databunch(bs, c_in=3, c_out=c_out, num_workers=8)
+
+learn = cnn_learner(xresnet18, data, loss_func, opt_func, norm=norm_imagenette)
+
+learn.fit(5, cbsched)
+
+# ## Custom head
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=3265)
+
+learn = cnn_learner(xresnet18, data, loss_func, opt_func, c_out=10, norm=norm_imagenette)
+
+st = torch.load(mdl_path/'iw5')
+
+m = learn.model
+
+m.load_state_dict(st)
+
+cut = next(i for i,o in enumerate(m.children()) if isinstance(o,nn.AdaptiveAvgPool2d))
+m_cut = m[:cut]
+
+xb,yb = get_batch(data.valid_dl, learn)
+
+pred = m_cut(xb)
+
+pred.shape
+
+ni = pred.shape[1]
+
+
+#export
+class AdaptiveConcatPool2d(nn.Module):
+    def __init__(self, sz=1):
+        super().__init__()
+        self.output_size = sz
+        self.ap = nn.AdaptiveAvgPool2d(sz)
+        self.mp = nn.AdaptiveMaxPool2d(sz)
+    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)
+
+
+# +
+nh = 40
+
+m_new = nn.Sequential(
+    m_cut, AdaptiveConcatPool2d(), Flatten(),
+    nn.Linear(ni*2, data.c_out))
+# -
+
+learn.model = m_new
+
+learn.fit(5, cbsched)
+
+
+# ## adapt_model and gradual unfreezing
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=3483)
+
+def adapt_model(learn, data):
+    cut = next(i for i,o in enumerate(learn.model.children())
+               if isinstance(o,nn.AdaptiveAvgPool2d))
+    m_cut = learn.model[:cut]
+    xb,yb = get_batch(data.valid_dl, learn)
+    pred = m_cut(xb)
+    ni = pred.shape[1]
+    m_new = nn.Sequential(
+        m_cut, AdaptiveConcatPool2d(), Flatten(),
+        nn.Linear(ni*2, data.c_out))
+    learn.model = m_new
+
+
+learn = cnn_learner(xresnet18, data, loss_func, opt_func, c_out=10, norm=norm_imagenette)
+learn.model.load_state_dict(torch.load(mdl_path/'iw5'))
+
+adapt_model(learn, data)
+
+for p in learn.model[0].parameters(): p.requires_grad_(False)
+
+learn.fit(3, sched_1cycle(1e-2, 0.5))
+
+for p in learn.model[0].parameters(): p.requires_grad_(True)
+
+learn.fit(5, cbsched, reset_opt=True)
+
+# ## Batch norm transfer
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=3567)
+
+learn = cnn_learner(xresnet18, data, loss_func, opt_func, c_out=10, norm=norm_imagenette)
+learn.model.load_state_dict(torch.load(mdl_path/'iw5'))
+adapt_model(learn, data)
+
+
+# +
+def apply_mod(m, f):
+    f(m)
+    for l in m.children(): apply_mod(l, f)
+
+def set_grad(m, b):
+    if isinstance(m, (nn.Linear,nn.BatchNorm2d)): return
+    if hasattr(m, 'weight'):
+        for p in m.parameters(): p.requires_grad_(b)
+
+
+# -
+
+apply_mod(learn.model, partial(set_grad, b=False))
+
+learn.fit(3, sched_1cycle(1e-2, 0.5))
+
+apply_mod(learn.model, partial(set_grad, b=True))
+
+learn.fit(5, cbsched, reset_opt=True)
+
+# Pytorch already has an `apply` method we can use:
+
+learn.model.apply(partial(set_grad, b=False));
+
+# ## Discriminative LR and param groups
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=3799)
+
+learn = cnn_learner(xresnet18, data, loss_func, opt_func, c_out=10, norm=norm_imagenette)
+
+learn.model.load_state_dict(torch.load(mdl_path/'iw5'))
+adapt_model(learn, data)
+
+
+def bn_splitter(m):
+    def _bn_splitter(l, g1, g2):
+        if isinstance(l, nn.BatchNorm2d): g2 += l.parameters()
+        elif hasattr(l, 'weight'): g1 += l.parameters()
+        for ll in l.children(): _bn_splitter(ll, g1, g2)
+        
+    g1,g2 = [],[]
+    _bn_splitter(m[0], g1, g2)
+    
+    g2 += m[1:].parameters()
+    return g1,g2
+
+
+a,b = bn_splitter(learn.model)
+
+test_eq(len(a)+len(b), len(list(m.parameters())))
+
+Learner.ALL_CBS
+
+#export
+from types import SimpleNamespace
+cb_types = SimpleNamespace(**{o:o for o in Learner.ALL_CBS})
+
+cb_types.after_backward
+
+
+#export
+class DebugCallback(Callback):
+    _order = 999
+    def __init__(self, cb_name, f=None): self.cb_name,self.f = cb_name,f
+    def __call__(self, cb_name):
+        if cb_name==self.cb_name:
+            if self.f: self.f(self.run)
+            else:      set_trace()
+
+
+#export
+def sched_1cycle(lrs, pct_start=0.3, mom_start=0.95, mom_mid=0.85, mom_end=0.95):
+    phases = create_phases(pct_start)
+    sched_lr  = [combine_scheds(phases, cos_1cycle_anneal(lr/10., lr, lr/1e5))
+                 for lr in lrs]
+    sched_mom = combine_scheds(phases, cos_1cycle_anneal(mom_start, mom_mid, mom_end))
+    return [ParamScheduler('lr', sched_lr),
+            ParamScheduler('mom', sched_mom)]
+
+
+disc_lr_sched = sched_1cycle([0,3e-2], 0.5)
+
+# +
+learn = cnn_learner(xresnet18, data, loss_func, opt_func,
+                    c_out=10, norm=norm_imagenette, splitter=bn_splitter)
+
+learn.model.load_state_dict(torch.load(mdl_path/'iw5'))
+adapt_model(learn, data)
+
+
+# +
+def _print_det(o): 
+    print (len(o.opt.param_groups), o.opt.hypers)
+    raise CancelTrainException()
+
+learn.fit(1, disc_lr_sched + [DebugCallback(cb_types.after_batch, _print_det)])
+# -
+
+learn.fit(3, disc_lr_sched)
+
+disc_lr_sched = sched_1cycle([1e-3,1e-2], 0.3)
+
+learn.fit(5, disc_lr_sched)
+
+# ## Export
+
+!./notebook2script.py 11a_transfer_learning.ipynb
+
+
diff --git a/nbs/dl2/12_text.ipynb b/nbs/dl2/12_text.ipynb
index 08fc7f7..de5c72a 100644
--- ./nbs/dl2/12_text.ipynb
+++ ./nbs/dl2/12_text.ipynb
@@ -1597,6 +1597,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/12_text.py b/nbs/dl2/12_text.py
new file mode 100644
index 0000000..094e462
--- /dev/null
+++ ./nbs/dl2/12_text.py
@@ -0,0 +1,487 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Preprocess text
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+#export
+from exp.nb_11a import *
+
+# ## Data
+
+# We will use the IMDB dataset that consists of 50,000 labeled reviews of movies (positive or negative) and 50,000 unlabelled ones.
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=4964)
+
+path = datasets.untar_data(datasets.URLs.IMDB)
+
+path.ls()
+
+
+# We define a subclass of `ItemList` that will read the texts in the corresponding filenames.
+
+# +
+#export
+def read_file(fn): 
+    with open(fn, 'r', encoding = 'utf8') as f: return f.read()
+    
+class TextList(ItemList):
+    @classmethod
+    def from_files(cls, path, extensions='.txt', recurse=True, include=None, **kwargs):
+        return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs)
+    
+    def get(self, i):
+        if isinstance(i, Path): return read_file(i)
+        return i
+
+
+# -
+
+# Just in case there are some text log files, we restrict the ones we take to the training, test, and unsupervised folders.
+
+il = TextList.from_files(path, include=['train', 'test', 'unsup'])
+
+# We should expect a total of 100,000 texts.
+
+len(il.items)
+
+# Here is the first one as an example.
+
+txt = il[0]
+txt
+
+# For text classification, we will split by the grand parent folder as before, but for language modeling, we take all the texts and just put 10% aside.
+
+sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1))
+
+sd
+
+# ## Tokenizing
+
+# We need to tokenize the dataset first, which is splitting a sentence in individual tokens. Those tokens are the basic words or punctuation signs with a few tweaks: don't for instance is split between do and n't. We will use a processor for this, in conjunction with the [spacy library](https://spacy.io/).
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=5070)
+
+#export
+import spacy,html
+
+# Before even tokenizeing, we will apply a bit of preprocessing on the texts to clean them up (we saw the one up there had some HTML code). These rules are applied before we split the sentences in tokens.
+
+# +
+#export
+#special tokens
+UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ = "xxunk xxpad xxbos xxeos xxrep xxwrep xxup xxmaj".split()
+
+def sub_br(t):
+    "Replaces the <br /> by \n"
+    re_br = re.compile(r'<\s*br\s*/?>', re.IGNORECASE)
+    return re_br.sub("\n", t)
+
+def spec_add_spaces(t):
+    "Add spaces around / and #"
+    return re.sub(r'([/#])', r' \1 ', t)
+
+def rm_useless_spaces(t):
+    "Remove multiple spaces"
+    return re.sub(' {2,}', ' ', t)
+
+def replace_rep(t):
+    "Replace repetitions at the character level: cccc -> TK_REP 4 c"
+    def _replace_rep(m:Collection[str]) -> str:
+        c,cc = m.groups()
+        return f' {TK_REP} {len(cc)+1} {c} '
+    re_rep = re.compile(r'(\S)(\1{3,})')
+    return re_rep.sub(_replace_rep, t)
+    
+def replace_wrep(t):
+    "Replace word repetitions: word word word -> TK_WREP 3 word"
+    def _replace_wrep(m:Collection[str]) -> str:
+        c,cc = m.groups()
+        return f' {TK_WREP} {len(cc.split())+1} {c} '
+    re_wrep = re.compile(r'(\b\w+\W+)(\1{3,})')
+    return re_wrep.sub(_replace_wrep, t)
+
+def fixup_text(x):
+    "Various messy things we've seen in documents"
+    re1 = re.compile(r'  +')
+    x = x.replace('#39;', "'").replace('amp;', '&').replace('#146;', "'").replace(
+        'nbsp;', ' ').replace('#36;', '$').replace('\\n', "\n").replace('quot;', "'").replace(
+        '<br />', "\n").replace('\\"', '"').replace('<unk>',UNK).replace(' @.@ ','.').replace(
+        ' @-@ ','-').replace('\\', ' \\ ')
+    return re1.sub(' ', html.unescape(x))
+    
+default_pre_rules = [fixup_text, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces, sub_br]
+default_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ]
+# -
+
+replace_rep('cccc')
+
+replace_wrep('word word word word word ')
+
+
+# These rules are applies after the tokenization on the list of tokens.
+
+# +
+#export
+def replace_all_caps(x):
+    "Replace tokens in ALL CAPS by their lower version and add `TK_UP` before."
+    res = []
+    for t in x:
+        if t.isupper() and len(t) > 1: res.append(TK_UP); res.append(t.lower())
+        else: res.append(t)
+    return res
+
+def deal_caps(x):
+    "Replace all Capitalized tokens in by their lower version and add `TK_MAJ` before."
+    res = []
+    for t in x:
+        if t == '': continue
+        if t[0].isupper() and len(t) > 1 and t[1:].islower(): res.append(TK_MAJ)
+        res.append(t.lower())
+    return res
+
+def add_eos_bos(x): return [BOS] + x + [EOS]
+
+default_post_rules = [deal_caps, replace_all_caps, add_eos_bos]
+# -
+
+replace_all_caps(['I', 'AM', 'SHOUTING'])
+
+deal_caps(['My', 'name', 'is', 'Jeremy'])
+
+# Since tokenizing and applying those rules takes a bit of time, we'll parallelize it using `ProcessPoolExecutor` to go faster.
+
+# +
+#export
+from spacy.symbols import ORTH
+from concurrent.futures import ProcessPoolExecutor
+
+def parallel(func, arr, max_workers=4):
+    if max_workers<2: results = list(progress_bar(map(func, enumerate(arr)), total=len(arr)))
+    else:
+        with ProcessPoolExecutor(max_workers=max_workers) as ex:
+            return list(progress_bar(ex.map(func, enumerate(arr)), total=len(arr)))
+    if any([o is not None for o in results]): return results
+
+
+# -
+
+#export
+class TokenizeProcessor(Processor):
+    def __init__(self, lang="en", chunksize=2000, pre_rules=None, post_rules=None, max_workers=4): 
+        self.chunksize,self.max_workers = chunksize,max_workers
+        self.tokenizer = spacy.blank(lang).tokenizer
+        for w in default_spec_tok:
+            self.tokenizer.add_special_case(w, [{ORTH: w}])
+        self.pre_rules  = default_pre_rules  if pre_rules  is None else pre_rules
+        self.post_rules = default_post_rules if post_rules is None else post_rules
+
+    def proc_chunk(self, args):
+        i,chunk = args
+        chunk = [compose(t, self.pre_rules) for t in chunk]
+        docs = [[d.text for d in doc] for doc in self.tokenizer.pipe(chunk)]
+        docs = [compose(t, self.post_rules) for t in docs]
+        return docs
+
+    def __call__(self, items): 
+        toks = []
+        if isinstance(items[0], Path): items = [read_file(i) for i in items]
+        chunks = [items[i: i+self.chunksize] for i in (range(0, len(items), self.chunksize))]
+        toks = parallel(self.proc_chunk, chunks, max_workers=self.max_workers)
+        return sum(toks, [])
+    
+    def proc1(self, item): return self.proc_chunk([item])[0]
+    
+    def deprocess(self, toks): return [self.deproc1(tok) for tok in toks]
+    def deproc1(self, tok):    return " ".join(tok)
+
+
+tp = TokenizeProcessor()
+
+txt[:250]
+
+' • '.join(tp(il[:100])[0])[:400]
+
+# ## Numericalizing
+
+# Once we have tokenized our texts, we replace each token by an individual number, this is called numericalizing. Again, we do this with a processor (not so different from the `CategoryProcessor`).
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=5491)
+
+# +
+#export
+import collections
+
+class NumericalizeProcessor(Processor):
+    def __init__(self, vocab=None, max_vocab=60000, min_freq=2): 
+        self.vocab,self.max_vocab,self.min_freq = vocab,max_vocab,min_freq
+    
+    def __call__(self, items):
+        #The vocab is defined on the first use.
+        if self.vocab is None:
+            freq = Counter(p for o in items for p in o)
+            self.vocab = [o for o,c in freq.most_common(self.max_vocab) if c >= self.min_freq]
+            for o in reversed(default_spec_tok):
+                if o in self.vocab: self.vocab.remove(o)
+                self.vocab.insert(0, o)
+        if getattr(self, 'otoi', None) is None:
+            self.otoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.vocab)}) 
+        return [self.proc1(o) for o in items]
+    def proc1(self, item):  return [self.otoi[o] for o in item]
+    
+    def deprocess(self, idxs):
+        assert self.vocab is not None
+        return [self.deproc1(idx) for idx in idxs]
+    def deproc1(self, idx): return [self.vocab[i] for i in idx]
+
+
+# -
+
+# When we do language modeling, we will infer the labels from the text during training, so there's no need to label. The training loop expects labels however, so we need to add dummy ones.
+
+proc_tok,proc_num = TokenizeProcessor(max_workers=8),NumericalizeProcessor()
+
+# %time ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok,proc_num])
+
+# Once the items have been processed they will become list of numbers, we can still access the underlying raw data in `x_obj` (or `y_obj` for the targets, but we don't have any here).
+
+ll.train.x_obj(0)
+
+# Since the preprocessing tajes time, we save the intermediate result using pickle. Don't use any lambda functions in your processors or they won't be able to pickle.
+
+pickle.dump(ll, open(path/'ld.pkl', 'wb'))
+
+ll = pickle.load(open(path/'ld.pkl', 'rb'))
+
+# ## Batching
+
+# We have a bit of work to convert our `LabelList` in a `DataBunch` as we don't just want batches of IMDB reviews. We want to stream through all the texts concatenated. We also have to prepare the targets that are the newt words in the text. All of this is done with the next object called `LM_PreLoader`. At the beginning of each epoch, it'll shuffle the articles (if `shuffle=True`) and create a big stream by concatenating all of them. We divide this big stream in `bs` smaller streams. That we will read in chunks of bptt length.
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=5565)
+
+# Just using those for illustration purposes, they're not used otherwise.
+from IPython.display import display,HTML
+import pandas as pd
+
+# Let's say our stream is:
+
+stream = """
+In this notebook, we will go back over the example of classifying movie reviews we studied in part 1 and dig deeper under the surface. 
+First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the Processor used in the data block API.
+Then we will study how we build a language model and train it.\n
+"""
+tokens = np.array(tp([stream])[0])
+
+# Then if we split it in 6 batches it would give something like this:
+
+bs,seq_len = 6,15
+d_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])
+df = pd.DataFrame(d_tokens)
+display(HTML(df.to_html(index=False,header=None)))
+
+# Then if we have a `bptt` of 5, we would go over those three batches.
+
+bs,bptt = 6,5
+for k in range(3):
+    d_tokens = np.array([tokens[i*seq_len + k*bptt:i*seq_len + (k+1)*bptt] for i in range(bs)])
+    df = pd.DataFrame(d_tokens)
+    display(HTML(df.to_html(index=False,header=None)))
+
+
+#export
+class LM_PreLoader():
+    def __init__(self, data, bs=64, bptt=70, shuffle=False):
+        self.data,self.bs,self.bptt,self.shuffle = data,bs,bptt,shuffle
+        total_len = sum([len(t) for t in data.x])
+        self.n_batch = total_len // bs
+        self.batchify()
+    
+    def __len__(self): return ((self.n_batch-1) // self.bptt) * self.bs
+    
+    def __getitem__(self, idx):
+        source = self.batched_data[idx % self.bs]
+        seq_idx = (idx // self.bs) * self.bptt
+        return source[seq_idx:seq_idx+self.bptt],source[seq_idx+1:seq_idx+self.bptt+1]
+    
+    def batchify(self):
+        texts = self.data.x
+        if self.shuffle: texts = texts[torch.randperm(len(texts))]
+        stream = torch.cat([tensor(t) for t in texts])
+        self.batched_data = stream[:self.n_batch * self.bs].view(self.bs, self.n_batch)
+
+
+dl = DataLoader(LM_PreLoader(ll.valid, shuffle=True), batch_size=64)
+
+# Let's check it all works ok: `x1`, `y1`, `x2` and `y2` should all be of size `bs`  by `bptt`. The texts in each row of `x1` should continue in `x2`. `y1` and `y2` should have the same texts as their `x` counterpart, shifted of one position to the right.
+
+iter_dl = iter(dl)
+x1,y1 = next(iter_dl)
+x2,y2 = next(iter_dl)
+
+x1.size(),y1.size()
+
+vocab = proc_num.vocab
+
+" ".join(vocab[o] for o in x1[0])
+
+" ".join(vocab[o] for o in y1[0])
+
+" ".join(vocab[o] for o in x2[0])
+
+
+# And let's prepare some convenience function to do this quickly.
+
+# +
+#export
+def get_lm_dls(train_ds, valid_ds, bs, bptt, **kwargs):
+    return (DataLoader(LM_PreLoader(train_ds, bs, bptt, shuffle=True), batch_size=bs, **kwargs),
+            DataLoader(LM_PreLoader(valid_ds, bs, bptt, shuffle=False), batch_size=2*bs, **kwargs))
+
+def lm_databunchify(sd, bs, bptt, **kwargs):
+    return DataBunch(*get_lm_dls(sd.train, sd.valid, bs, bptt, **kwargs))
+
+
+# -
+
+bs,bptt = 64,70
+data = lm_databunchify(ll, bs, bptt)
+
+# ## Batching for classification
+
+# When we will want to tackle classification, gathering the data will be a bit different: first we will label our texts with the folder they come from, and then we will need to apply padding to batch them together. To avoid mixing very long texts with very short ones, we will also use `Sampler` to sort (with a bit of randomness for the training set) our samples by length.
+#
+# First the data block API calls shold look familiar.
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=5877)
+
+proc_cat = CategoryProcessor()
+
+il = TextList.from_files(path, include=['train', 'test'])
+sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='test'))
+ll = label_by_func(sd, parent_labeler, proc_x = [proc_tok, proc_num], proc_y=proc_cat)
+
+pickle.dump(ll, open(path/'ll_clas.pkl', 'wb'))
+
+ll = pickle.load(open(path/'ll_clas.pkl', 'rb'))
+
+# Let's check the labels seem consistent with the texts.
+
+[(ll.train.x_obj(i), ll.train.y_obj(i)) for i in [1,12552]]
+
+# We saw samplers in notebook 03. For the validation set, we will simply sort the samples by length, and we begin with the longest ones for memory reasons (it's better to always have the biggest tensors first).
+
+# +
+#export
+from torch.utils.data import Sampler
+
+class SortSampler(Sampler):
+    def __init__(self, data_source, key): self.data_source,self.key = data_source,key
+    def __len__(self): return len(self.data_source)
+    def __iter__(self):
+        return iter(sorted(list(range(len(self.data_source))), key=self.key, reverse=True))
+
+
+# -
+
+# For the training set, we want some kind of randomness on top of this. So first, we shuffle the texts and build megabatches of size `50 * bs`. We sort those megabatches by length before splitting them in 50 minibatches. That way we will have randomized batches of roughly the same length.
+#
+# Then we make sure to have the biggest batch first and shuffle the order of the other batches. We also make sure the last batch stays at the end because its size is probably lower than batch size.
+
+#export
+class SortishSampler(Sampler):
+    def __init__(self, data_source, key, bs):
+        self.data_source,self.key,self.bs = data_source,key,bs
+
+    def __len__(self) -> int: return len(self.data_source)
+
+    def __iter__(self):
+        idxs = torch.randperm(len(self.data_source))
+        megabatches = [idxs[i:i+self.bs*50] for i in range(0, len(idxs), self.bs*50)]
+        sorted_idx = torch.cat([tensor(sorted(s, key=self.key, reverse=True)) for s in megabatches])
+        batches = [sorted_idx[i:i+self.bs] for i in range(0, len(sorted_idx), self.bs)]
+        max_idx = torch.argmax(tensor([self.key(ck[0]) for ck in batches]))  # find the chunk with the largest key,
+        batches[0],batches[max_idx] = batches[max_idx],batches[0]            # then make sure it goes first.
+        batch_idxs = torch.randperm(len(batches)-2)
+        sorted_idx = torch.cat([batches[i+1] for i in batch_idxs]) if len(batches) > 1 else LongTensor([])
+        sorted_idx = torch.cat([batches[0], sorted_idx, batches[-1]])
+        return iter(sorted_idx)
+
+
+# Padding: we had the padding token (that as an id of 1) at the end of each sequence to make them all the same size when batching them. Note that we need padding at the end to be able to use `PyTorch` convenience functions that will let us ignore that padding (see 12c).
+
+#export
+def pad_collate(samples, pad_idx=1, pad_first=False):
+    max_len = max([len(s[0]) for s in samples])
+    res = torch.zeros(len(samples), max_len).long() + pad_idx
+    for i,s in enumerate(samples):
+        if pad_first: res[i, -len(s[0]):] = LongTensor(s[0])
+        else:         res[i, :len(s[0]) ] = LongTensor(s[0])
+    return res, tensor([s[1] for s in samples])
+
+
+bs = 64
+train_sampler = SortishSampler(ll.train.x, key=lambda t: len(ll.train[int(t)][0]), bs=bs)
+train_dl = DataLoader(ll.train, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate)
+
+iter_dl = iter(train_dl)
+x,y = next(iter_dl)
+
+lengths = []
+for i in range(x.size(0)): lengths.append(x.size(1) - (x[i]==1).sum().item())
+lengths[:5], lengths[-1]
+
+# The last one is the minimal length. This is the first batch so it has the longest sequence, but if look at the next one that is more random, we see lengths are roughly the sames.
+
+x,y = next(iter_dl)
+lengths = []
+for i in range(x.size(0)): lengths.append(x.size(1) - (x[i]==1).sum().item())
+lengths[:5], lengths[-1]
+
+# We can see the padding at the end:
+
+x
+
+
+# And we add a convenience function:
+
+# +
+#export
+def get_clas_dls(train_ds, valid_ds, bs, **kwargs):
+    train_sampler = SortishSampler(train_ds.x, key=lambda t: len(train_ds.x[t]), bs=bs)
+    valid_sampler = SortSampler(valid_ds.x, key=lambda t: len(valid_ds.x[t]))
+    return (DataLoader(train_ds, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate, **kwargs),
+            DataLoader(valid_ds, batch_size=bs*2, sampler=valid_sampler, collate_fn=pad_collate, **kwargs))
+
+def clas_databunchify(sd, bs, **kwargs):
+    return DataBunch(*get_clas_dls(sd.train, sd.valid, bs, **kwargs))
+
+
+# -
+
+bs,bptt = 64,70
+data = clas_databunchify(ll, bs)
+
+# ## Export
+
+# !python notebook2script.py 12_text.ipynb
+
+
diff --git a/nbs/dl2/12a_awd_lstm.ipynb b/nbs/dl2/12a_awd_lstm.ipynb
index 319b26a..b28b9a5 100644
--- ./nbs/dl2/12a_awd_lstm.ipynb
+++ ./nbs/dl2/12a_awd_lstm.ipynb
@@ -1185,6 +1185,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/12a_awd_lstm.py b/nbs/dl2/12a_awd_lstm.py
new file mode 100644
index 0000000..8c005bd
--- /dev/null
+++ ./nbs/dl2/12a_awd_lstm.py
@@ -0,0 +1,538 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # AWD-LSTM
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+#export
+from exp.nb_12 import *
+
+# ## Data
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=6317)
+
+path = datasets.untar_data(datasets.URLs.IMDB)
+
+# We have to preprocess the data again to pickle it because if we try to load the previous `SplitLabeledData` with pickle, it will complain some of the functions aren't in main.
+
+il = TextList.from_files(path, include=['train', 'test', 'unsup'])
+sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1))
+
+proc_tok,proc_num = TokenizeProcessor(max_workers=8),NumericalizeProcessor()
+
+ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok,proc_num])
+
+pickle.dump(ll, open(path/'ll_lm.pkl', 'wb'))
+pickle.dump(proc_num.vocab, open(path/'vocab_lm.pkl', 'wb'))
+
+ll = pickle.load(open(path/'ll_lm.pkl', 'rb'))
+vocab = pickle.load(open(path/'vocab_lm.pkl', 'rb'))
+
+bs,bptt = 64,70
+data = lm_databunchify(ll, bs, bptt)
+
+
+# ## AWD-LSTM
+
+# Before explaining what an AWD LSTM is, we need to start with an LSTM. RNNs were covered in part 1, if you need a refresher, there is a great visualization of them on [this website](http://joshvarty.github.io/VisualizingRNNs/).
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=6330)
+
+# ### LSTM from scratch
+
+# We need to implement those equations (where $\sigma$ stands for sigmoid):
+#
+# ![LSTM cell and equations](images/lstm.jpg)
+# (picture from [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah.)
+#
+# If we want to take advantage of our GPU, it's better to do one big matrix multiplication than four smaller ones. So we compute the values of the four gates all at once. Since there is a matrix multiplication and a bias, we use `nn.Linear` to do it.
+#
+# We need two linear layers: one for the input and one for the hidden state.
+
+class LSTMCell(nn.Module):
+    def __init__(self, ni, nh):
+        super().__init__()
+        self.ih = nn.Linear(ni,4*nh)
+        self.hh = nn.Linear(nh,4*nh)
+
+    def forward(self, input, state):
+        h,c = state
+        #One big multiplication for all the gates is better than 4 smaller ones
+        gates = (self.ih(input) + self.hh(h)).chunk(4, 1)
+        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])
+        cellgate = gates[3].tanh()
+
+        c = (forgetgate*c) + (ingate*cellgate)
+        h = outgate * c.tanh()
+        return h, (h,c)
+
+
+# Then an LSTM layer just applies the cell on all the time steps in order.
+
+class LSTMLayer(nn.Module):
+    def __init__(self, cell, *cell_args):
+        super().__init__()
+        self.cell = cell(*cell_args)
+
+    def forward(self, input, state):
+        inputs = input.unbind(1)
+        outputs = []
+        for i in range(len(inputs)):
+            out, state = self.cell(inputs[i], state)
+            outputs += [out]
+        return torch.stack(outputs, dim=1), state
+
+
+# Now let's try it out and see how fast we are. We only measure the forward pass.
+
+lstm = LSTMLayer(LSTMCell, 300, 300)
+
+x = torch.randn(64, 70, 300)
+h = (torch.zeros(64, 300),torch.zeros(64, 300))
+
+# CPU
+
+# %timeit -n 10 y,h1 = lstm(x,h)
+
+lstm = lstm.cuda()
+x = x.cuda()
+h = (h[0].cuda(), h[1].cuda())
+
+
+def time_fn(f):
+    f()
+    torch.cuda.synchronize()
+
+
+# CUDA
+
+f = partial(lstm,x,h)
+time_fn(f)
+
+# %timeit -n 10 time_fn(f)
+
+# ### Builtin version
+
+# Let's compare with PyTorch!
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=6718)
+
+lstm = nn.LSTM(300, 300, 1, batch_first=True)
+
+x = torch.randn(64, 70, 300)
+h = (torch.zeros(1, 64, 300),torch.zeros(1, 64, 300))
+
+# CPU
+
+# %timeit -n 10 y,h1 = lstm(x,h)
+
+lstm = lstm.cuda()
+x = x.cuda()
+h = (h[0].cuda(), h[1].cuda())
+
+f = partial(lstm,x,h)
+time_fn(f)
+
+# GPU
+
+# %timeit -n 10 time_fn(f)
+
+# So our version is running at almost the same speed on the CPU. However, on the GPU, PyTorch uses CuDNN behind the scenes that optimizes greatly the for loop.
+
+# ### Jit version
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=6744)
+
+import torch.jit as jit
+from torch import Tensor
+
+
+# We have to write everything from scratch to be a bit faster, so we don't use the linear layers here.
+
+class LSTMCell(jit.ScriptModule):
+    def __init__(self, ni, nh):
+        super().__init__()
+        self.ni = ni
+        self.nh = nh
+        self.w_ih = nn.Parameter(torch.randn(4 * nh, ni))
+        self.w_hh = nn.Parameter(torch.randn(4 * nh, nh))
+        self.bias_ih = nn.Parameter(torch.randn(4 * nh))
+        self.bias_hh = nn.Parameter(torch.randn(4 * nh))
+
+    @jit.script_method
+    def forward(self, input:Tensor, state:Tuple[Tensor, Tensor])->Tuple[Tensor, Tuple[Tensor, Tensor]]:
+        hx, cx = state
+        gates = (input @ self.w_ih.t() + self.bias_ih +
+                 hx @ self.w_hh.t() + self.bias_hh)
+        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)
+
+        ingate = torch.sigmoid(ingate)
+        forgetgate = torch.sigmoid(forgetgate)
+        cellgate = torch.tanh(cellgate)
+        outgate = torch.sigmoid(outgate)
+
+        cy = (forgetgate * cx) + (ingate * cellgate)
+        hy = outgate * torch.tanh(cy)
+
+        return hy, (hy, cy)
+
+
+class LSTMLayer(jit.ScriptModule):
+    def __init__(self, cell, *cell_args):
+        super().__init__()
+        self.cell = cell(*cell_args)
+
+    @jit.script_method
+    def forward(self, input:Tensor, state:Tuple[Tensor, Tensor])->Tuple[Tensor, Tuple[Tensor, Tensor]]:
+        inputs = input.unbind(1)
+        outputs = []
+        for i in range(len(inputs)):
+            out, state = self.cell(inputs[i], state)
+            outputs += [out]
+        return torch.stack(outputs, dim=1), state
+
+
+lstm = LSTMLayer(LSTMCell, 300, 300)
+
+x = torch.randn(64, 70, 300)
+h = (torch.zeros(64, 300),torch.zeros(64, 300))
+
+# %timeit -n 10 y,h1 = lstm(x,h)
+
+lstm = lstm.cuda()
+x = x.cuda()
+h = (h[0].cuda(), h[1].cuda())
+
+f = partial(lstm,x,h)
+time_fn(f)
+
+
+# %timeit -n 10 time_fn(f)
+
+# With jit, we almost get to the CuDNN speed!
+
+# ### Dropout
+
+# We want to use the AWD-LSTM from [Stephen Merity et al.](https://arxiv.org/abs/1708.02182). First, we'll need all different kinds of dropouts. Dropout consists into replacing some coefficients by 0 with probability p. To ensure that the average of the weights remains constant, we apply a correction to the weights that aren't nullified of a factor `1/(1-p)` (think of what happens to the activations if you want to figure out why!)
+#
+# We usually apply dropout by drawing a mask that tells us which elements to nullify or not:
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=7003)
+
+#export
+def dropout_mask(x, sz, p):
+    return x.new(*sz).bernoulli_(1-p).div_(1-p)
+
+
+x = torch.randn(10,10)
+mask = dropout_mask(x, (10,10), 0.5); mask
+
+# Once with have a dropout mask `mask`, applying the dropout to `x` is simply done by `x = x * mask`. We create our own dropout mask and don't rely on pytorch dropout because we do not want to nullify all the coefficients randomly: on the sequence dimension, we will want to have always replace the same positions by zero along the seq_len dimension.
+
+(x*mask).std(),x.std()
+
+
+# Inside a RNN, a tensor x will have three dimensions: bs, seq_len, vocab_size.  Recall that we want to consistently apply the dropout mask across the seq_len dimension, therefore, we create a dropout mask for the first and third dimension and broadcast it to the seq_len dimension.
+
+#export
+class RNNDropout(nn.Module):
+    def __init__(self, p=0.5):
+        super().__init__()
+        self.p=p
+
+    def forward(self, x):
+        if not self.training or self.p == 0.: return x
+        m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)
+        return x * m
+
+
+dp = RNNDropout(0.3)
+tst_input = torch.randn(3,3,7)
+tst_input, dp(tst_input)
+
+# WeightDropout is dropout applied to the weights of the inner LSTM hidden to hidden matrix. This is a little hacky if we want to preserve the CuDNN speed and not reimplement the cell from scratch. We add a parameter that will contain the raw weights, and we replace the weight matrix in the LSTM at the beginning of the forward pass.
+
+# +
+#export
+import warnings
+
+WEIGHT_HH = 'weight_hh_l0'
+
+class WeightDropout(nn.Module):
+    def __init__(self, module, weight_p=[0.], layer_names=[WEIGHT_HH]):
+        super().__init__()
+        self.module,self.weight_p,self.layer_names = module,weight_p,layer_names
+        for layer in self.layer_names:
+            #Makes a copy of the weights of the selected layers.
+            w = getattr(self.module, layer)
+            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))
+            self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training=False)
+
+    def _setweights(self):
+        for layer in self.layer_names:
+            raw_w = getattr(self, f'{layer}_raw')
+            self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training)
+
+    def forward(self, *args):
+        self._setweights()
+        with warnings.catch_warnings():
+            #To avoid the warning that comes because the weights aren't flattened.
+            warnings.simplefilter("ignore")
+            return self.module.forward(*args)
+
+
+# -
+
+# Let's try it!
+
+module = nn.LSTM(5, 2)
+dp_module = WeightDropout(module, 0.4)
+getattr(dp_module.module, WEIGHT_HH)
+
+# It's at the beginning of a forward pass that the dropout is applied to the weights.
+
+tst_input = torch.randn(4,20,5)
+h = (torch.zeros(1,20,2), torch.zeros(1,20,2))
+x,h = dp_module(tst_input,h)
+getattr(dp_module.module, WEIGHT_HH)
+
+
+# EmbeddingDropout applies dropout to full rows of the embedding matrix.
+
+#export
+class EmbeddingDropout(nn.Module):
+    "Applies dropout in the embedding layer by zeroing out some elements of the embedding vector."
+    def __init__(self, emb, embed_p):
+        super().__init__()
+        self.emb,self.embed_p = emb,embed_p
+        self.pad_idx = self.emb.padding_idx
+        if self.pad_idx is None: self.pad_idx = -1
+
+    def forward(self, words, scale=None):
+        if self.training and self.embed_p != 0:
+            size = (self.emb.weight.size(0),1)
+            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)
+            masked_embed = self.emb.weight * mask
+        else: masked_embed = self.emb.weight
+        if scale: masked_embed.mul_(scale)
+        return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,
+                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)
+
+
+enc = nn.Embedding(100, 7, padding_idx=1)
+enc_dp = EmbeddingDropout(enc, 0.5)
+tst_input = torch.randint(0,100,(8,))
+enc_dp(tst_input)
+
+
+# ### Main model
+
+# The main model is a regular LSTM with several layers, but using all those kinds of dropouts.
+
+#export
+def to_detach(h):
+    "Detaches `h` from its history."
+    return h.detach() if type(h) == torch.Tensor else tuple(to_detach(v) for v in h)
+
+
+#export
+class AWD_LSTM(nn.Module):
+    "AWD-LSTM inspired by https://arxiv.org/abs/1708.02182."
+    initrange=0.1
+
+    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,
+                 hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):
+        super().__init__()
+        self.bs,self.emb_sz,self.n_hid,self.n_layers = 1,emb_sz,n_hid,n_layers
+        self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)
+        self.emb_dp = EmbeddingDropout(self.emb, embed_p)
+        self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz), 1,
+                             batch_first=True) for l in range(n_layers)]
+        self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns])
+        self.emb.weight.data.uniform_(-self.initrange, self.initrange)
+        self.input_dp = RNNDropout(input_p)
+        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])
+
+    def forward(self, input):
+        bs,sl = input.size()
+        if bs!=self.bs:
+            self.bs=bs
+            self.reset()
+        raw_output = self.input_dp(self.emb_dp(input))
+        new_hidden,raw_outputs,outputs = [],[],[]
+        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):
+            raw_output, new_h = rnn(raw_output, self.hidden[l])
+            new_hidden.append(new_h)
+            raw_outputs.append(raw_output)
+            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)
+            outputs.append(raw_output) 
+        self.hidden = to_detach(new_hidden)
+        return raw_outputs, outputs
+
+    def _one_hidden(self, l):
+        "Return one hidden state."
+        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz
+        return next(self.parameters()).new(1, self.bs, nh).zero_()
+
+    def reset(self):
+        "Reset the hidden states."
+        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]
+
+
+# On top of this, we will apply a linear decoder. It's often best to use the same matrix as the one for the embeddings in the weights of the decoder.
+
+#export
+class LinearDecoder(nn.Module):
+    def __init__(self, n_out, n_hid, output_p, tie_encoder=None, bias=True):
+        super().__init__()
+        self.output_dp = RNNDropout(output_p)
+        self.decoder = nn.Linear(n_hid, n_out, bias=bias)
+        if bias: self.decoder.bias.data.zero_()
+        if tie_encoder: self.decoder.weight = tie_encoder.weight
+        else: init.kaiming_uniform_(self.decoder.weight)
+
+    def forward(self, input):
+        raw_outputs, outputs = input
+        output = self.output_dp(outputs[-1]).contiguous()
+        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))
+        return decoded, raw_outputs, outputs
+
+
+#export
+class SequentialRNN(nn.Sequential):
+    "A sequential module that passes the reset call to its children."
+    def reset(self):
+        for c in self.children():
+            if hasattr(c, 'reset'): c.reset()
+
+
+# And now we stack all of this together!
+
+#export
+def get_language_model(vocab_sz, emb_sz, n_hid, n_layers, pad_token, output_p=0.4, hidden_p=0.2, input_p=0.6, 
+                       embed_p=0.1, weight_p=0.5, tie_weights=True, bias=True):
+    rnn_enc = AWD_LSTM(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,
+                       hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)
+    enc = rnn_enc.emb if tie_weights else None
+    return SequentialRNN(rnn_enc, LinearDecoder(vocab_sz, emb_sz, output_p, tie_encoder=enc, bias=bias))
+
+
+tok_pad = vocab.index(PAD)
+
+# Now we can test this all works without throwing a bug.
+
+tst_model = get_language_model(len(vocab), 300, 300, 2, tok_pad)
+tst_model = tst_model.cuda()
+
+x,y = next(iter(data.train_dl))
+
+z = tst_model(x.cuda())
+
+# We return three things to help with regularization: the true output (probabilities for each word), but also the activations of the encoder, with or without dropouts.
+
+len(z)
+
+decoded, raw_outputs, outputs = z
+
+# The decoded tensor is flattened to `bs * seq_len` by `len(vocab)`:
+
+decoded.size()
+
+# `raw_outputs` and `outputs` each contain the results of the intermediary layers:
+
+len(raw_outputs),len(outputs)
+
+[o.size() for o in raw_outputs], [o.size() for o in outputs]
+
+
+# ### Callbacks to train the model
+
+# We need to add a few tweaks to train a language model: first we will clip the gradients. This is a classic technique that will allow us to use a higher learning rate by putting a maximum value on the norm of the gradients.
+
+#export
+class GradientClipping(Callback):
+    def __init__(self, clip=None): self.clip = clip
+    def after_backward(self):
+        if self.clip:  nn.utils.clip_grad_norm_(self.run.model.parameters(), self.clip)
+
+
+# Then we add an `RNNTrainer` that will do four things:
+# - change the output to make it contain only the `decoded` tensor (for the loss function) and store the `raw_outputs` and `outputs`
+# - apply Activation Regularization (AR): we add to the loss an L2 penalty on the last activations of the AWD LSTM (with dropout applied)
+# - apply Temporal Activation Regularization (TAR): we add to the loss an L2 penalty on the difference between two consecutive (in terms of words) raw outputs
+# - trigger the shuffle of the LMDataset at the beginning of each epoch
+
+#export
+class RNNTrainer(Callback):
+    def __init__(self, α, β): self.α,self.β = α,β
+    
+    def after_pred(self):
+        #Save the extra outputs for later and only returns the true output.
+        self.raw_out,self.out = self.pred[1],self.pred[2]
+        self.run.pred = self.pred[0]
+    
+    def after_loss(self):
+        #AR and TAR
+        if self.α != 0.:  self.run.loss += self.α * self.out[-1].float().pow(2).mean()
+        if self.β != 0.:
+            h = self.raw_out[-1]
+            if h.size(1)>1: self.run.loss += self.β * (h[:,1:] - h[:,:-1]).float().pow(2).mean()
+                
+    def begin_epoch(self):
+        #Shuffle the texts at the beginning of the epoch
+        if hasattr(self.dl.dataset, "batchify"): self.dl.dataset.batchify()
+
+
+# Lastly we write a flattened version of the cross entropy loss and the accuracy metric.
+
+# +
+#export
+def cross_entropy_flat(input, target):
+    bs,sl = target.size()
+    return F.cross_entropy(input.view(bs * sl, -1), target.view(bs * sl))
+
+def accuracy_flat(input, target):
+    bs,sl = target.size()
+    return accuracy(input.view(bs * sl, -1), target.view(bs * sl))
+
+
+# -
+
+emb_sz, nh, nl = 300, 300, 2
+model = get_language_model(len(vocab), emb_sz, nh, nl, tok_pad, input_p=0.6, output_p=0.4, weight_p=0.5, 
+                           embed_p=0.1, hidden_p=0.2)
+
+cbs = [partial(AvgStatsCallback,accuracy_flat),
+       CudaCallback, Recorder,
+       partial(GradientClipping, clip=0.1),
+       partial(RNNTrainer, α=2., β=1.),
+       ProgressCallback]
+
+learn = Learner(model, data, cross_entropy_flat, lr=5e-3, cb_funcs=cbs, opt_func=adam_opt())
+
+learn.fit(1)
+
+# ## Export
+
+# !python notebook2script.py 12a_awd_lstm.ipynb
+
+
diff --git a/nbs/dl2/12b_lm_pretrain.ipynb b/nbs/dl2/12b_lm_pretrain.ipynb
index 6a959fc..a99fd64 100644
--- ./nbs/dl2/12b_lm_pretrain.ipynb
+++ ./nbs/dl2/12b_lm_pretrain.ipynb
@@ -294,6 +294,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/12b_lm_pretrain.py b/nbs/dl2/12b_lm_pretrain.py
new file mode 100644
index 0000000..7b58c4c
--- /dev/null
+++ ./nbs/dl2/12b_lm_pretrain.py
@@ -0,0 +1,119 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Pretraining on WT103
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+#export
+from exp.nb_12a import *
+
+# ## Data
+
+# One time download
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=7410)
+
+# +
+#path = datasets.Config().data_path()
+#version = '103' #2
+
+# +
+# #! wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-{version}-v1.zip -P {path}
+# #! unzip -q -n {path}/wikitext-{version}-v1.zip  -d {path}
+# #! mv {path}/wikitext-{version}/wiki.train.tokens {path}/wikitext-{version}/train.txt
+# #! mv {path}/wikitext-{version}/wiki.valid.tokens {path}/wikitext-{version}/valid.txt
+# #! mv {path}/wikitext-{version}/wiki.test.tokens {path}/wikitext-{version}/test.txt
+# -
+
+# Split the articles: WT103 is given as one big text file and we need to chunk it in different articles if we want to be able to shuffle them at the beginning of each epoch.
+
+path = datasets.Config().data_path()/'wikitext-103'
+
+
+def istitle(line):
+    return len(re.findall(r'^ = [^=]* = $', line)) != 0
+
+
+def read_wiki(filename):
+    articles = []
+    with open(filename, encoding='utf8') as f:
+        lines = f.readlines()
+    current_article = ''
+    for i,line in enumerate(lines):
+        current_article += line
+        if i < len(lines)-2 and lines[i+1] == ' \n' and istitle(lines[i+2]):
+            current_article = current_article.replace('<unk>', UNK)
+            articles.append(current_article)
+            current_article = ''
+    current_article = current_article.replace('<unk>', UNK)
+    articles.append(current_article)
+    return articles
+
+
+train = TextList(read_wiki(path/'train.txt'), path=path) #+read_file(path/'test.txt')
+valid = TextList(read_wiki(path/'valid.txt'), path=path)
+
+len(train), len(valid)
+
+sd = SplitData(train, valid)
+
+proc_tok,proc_num = TokenizeProcessor(),NumericalizeProcessor()
+
+ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok,proc_num])
+
+pickle.dump(ll, open(path/'ld.pkl', 'wb'))
+
+ll = pickle.load( open(path/'ld.pkl', 'rb'))
+
+bs,bptt = 128,70
+data = lm_databunchify(ll, bs, bptt)
+
+vocab = ll.train.proc_x[-1].vocab
+len(vocab)
+
+# ## Model
+
+dps = np.array([0.1, 0.15, 0.25, 0.02, 0.2]) * 0.2
+tok_pad = vocab.index(PAD)
+
+emb_sz, nh, nl = 300, 300, 2
+model = get_language_model(len(vocab), emb_sz, nh, nl, tok_pad, *dps)
+
+cbs = [partial(AvgStatsCallback,accuracy_flat),
+       CudaCallback, Recorder,
+       partial(GradientClipping, clip=0.1),
+       partial(RNNTrainer, α=2., β=1.),
+       ProgressCallback]
+
+learn = Learner(model, data, cross_entropy_flat, lr=5e-3, cb_funcs=cbs, opt_func=adam_opt())
+
+lr = 5e-3
+sched_lr  = combine_scheds([0.3,0.7], cos_1cycle_anneal(lr/10., lr, lr/1e5))
+sched_mom = combine_scheds([0.3,0.7], cos_1cycle_anneal(0.8, 0.7, 0.8))
+cbsched = [ParamScheduler('lr', sched_lr), ParamScheduler('mom', sched_mom)]
+
+learn.fit(10, cbs=cbsched)
+
+torch.save(learn.model.state_dict(), path/'pretrained.pth')
+pickle.dump(vocab, open(path/'vocab.pkl', 'wb'))
+
+
diff --git a/nbs/dl2/12c_ulmfit.ipynb b/nbs/dl2/12c_ulmfit.ipynb
index 40731eb..a926629 100644
--- ./nbs/dl2/12c_ulmfit.ipynb
+++ ./nbs/dl2/12c_ulmfit.ipynb
@@ -1243,6 +1243,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/12c_ulmfit.py b/nbs/dl2/12c_ulmfit.py
new file mode 100644
index 0000000..d5a677e
--- /dev/null
+++ ./nbs/dl2/12c_ulmfit.py
@@ -0,0 +1,466 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # ULMFit
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+#export
+from exp.nb_12a import *
+
+# ## Data
+
+# We load the data from 12a, instructions to create that file are there if you don't have it yet so go ahead and see.
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=7459)
+
+path = datasets.untar_data(datasets.URLs.IMDB)
+
+ll = pickle.load(open(path/'ll_lm.pkl', 'rb'))
+
+bs,bptt = 128,70
+data = lm_databunchify(ll, bs, bptt)
+
+vocab = ll.train.proc_x[1].vocab
+
+# ## Finetuning the LM
+
+# Before tackling the classification task, we have to finetune our language model to the IMDB corpus.
+
+# We have pretrained a small model on [wikitext 103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) that you can download by uncommenting the following cell.
+
+# +
+# # ! wget http://files.fast.ai/models/wt103_tiny.tgz -P {path}
+# # ! tar xf {path}/wt103_tiny.tgz -C {path}
+# -
+
+dps = tensor([0.1, 0.15, 0.25, 0.02, 0.2]) * 0.5
+tok_pad = vocab.index(PAD)
+
+emb_sz, nh, nl = 300, 300, 2
+model = get_language_model(len(vocab), emb_sz, nh, nl, tok_pad, *dps)
+
+old_wgts  = torch.load(path/'pretrained'/'pretrained.pth')
+old_vocab = pickle.load(open(path/'pretrained'/'vocab.pkl', 'rb'))
+
+# In our current vocabulary, it is very unlikely that the ids correspond to what is in the vocabulary used to train the pretrain model. The tokens are sorted by frequency (apart from the special tokens that are all first) so that order is specific to the corpus used. For instance, the word 'house' has different ids in the our current vocab and the pretrained one.
+
+idx_house_new, idx_house_old = vocab.index('house'),old_vocab.index('house')
+
+# We somehow need to match our pretrained weights to the new vocabulary. This is done on the embeddings and the decoder (since the weights between embeddings and decoders are tied) by putting the rows of the embedding matrix (or decoder bias) in the right order.
+#
+# It may also happen that we have words that aren't in the pretrained vocab, in this case, we put the mean of the pretrained embedding weights/decoder bias.
+
+house_wgt  = old_wgts['0.emb.weight'][idx_house_old]
+house_bias = old_wgts['1.decoder.bias'][idx_house_old] 
+
+
+def match_embeds(old_wgts, old_vocab, new_vocab):
+    wgts = old_wgts['0.emb.weight']
+    bias = old_wgts['1.decoder.bias']
+    wgts_m,bias_m = wgts.mean(dim=0),bias.mean()
+    new_wgts = wgts.new_zeros(len(new_vocab), wgts.size(1))
+    new_bias = bias.new_zeros(len(new_vocab))
+    otoi = {v:k for k,v in enumerate(old_vocab)}
+    for i,w in enumerate(new_vocab): 
+        if w in otoi:
+            idx = otoi[w]
+            new_wgts[i],new_bias[i] = wgts[idx],bias[idx]
+        else: new_wgts[i],new_bias[i] = wgts_m,bias_m
+    old_wgts['0.emb.weight']    = new_wgts
+    old_wgts['0.emb_dp.emb.weight'] = new_wgts
+    old_wgts['1.decoder.weight']    = new_wgts
+    old_wgts['1.decoder.bias']      = new_bias
+    return old_wgts
+
+
+wgts = match_embeds(old_wgts, old_vocab, vocab)
+
+# Now let's check that the word "*house*" was properly converted.
+
+test_near(wgts['0.emb.weight'][idx_house_new],house_wgt)
+test_near(wgts['1.decoder.bias'][idx_house_new],house_bias)
+
+# We can load the pretrained weights in our model before beginning training.
+
+model.load_state_dict(wgts)
+
+# If we want to apply discriminative learning rates, we need to split our model in different layer groups. Let's have a look at our model.
+
+model
+
+
+# Then we split by doing two groups for each rnn/corresponding dropout, then one last group that contains the embeddings/decoder. This is the one that needs to be trained the most as we may have new embeddings vectors.
+
+def lm_splitter(m):
+    groups = []
+    for i in range(len(m[0].rnns)): groups.append(nn.Sequential(m[0].rnns[i], m[0].hidden_dps[i]))
+    groups += [nn.Sequential(m[0].emb, m[0].emb_dp, m[0].input_dp, m[1])]
+    return [list(o.parameters()) for o in groups]
+
+
+# First we train with the RNNs freezed.
+
+for rnn in model[0].rnns:
+    for p in rnn.parameters(): p.requires_grad_(False)
+
+cbs = [partial(AvgStatsCallback,accuracy_flat),
+       CudaCallback, Recorder,
+       partial(GradientClipping, clip=0.1),
+       partial(RNNTrainer, α=2., β=1.),
+       ProgressCallback]
+
+learn = Learner(model, data, cross_entropy_flat, opt_func=adam_opt(),
+                cb_funcs=cbs, splitter=lm_splitter)
+
+lr = 2e-2
+cbsched = sched_1cycle([lr], pct_start=0.5, mom_start=0.8, mom_mid=0.7, mom_end=0.8)
+
+learn.fit(1, cbs=cbsched)
+
+# Then the whole model with discriminative learning rates.
+
+for rnn in model[0].rnns:
+    for p in rnn.parameters(): p.requires_grad_(True)
+
+lr = 2e-3
+cbsched = sched_1cycle([lr/2., lr/2., lr], pct_start=0.5, mom_start=0.8, mom_mid=0.7, mom_end=0.8)
+
+learn.fit(10, cbs=cbsched)
+
+# We only need to save the encoder (first part of the model) for the classification, as well as the vocabulary used (we will need to use the same in the classification task).
+
+torch.save(learn.model[0].state_dict(), path/'finetuned_enc.pth')
+
+pickle.dump(vocab, open(path/'vocab_lm.pkl', 'wb'))
+
+torch.save(learn.model.state_dict(), path/'finetuned.pth')
+
+# ## Classifier
+
+# We have to process the data again otherwise pickle will complain. We also have to use the same vocab as the language model.
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=7554)
+
+vocab = pickle.load(open(path/'vocab_lm.pkl', 'rb'))
+proc_tok,proc_num,proc_cat = TokenizeProcessor(),NumericalizeProcessor(vocab=vocab),CategoryProcessor()
+
+il = TextList.from_files(path, include=['train', 'test'])
+sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='test'))
+ll = label_by_func(sd, parent_labeler, proc_x = [proc_tok, proc_num], proc_y=proc_cat)
+
+pickle.dump(ll, open(path/'ll_clas.pkl', 'wb'))
+
+ll = pickle.load(open(path/'ll_clas.pkl', 'rb'))
+vocab = pickle.load(open(path/'vocab_lm.pkl', 'rb'))
+
+bs,bptt = 64,70
+data = clas_databunchify(ll, bs)
+
+# ### Ignore padding
+
+# We will those two utility functions from PyTorch to ignore the padding in the inputs.
+
+#export
+from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
+
+# Let's see how this works: first we grab a batch of the training set.
+
+x,y = next(iter(data.train_dl))
+
+x.size()
+
+# We need to pass to the utility functions the lengths of our sentences because it's applied after the embedding, so we can't see the padding anymore.
+
+lengths = x.size(1) - (x == 1).sum(1)
+lengths[:5]
+
+tst_emb = nn.Embedding(len(vocab), 300)
+
+tst_emb(x).shape
+
+128*70
+
+# We create a `PackedSequence` object that contains all of our unpadded sequences
+
+packed = pack_padded_sequence(tst_emb(x), lengths, batch_first=True)
+
+packed
+
+packed.data.shape
+
+len(packed.batch_sizes)
+
+8960//70
+
+# This object can be passed to any RNN directly while retaining the speed of CuDNN.
+
+tst = nn.LSTM(300, 300, 2)
+
+y,h = tst(packed)
+
+# Then we can unpad it with the following function for other modules:
+
+unpack = pad_packed_sequence(y, batch_first=True)
+
+unpack[0].shape
+
+unpack[1]
+
+
+# We need to change our model a little bit to use this.
+
+#export
+class AWD_LSTM1(nn.Module):
+    "AWD-LSTM inspired by https://arxiv.org/abs/1708.02182."
+    initrange=0.1
+
+    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,
+                 hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):
+        super().__init__()
+        self.bs,self.emb_sz,self.n_hid,self.n_layers,self.pad_token = 1,emb_sz,n_hid,n_layers,pad_token
+        self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)
+        self.emb_dp = EmbeddingDropout(self.emb, embed_p)
+        self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz), 1,
+                             batch_first=True) for l in range(n_layers)]
+        self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns])
+        self.emb.weight.data.uniform_(-self.initrange, self.initrange)
+        self.input_dp = RNNDropout(input_p)
+        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])
+
+    def forward(self, input):
+        bs,sl = input.size()
+        mask = (input == self.pad_token)
+        lengths = sl - mask.long().sum(1)
+        n_empty = (lengths == 0).sum()
+        if n_empty > 0:
+            input = input[:-n_empty]
+            lengths = lengths[:-n_empty]
+            self.hidden = [(h[0][:,:input.size(0)], h[1][:,:input.size(0)]) for h in self.hidden]
+        raw_output = self.input_dp(self.emb_dp(input))
+        new_hidden,raw_outputs,outputs = [],[],[]
+        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):
+            raw_output = pack_padded_sequence(raw_output, lengths, batch_first=True)
+            raw_output, new_h = rnn(raw_output, self.hidden[l])
+            raw_output = pad_packed_sequence(raw_output, batch_first=True)[0]
+            raw_outputs.append(raw_output)
+            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)
+            outputs.append(raw_output)
+            new_hidden.append(new_h)
+        self.hidden = to_detach(new_hidden)
+        return raw_outputs, outputs, mask
+
+    def _one_hidden(self, l):
+        "Return one hidden state."
+        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz
+        return next(self.parameters()).new(1, self.bs, nh).zero_()
+
+    def reset(self):
+        "Reset the hidden states."
+        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]
+
+
+# ### Concat pooling
+
+# We will use three things for the classification head of the model: the last hidden state, the average of all the hidden states and the maximum of all the hidden states. The trick is just to, once again, ignore the padding in the last element/average/maximum.
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=7604)
+
+class Pooling(nn.Module):
+    def forward(self, input):
+        raw_outputs,outputs,mask = input
+        output = outputs[-1]
+        lengths = output.size(1) - mask.long().sum(dim=1)
+        avg_pool = output.masked_fill(mask[:,:,None], 0).sum(dim=1)
+        avg_pool.div_(lengths.type(avg_pool.dtype)[:,None])
+        max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]
+        x = torch.cat([output[torch.arange(0, output.size(0)),lengths-1], max_pool, avg_pool], 1) #Concat pooling.
+        return output,x
+
+
+emb_sz, nh, nl = 300, 300, 2
+tok_pad = vocab.index(PAD)
+
+enc = AWD_LSTM1(len(vocab), emb_sz, n_hid=nh, n_layers=nl, pad_token=tok_pad)
+pool = Pooling()
+enc.bs = bs
+enc.reset()
+
+x,y = next(iter(data.train_dl))
+output,c = pool(enc(x))
+
+# We can check we have padding with 1s at the end of each text (except the first which is the longest).
+
+x
+
+# PyTorch puts 0s everywhere we had padding in the `output` when unpacking.
+
+test_near((output.sum(dim=2) == 0).float(), (x==tok_pad).float())
+
+# So the last hidden state isn't the last element of `output`. Let's check we got everything right. 
+
+for i in range(bs):
+    length = x.size(1) - (x[i]==1).long().sum()
+    out_unpad = output[i,:length]
+    test_near(out_unpad[-1], c[i,:300])
+    test_near(out_unpad.max(0)[0], c[i,300:600])
+    test_near(out_unpad.mean(0), c[i,600:])
+
+
+# Our pooling layer properly ignored the padding, so now let's group it with a classifier.
+
+def bn_drop_lin(n_in, n_out, bn=True, p=0., actn=None):
+    layers = [nn.BatchNorm1d(n_in)] if bn else []
+    if p != 0: layers.append(nn.Dropout(p))
+    layers.append(nn.Linear(n_in, n_out))
+    if actn is not None: layers.append(actn)
+    return layers
+
+
+class PoolingLinearClassifier(nn.Module):
+    "Create a linear classifier with pooling."
+
+    def __init__(self, layers, drops):
+        super().__init__()
+        mod_layers = []
+        activs = [nn.ReLU(inplace=True)] * (len(layers) - 2) + [None]
+        for n_in, n_out, p, actn in zip(layers[:-1], layers[1:], drops, activs):
+            mod_layers += bn_drop_lin(n_in, n_out, p=p, actn=actn)
+        self.layers = nn.Sequential(*mod_layers)
+
+    def forward(self, input):
+        raw_outputs,outputs,mask = input
+        output = outputs[-1]
+        lengths = output.size(1) - mask.long().sum(dim=1)
+        avg_pool = output.masked_fill(mask[:,:,None], 0).sum(dim=1)
+        avg_pool.div_(lengths.type(avg_pool.dtype)[:,None])
+        max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]
+        x = torch.cat([output[torch.arange(0, output.size(0)),lengths-1], max_pool, avg_pool], 1) #Concat pooling.
+        x = self.layers(x)
+        return x
+
+
+# Then we just have to feed our texts to those two blocks, (but we can't give them all at once to the AWD_LSTM or we might get OOM error: we'll go for chunks of bptt length to regularly detach the history of our hidden states.)
+
+def pad_tensor(t, bs, val=0.):
+    if t.size(0) < bs:
+        return torch.cat([t, val + t.new_zeros(bs-t.size(0), *t.shape[1:])])
+    return t
+
+
+class SentenceEncoder(nn.Module):
+    def __init__(self, module, bptt, pad_idx=1):
+        super().__init__()
+        self.bptt,self.module,self.pad_idx = bptt,module,pad_idx
+
+    def concat(self, arrs, bs):
+        return [torch.cat([pad_tensor(l[si],bs) for l in arrs], dim=1) for si in range(len(arrs[0]))]
+    
+    def forward(self, input):
+        bs,sl = input.size()
+        self.module.bs = bs
+        self.module.reset()
+        raw_outputs,outputs,masks = [],[],[]
+        for i in range(0, sl, self.bptt):
+            r,o,m = self.module(input[:,i: min(i+self.bptt, sl)])
+            masks.append(pad_tensor(m, bs, 1))
+            raw_outputs.append(r)
+            outputs.append(o)
+        return self.concat(raw_outputs, bs),self.concat(outputs, bs),torch.cat(masks,dim=1)
+
+
+def get_text_classifier(vocab_sz, emb_sz, n_hid, n_layers, n_out, pad_token, bptt, output_p=0.4, hidden_p=0.2, 
+                        input_p=0.6, embed_p=0.1, weight_p=0.5, layers=None, drops=None):
+    "To create a full AWD-LSTM"
+    rnn_enc = AWD_LSTM1(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,
+                        hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)
+    enc = SentenceEncoder(rnn_enc, bptt)
+    if layers is None: layers = [50]
+    if drops is None:  drops = [0.1] * len(layers)
+    layers = [3 * emb_sz] + layers + [n_out] 
+    drops = [output_p] + drops
+    return SequentialRNN(enc, PoolingLinearClassifier(layers, drops))
+
+
+emb_sz, nh, nl = 300, 300, 2
+dps = tensor([0.4, 0.3, 0.4, 0.05, 0.5]) * 0.25
+model = get_text_classifier(len(vocab), emb_sz, nh, nl, 2, 1, bptt, *dps)
+
+
+# ### Training
+
+# We load our pretrained encoder and freeze it.
+
+# [Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=7684)
+
+def class_splitter(m):
+    enc = m[0].module
+    groups = [nn.Sequential(enc.emb, enc.emb_dp, enc.input_dp)]
+    for i in range(len(enc.rnns)): groups.append(nn.Sequential(enc.rnns[i], enc.hidden_dps[i]))
+    groups.append(m[1])
+    return [list(o.parameters()) for o in groups]
+
+
+for p in model[0].parameters(): p.requires_grad_(False)
+
+cbs = [partial(AvgStatsCallback,accuracy),
+       CudaCallback, Recorder,
+       partial(GradientClipping, clip=0.1),
+       ProgressCallback]
+
+model[0].module.load_state_dict(torch.load(path/'finetuned_enc.pth'))
+
+learn = Learner(model, data, F.cross_entropy, opt_func=adam_opt(), cb_funcs=cbs, splitter=class_splitter)
+
+lr = 1e-2
+cbsched = sched_1cycle([lr], mom_start=0.8, mom_mid=0.7, mom_end=0.8)
+
+learn.fit(1, cbs=cbsched)
+
+for p in model[0].module.rnns[-1].parameters(): p.requires_grad_(True)
+
+lr = 5e-3
+cbsched = sched_1cycle([lr/2., lr/2., lr/2., lr], mom_start=0.8, mom_mid=0.7, mom_end=0.8)
+
+learn.fit(1, cbs=cbsched)
+
+for p in model[0].parameters(): p.requires_grad_(True)
+
+lr = 1e-3
+cbsched = sched_1cycle([lr/8., lr/4., lr/2., lr], mom_start=0.8, mom_mid=0.7, mom_end=0.8)
+
+learn.fit(2, cbs=cbsched)
+
+x,y = next(iter(data.valid_dl))
+
+# Predicting on the padded batch or on the individual unpadded samples give the same results.
+
+pred_batch = learn.model.eval()(x.cuda())
+
+pred_ind = []
+for inp in x:
+    length = x.size(1) - (inp == 1).long().sum()
+    inp = inp[:length]
+    pred_ind.append(learn.model.eval()(inp[None].cuda()))
+
+assert near(pred_batch, torch.cat(pred_ind))
+
+
diff --git a/nbs/dl2/audio.ipynb b/nbs/dl2/audio.ipynb
index 1a4b3a4..5d1cf9e 100644
--- ./nbs/dl2/audio.ipynb
+++ ./nbs/dl2/audio.ipynb
@@ -1247,6 +1247,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/audio.py b/nbs/dl2/audio.py
new file mode 100644
index 0000000..4220910
--- /dev/null
+++ ./nbs/dl2/audio.py
@@ -0,0 +1,401 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Audio classification the from-scratch way
+
+# Thanks to the SF Study Group practitioners & participants in the [Deep Learning for Audio fastai forum thread](https://forums.fast.ai/t/deep-learning-with-audio-thread/), especially: [Molly Beavers](https://forums.fast.ai/u/marii/summary), [Simon Grest](https://forums.fast.ai/u/simonjhb/), [Stefano Giomo](https://forums.fast.ai/u/ste), [Thom Mackey](https://forums.fast.ai/u/ThomM), [Zach Caceres](https://forums.fast.ai/u/zachcaceres), [Harry Coultas Blum](https://forums.fast.ai/u/baz), & [Robert Bracco](https://forums.fast.ai/u/madeupmasters).
+
+# We're going to demonstrate the technique of classifying audio samples by first converting the audio into spectrograms, then treating the spectrograms as images. Once we've converted the spectrograms to images, the workflow is just the same as using imagenette or any other image classification task.
+#
+# What do we need to do?
+# * Download the data
+# * Load the data 
+# * Transform the data into spectrograms
+# * Load the audio data into a databunch such that we can use our previously-defined `learner` object
+#
+# Still to come - 1D convolutional models, RNNs with audio… and more, with your contribution :)
+
+# ### Setup & imports
+
+# %matplotlib inline
+
+# We rely heavily on [torchaudio](https://github.com/pytorch/audio) - which you'll have to compile to install.
+
+#export
+from exp.nb_12a import *
+import torchaudio
+from torchaudio import transforms
+
+#export
+AUDIO_EXTS = {str.lower(k) for k,v in mimetypes.types_map.items() if v.startswith('audio/')}
+
+# ### Download
+
+# This should be one line; it's only so complicated because the target .tgz file doesn't extract itself to its own directory.
+
+dsid = "ST-AEDS-20180100_1-OS"
+data_url = f'http://www.openslr.org/resources/45/{dsid}' # actual URL has .tgz extension but untar_data doesn't like that
+path = Path.home() / Path(f".fastai/data/{dsid}/")
+datasets.untar_data(data_url, dest=path)
+path
+
+# ## Loading into an AudioList
+
+# Getting a file list the `08_data_block` way.
+
+# The "manual" way using `get_files`…
+
+audios = get_files(path, extensions=AUDIO_EXTS)
+print(f"Found {len(audios)} audio files")
+audios[:5]
+
+
+# …But that's not very exciting. Let's make an `AudioList`, so we can use transforms, and define how to `get` an Audio.
+
+# ### AudioList
+
+#export
+class AudioList(ItemList):
+    @classmethod
+    def from_files(cls, path, extensions=None, recurse=True, include=None, **kwargs):
+        if extensions is None: extensions = AUDIO_EXTS
+        return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs)
+    
+    def get(self, fn): 
+        sig, sr = torchaudio.load(fn)
+        assert sig.size(0) == 1, "Non-mono audio detected, mono only supported for now."
+        return (sig, sr)
+
+
+al = AudioList.from_files(path); al
+
+# It looks like this is full of file paths, but that's just the `repr` talking. Actually accessing an item from the list calls the `get` method and returns a `(Tensor, Int)` tuple representing the signal & sample rate.
+
+al[0]
+
+# ## Splitting into train/validation
+
+# Our data is all in one folder, there's no specific validation set, so let's just split it at random.
+
+sd = SplitData.split_by_func(al, partial(random_splitter, p_valid=0.2))
+
+sd
+
+
+# ## Labeling
+
+# Our labels are encoded in our filenames. For example, `m0003_us_m0003_00032.wav` has the label `m0003`. Let's make a regex labeler, then use it.
+
+#export
+def re_labeler(fn, pat): return re.findall(pat, str(fn))[0]
+
+
+label_pat = r'/([mf]\d+)_'
+speaker_labeler = partial(re_labeler, pat=label_pat)
+ll = label_by_func(sd, speaker_labeler, proc_y=CategoryProcessor())
+
+ll
+
+# ## Transforms: audio clipping & conversion to spectrograms
+
+# The pytorch dataloader needs to be all tensors to be the same size, but our input audio files are of different sizes, so we need to trim them. Also, recall that we're not going to send the model the audio signal directly; we're going to convert it to spectrograms first. We can treat these steps as transforms. We can also apply data augmentation to both the signal and the spectrograms. 
+
+# Small helpers to show some audio & compare transformed versions.
+
+#export
+from IPython.display import Audio
+def show_audio(ad):
+    sig,sr=ad
+    display(Audio(data=sig, rate=sr))
+
+
+#export
+def show_audio_in_out(orig, trans):
+    """Helper to plot input and output signal in different colors"""
+    osig,osr = orig
+    tsig,tsr = trans
+    print("↓ Original ↓")
+    show_audio(orig)
+    print("↓ Transformed ↓")
+    show_audio(trans)
+    if orig is not None: plt.plot(osig[0], 'm', label="Orig.")
+    if trans is not None: plt.plot(tsig[0], 'c', alpha=0.5, label="Transf.")
+    plt.legend()
+    plt.show()
+
+
+# ### toCuda
+
+# The other transforms both use all-tensor ops, so it should help. Let's try it out.
+
+#export
+class ToCuda(Transform):
+    _order=10
+    def __call__(self, ad):
+        sig,sr = ad
+        return (sig.cuda(), sr)
+
+
+ToCuda()(ll.train[0][0])
+
+
+# ### PadOrTrim
+
+# `torchaudio` has one for this already. All we're doing is taking an argument in milliseconds rather than frames.
+
+#export
+class PadOrTrim(Transform):
+    _order=11
+    def __init__(self,msecs): self.msecs = msecs
+    def __call__(self, ad): 
+        sig,sr = ad
+        mx = sr//1000 * self.msecs
+        return (transforms.PadTrim(mx)(sig), sr)
+
+
+# *Note - this won't work if you've already run the notebook all the way through, because `ll` now contains Tensors representing Spectrograms, not `(Signal, SampleRate)` tuples.*
+
+pt = PadOrTrim(3000) ## duration in milliseconds
+show_audio_in_out(ll.train[0][0], pt(ll.train[0][0]))
+
+
+# ### Signal shifting
+
+# Signal shifting is a useful augmentation which can be applied to the signal itself; it simply shifts the input to the left or right by some %. Intuitively, this is useful for classification, but perhaps not for predictive models, as it can change the temporal order of things when the end of the signal wraps around to the front.
+#
+# This implementation lightly adapted from the [original implementation](https://github.com/mogwai/fastai-audio/blob/sg-cache/Google%20Small%20ASR.ipynb) by [Harry Coultas Blum](https://github.com/mogwai/).
+
+#export
+class SignalShift(Transform):
+    _order=20
+    def __init__(self, max_shift_pct=.6): self.max_shift_pct = max_shift_pct
+    def __call__(self, ad):
+        sig,sr = ad
+        roll_by = int(random.random()*self.max_shift_pct*len(sig[0]))
+        return (sig.roll(roll_by), sr)
+
+
+shifter = SignalShift()
+show_audio_in_out(ll.train[0][0], shifter(ll.train[0][0]))
+
+
+# ### Spectrogram
+
+# Once we're done augmenting the signal directly, we want to convert our 1D signal to a 2D [spectrogram](https://en.wikipedia.org/wiki/Spectrogram).
+#
+# Luckily, `torchaudio` has functions for calculation & conversion to spectrograms. Let's wrap their functions.
+
+#export
+class Spectrogrammer(Transform):
+    _order=90
+    def __init__(self, to_mel=True, to_db=True, n_fft=400, ws=None, hop=None, 
+                 f_min=0.0, f_max=None, pad=0, n_mels=128, top_db=None, normalize=False):
+        self.to_mel, self.to_db, self.n_fft, self.ws, self.hop, self.f_min, self.f_max, \
+        self.pad, self.n_mels, self.top_db, self.normalize = to_mel, to_db, n_fft, \
+        ws, hop, f_min, f_max, pad, n_mels, top_db, normalize
+
+    def __call__(self, ad):
+        sig,sr = ad
+        if self.to_mel:
+            spec = transforms.MelSpectrogram(sr, self.n_fft, self.ws, self.hop, self.f_min, 
+                                             self.f_max, self.pad, self.n_mels)(sig)
+        else: 
+            spec = transforms.Spectrogram(self.n_fft, self.ws, self.hop, self.pad, 
+                                          normalize=self.normalize)(sig)
+        if self.to_db:
+            spec = transforms.SpectrogramToDB(top_db=self.top_db)(spec)
+        spec = spec.permute(0,2,1) # reshape so it looks good to humans
+        return spec
+
+
+# Small helper to show a spectrogram.
+
+#export
+def show_spectro(img, ax=None, figsize=(6,6), with_shape=True):
+    if hasattr(img,"device") & str(img.device).startswith("cuda"): img = img.cpu()
+    if ax is None: _,ax = plt.subplots(1, 1, figsize=figsize)
+    ax.imshow(img if (img.shape[0]==3) else img.squeeze(0))
+    if with_shape: display(f'Tensor shape={img.shape}')
+
+
+# *Note - this won't work if you've already run the notebook all the way through, because `ll` now contains Tensors representing Spectrograms, not `(Signal, SampleRate)` tuples.*
+
+speccer = Spectrogrammer(to_db=True, n_fft=1024, n_mels=64, top_db=80)
+show_spectro(speccer(ll.train[0][0]))
+
+
+# ### SpecAugment
+
+# An implementation of the [SpecAugment data augmentation technique](https://arxiv.org/abs/1904.08779) from Google Brain, which was shown to improve on SOTA results for audio classification. Effectively, it masks some number of contiguous frequency bands or timesteps. The intuition is similar to [cutout](https://arxiv.org/abs/1708.04552) for "normal" images. See their [blog post](https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html) for a good discussion.
+#
+# This implementation lightly adapted from both [Zach Caceres](https://github.com/zcaceres/) and [Harry Coultas Blum](https://github.com/mogwai/) & [Robert Bracco](https://forums.fast.ai/u/madeupmasters). Rather than static `F` or `T` parameters for a maximum width of the masks per the paper, we're taking a "maximum mask %" and calculating the `F` and `T` from that.
+#
+# This does not attempt to implement the "time warping" augmentation from the paper, as it is not directly supported by pytorch, and provides only modest gains for considerable computational cost. [Zach Caceres's SpecAugment implementation](https://github.com/zcaceres/spec_augment) provides a full implementation of time warping.
+
+#export
+class SpecAugment(Transform):
+    _order=99
+    def __init__(self, max_mask_pct=0.2, freq_masks=1, time_masks=1, replace_with_zero=False):
+        self.max_mask_pct, self.freq_masks, self.time_masks, self.replace_with_zero = \
+        max_mask_pct, freq_masks, time_masks, replace_with_zero
+        if not 0 <= self.max_mask_pct <= 1.0: 
+            raise ValueError( f"max_mask_pct must be between 0.0 and 1.0, but it's {self.max_mask_pct}")
+
+    def __call__(self, spec):
+        _, n_mels, n_steps = spec.shape
+        F = math.ceil(n_mels * self.max_mask_pct) # rounding up in case of small %
+        T = math.ceil(n_steps * self.max_mask_pct)
+        fill = 0 if self.replace_with_zero else spec.mean()
+        for i in range(0, self.freq_masks):
+            f = random.randint(0, F)
+            f0 = random.randint(0, n_mels-f)
+            spec[0][f0:f0+f] = fill
+        for i in range(0, self.time_masks):
+            t = random.randint(0, T)
+            t0 = random.randint(0, n_steps-t)
+            spec[0][:,t0:t0+t] = fill
+        return spec
+
+
+masker = SpecAugment(freq_masks=2,time_masks=2,max_mask_pct=0.1)
+show_spectro(masker(speccer(ll.train[0][0])))
+
+# ### Using the transforms
+
+# Now let's create the transforms with the params we want, and rebuild our label lists using them. 
+#
+# Note that now the items in the final `LabelList` won't be tuples anymore, they'll just be tensors. This is convenient for actually using the data, but it means you can't really go back and listen to your audio anymore. We can probably find a way around this, but let's press on for now.
+
+# +
+pad_3sec = PadOrTrim(3000)
+shifter = SignalShift()
+speccer = Spectrogrammer(n_fft=1024, n_mels=64, top_db=80)
+masker = SpecAugment(freq_masks=2, time_masks=2, max_mask_pct=0.1)
+
+tfms = [ToCuda(), shifter, pad_3sec, speccer, masker]
+
+al = AudioList.from_files(path, tfms=tfms)
+sd = SplitData.split_by_func(al, partial(random_splitter, p_valid=0.2))
+ll = label_by_func(sd, speaker_labeler, proc_y=CategoryProcessor())
+# -
+
+show_spectro(ll.train[4][0])
+
+# ## Databunch
+
+# Now we've got our beautifully transformed tensors, let's add them into a databunch, so we can feed a model easily.
+#
+# We can use our `get_dls` func which we defined in `03_minibatch_training`, but let's use the to_databunch func we defined in `08_data_block` instead, it's much nicer.
+
+# +
+bs=64
+
+c_in = ll.train[0][0].shape[0]
+c_out = len(uniqueify(ll.train.y))
+# -
+
+data = ll.to_databunch(bs,c_in=c_in,c_out=c_out)
+
+# Check the dataloader's batching functionality.
+
+x,y = next(iter(data.train_dl))
+
+
+# Here we're overriding `10_augmentation`'s `show_batch` method to take a "shower" function so we can give it a spectrogram shower.
+
+#export
+def show_batch(x, c=4, r=None, figsize=None, shower=show_image):
+    n = len(x)
+    if r is None: r = int(math.ceil(n/c))
+    if figsize is None: figsize=(c*3,r*3)
+    fig,axes = plt.subplots(r,c, figsize=figsize)
+    for xi,ax in zip(x,axes.flat): shower(xi, ax)
+
+
+show_spec_batch = partial(show_batch, c=4, r=2, figsize=None, 
+                          shower=partial(show_spectro, with_shape=False))
+
+show_spec_batch(x)
+
+# Looking good.
+
+# ## Training
+
+# Go for gold! As a proof of concept, let's use the *pièce de résistance* learner builder with the hyperparameters from Lesson 11 `11_train_imagenette`.
+
+opt_func = adam_opt(mom=0.9, mom_sqr=0.99, eps=1e-6, wd=1e-2)
+loss_func = LabelSmoothingCrossEntropy()
+lr = 1e-2
+pct_start = 0.5
+phases = create_phases(pct_start)
+sched_lr  = combine_scheds(phases, cos_1cycle_anneal(lr/10., lr, lr/1e5))
+sched_mom = combine_scheds(phases, cos_1cycle_anneal(0.95,0.85, 0.95))
+cbscheds = [ParamScheduler('lr', sched_lr), 
+            ParamScheduler('mom', sched_mom)]
+
+learn = cnn_learner(xresnet34, data, loss_func, opt_func)
+
+learn.fit(5, cbs=cbscheds)
+
+# ## Demo - all at once
+
+# This is all the code it takes to do it end-to-end (not counting the `#export` cells above).
+
+# +
+# dsid = "ST-AEDS-20180100_1-OS"
+# data_url = f'http://www.openslr.org/resources/45/{dsid}' # actual URL has .tgz extension but untar_data doesn't like that
+# path = Path.home() / Path(f".fastai/data/{dsid}/")
+# datasets.untar_data(data_url, dest=path)
+
+# pad_3sec = PadOrTrim(3000)
+# shifter = SignalShift()
+# speccer = Spectrogrammer(n_fft=1024, n_mels=64, top_db=80)
+# masker = SpecAugment(freq_masks=2, time_masks=2, max_mask_pct=0.1)
+
+# tfms = [ToCuda(), shifter, pad_3sec, speccer, masker]
+# al = AudioList.from_files(path, tfms=tfms)
+
+# sd = SplitData.split_by_func(al, partial(random_splitter, p_valid=0.2))
+
+# label_pat = r'/([mf]\d+)_'
+# speaker_labeler = partial(re_labeler, pat=label_pat)
+# ll = label_by_func(sd, speaker_labeler, proc_y=CategoryProcessor())
+
+# bs=64
+# c_in = ll.train[0][0].shape[0]
+# c_out = len(uniqueify(ll.train.y))
+
+# data = ll.to_databunch(bs,c_in=c_in,c_out=c_out)
+
+# opt_func = adam_opt(mom=0.9, mom_sqr=0.99, eps=1e-6, wd=1e-2)
+# loss_func = LabelSmoothingCrossEntropy()
+# lr = 1e-2
+# pct_start = 0.5
+# phases = create_phases(pct_start)
+# sched_lr  = combine_scheds(phases, cos_1cycle_anneal(lr/10., lr, lr/1e5))
+# sched_mom = combine_scheds(phases, cos_1cycle_anneal(0.95,0.85, 0.95))
+# cbscheds = [ParamScheduler('lr', sched_lr), 
+#             ParamScheduler('mom', sched_mom)]
+
+# learn = cnn_learner(xresnet34, data, loss_func, opt_func)
+# learn.fit(5, cbs=cbscheds)
+# -
+
+# ## Fin
+
+nb_auto_export()
+
+
diff --git a/nbs/dl2/bleu_metric.ipynb b/nbs/dl2/bleu_metric.ipynb
index a789ebd..5f066de 100644
--- ./nbs/dl2/bleu_metric.ipynb
+++ ./nbs/dl2/bleu_metric.ipynb
@@ -264,6 +264,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/bleu_metric.py b/nbs/dl2/bleu_metric.py
new file mode 100644
index 0000000..aa66fd8
--- /dev/null
+++ ./nbs/dl2/bleu_metric.py
@@ -0,0 +1,157 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+from fastai.text import *
+
+# ## What is the BLEU metric?
+
+# The BLEU metric has been introduced in [this article](https://www.aclweb.org/anthology/P02-1040) to come with some kind of way to evaluate the performance of translation models. It's based on the precision you hit with n-grams in your prediction compared to your target. Let's see this on an example. Imagine you have the target sentence
+# ```
+# the cat is walking in the garden
+# ```
+# and your model gives you the following output
+# ```
+# the cat is running in the fields
+# ```
+# We are going to compute the precision, which is the number of correctly predicted n-grams divided by the number of predicted n-grams for n going from 1 to 4.
+#
+# For 1-grams (or tokens, more simply), we have predicted 5 correct words out of 7, so we get a precision of 5/7. Note that the order doesn't matter, for instance predicting
+# ```
+# she read the book because she was interested in world history
+# ```
+# instead of
+# ```
+# she was interested in world history because she read the book
+# ```
+# would give a precision of 1 for the 1-grams.
+#
+# For 2-grams, in the first example, we have 3 correct 2-grams ("the cat", "cat is" and "in the") out of 6, so a precision of 3/6. In the second example, the precision for 2-grams is 9/10.
+#
+# For 3-grams, in the first example, we have only 1 correct 3-gram ("the cat is") out of 5, so a precision of 1/5. In the second example, the precision for 3-grams is 6/9.
+#
+# For 4-grams, in the first example, we don't have any 4-gram that is correct, so the precision is 0. In the second example, it's 4/8.
+#
+# There is one big drawback in just taking the raw precision: very often a seq2seq model will predict the same easy word. If take the prediction
+# ```
+# the the the the the the the the
+# ```
+# for our first example, it would score a precision of 1. for 1-grams (because `the` is in the target sentence, so all the words are considered correct). To avoid that, we put a maximum for a given words to the number of times it can be considered correct, which is the number of times that word is in the target sentence. So in our example, only 2 of the 7 the are considered correct and this clamped precision gives us 2/7 for 1-grams.
+#
+# One thing to note is that when we deal with a whole corpus, we take the sum of all the corrects over all the sentences then divide by the sum of all the predicted over all the sentences (we don't average precisions over each sentence).
+#
+# To compute the BLEU score, the final formula is then
+# ```
+# BLEU = length_penalty * ((p1 * p2 * p3 * p4) ** 0.25)
+# ```
+# which is the geometric average of `p1`, `p2`, `p3` and `p4` (our n-gram precision scores) multiplied by a penalty given for the length: we penalize longer predictions with the precision scores, but short ones get it easier, especially if they only contain correct words. So we apply the following penalty:
+# ```
+# length_penalty = 1 if len(pred) >= len(targ) else (1 - exp(len(targ)/len(pred)))
+# ```
+#
+# And if we are taking the BLEU score of a whole corpus, we use the sum of the lengths of predicted sentences and the sum of the lengths of predicted targets.
+
+# ## Let's code this
+
+# There is an implementation of BLEU in nltk, but the problem is that it's designed to support lists of tokenized texts, and therefore is very slow (5 hours announced on the validation set of the translation notebook for the average of BLEU scores of sentences). We have numericalized text, so it's easier to reimplement this and only deal with integers.
+#
+# Specifically we are going to use the Counter class, which is going to count the number of instances of each n-gram in the predicted sentence and the target one:
+
+targ = [1,2,3,4,5,1,2]
+pred = [1,2,3,7,5,1,1]
+
+cnt_pred,cnt_targ = Counter(pred),Counter(targ)
+
+# Now the number of corrects is the number of words in `pred` that are in `targ`, with a cap that is the number of times they appear in `targ`.
+
+corrects = sum([min(c, cnt_targ[g]) for g,c in cnt_pred.items()])
+corrects
+
+
+# Now this works very well for ints, but it won't work for a list of ints, since a `Counter` requires the objects inside to be hashable. That's why we define a custom class:
+
+class NGram():
+    def __init__(self, ngram, max_n=5000): self.ngram,self.max_n = ngram,max_n
+    def __eq__(self, other):
+        if len(self.ngram) != len(other.ngram): return False
+        return np.all(np.array(self.ngram) == np.array(other.ngram))
+    def __hash__(self): return int(sum([o * self.max_n**i for i,o in enumerate(self.ngram)]))
+
+
+# `max_n` should be set to the vocab size, so we're sure we don't have two different ngrams with the same hash (it needs to be an int).
+#
+# Then we can define `get_grams` to gather all the possible ngrams:
+
+def get_grams(x, n, max_n=5000):
+    return x if n==1 else [NGram(x[i:i+n], max_n=max_n) for i in range(len(x)-n+1)]
+
+
+# From here, it's easy to compute correctly predicted ngrams:
+
+def get_correct_ngrams(pred, targ, n, max_n=5000):
+    pred_grams,targ_grams = get_grams(pred, n, max_n=max_n),get_grams(targ, n, max_n=max_n)
+    pred_cnt,targ_cnt = Counter(pred_grams),Counter(targ_grams)
+    return sum([min(c, targ_cnt[g]) for g,c in pred_cnt.items()]),len(pred_grams)
+
+
+# The BLEU metric over two sentences can be written as:
+
+def sentence_bleu(pred, targ, max_n=5000):
+    corrects = [get_correct_ngrams(pred,targ,n,max_n=max_n) for n in range(1,5)]
+    n_precs = [c/t for c,t in corrects]
+    len_penalty = exp(1 - len(targ)/len(pred)) if len(pred) < len(targ) else 1
+    return len_penalty * ((n_precs[0]*n_precs[1]*n_precs[2]*n_precs[3]) ** 0.25)
+
+
+# And the BLEU metric over a corpus is:
+
+def corpus_bleu(preds, targs, max_n=5000):
+    pred_len,targ_len,n_precs,counts = 0,0,[0]*4,[0]*4
+    for pred,targ in zip(preds,targs):
+        pred_len += len(pred)
+        targ_len += len(targ)
+        for i in range(4):
+            c,t = ngram_corrects(pred, targ, i+1, max_n=max_n)
+            n_precs[i] += c
+            counts[i] += t
+    n_precs = [c/t for c,t in zip(n_precs,counts)]
+    len_penalty = exp(1 - targ_len/pred_len) if pred_len < targ_len else 1
+    return len_penalty * ((n_precs[0]*n_precs[1]*n_precs[2]*n_precs[3]) ** 0.25)
+
+
+# This takes 11s to run on our validation set (instead of 5 hours), so we can even use it as a metric during training, we have to define it as a `Callback`
+
+class CorpusBLEU(Callback):
+    def __init__(self, vocab_sz):
+        self.vocab_sz = vocab_sz
+        self.name = 'bleu'
+    
+    def on_epoch_begin(self, **kwargs):
+        self.pred_len,self.targ_len,self.n_precs,self.counts = 0,0,[0]*4,[0]*4
+    
+    def on_batch_end(self, last_output, last_target, **kwargs):
+        last_output = last_output.argmax(dim=-1)
+        for pred,targ in zip(last_output.cpu().numpy(),last_target.cpu().numpy()):
+            self.pred_len += len(pred)
+            self.targ_len += len(targ)
+            for i in range(4):
+                c,t = get_correct_ngrams(pred, targ, i+1, max_n=self.vocab_sz)
+                self.n_precs[i] += c
+                self.counts[i] += t
+    
+    def on_epoch_end(self, last_metrics, **kwargs):
+        n_precs = [c/t for c,t in zip(n_precs,counts)]
+        len_penalty = exp(1 - targ_len/pred_len) if pred_len < targ_len else 1
+        bleu = len_penalty * ((n_precs[0]*n_precs[1]*n_precs[2]*n_precs[3]) ** 0.25)
+        return add_metrics(last_metrics, bleu)
diff --git a/nbs/dl2/cyclegan.ipynb b/nbs/dl2/cyclegan.ipynb
index bd0815b..2aee258 100644
--- ./nbs/dl2/cyclegan.ipynb
+++ ./nbs/dl2/cyclegan.ipynb
@@ -711,6 +711,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/cyclegan.py b/nbs/dl2/cyclegan.py
new file mode 100644
index 0000000..3c1b15f
--- /dev/null
+++ ./nbs/dl2/cyclegan.py
@@ -0,0 +1,404 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+from fastai.vision import *
+
+# ## Data
+
+# One-time download, uncomment the next cells to get the data.
+
+# +
+#path = Config().data_path()
+
+# +
+# #! wget https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip -P {path}
+# #! unzip -q -n {path}/horse2zebra.zip -d {path}
+# #! rm {path}/horse2zebra.zip
+# -
+
+path = Config().data_path()/'horse2zebra'
+path.ls()
+
+
+# See [this tutorial](https://docs.fast.ai/tutorial.itemlist.html) for a detailed walkthrough of how/why this custom `ItemList` was created.
+
+class ImageTuple(ItemBase):
+    def __init__(self, img1, img2):
+        self.img1,self.img2 = img1,img2
+        self.obj,self.data = (img1,img2),[-1+2*img1.data,-1+2*img2.data]
+    
+    def apply_tfms(self, tfms, **kwargs):
+        self.img1 = self.img1.apply_tfms(tfms, **kwargs)
+        self.img2 = self.img2.apply_tfms(tfms, **kwargs)
+        return self
+    
+    def to_one(self): return Image(0.5+torch.cat(self.data,2)/2)
+
+
+class TargetTupleList(ItemList):
+    def reconstruct(self, t:Tensor): 
+        if len(t.size()) == 0: return t
+        return ImageTuple(Image(t[0]/2+0.5),Image(t[1]/2+0.5))
+
+
+class ImageTupleList(ImageList):
+    _label_cls=TargetTupleList
+    def __init__(self, items, itemsB=None, **kwargs):
+        self.itemsB = itemsB
+        super().__init__(items, **kwargs)
+    
+    def new(self, items, **kwargs):
+        return super().new(items, itemsB=self.itemsB, **kwargs)
+    
+    def get(self, i):
+        img1 = super().get(i)
+        fn = self.itemsB[random.randint(0, len(self.itemsB)-1)]
+        return ImageTuple(img1, open_image(fn))
+    
+    def reconstruct(self, t:Tensor): 
+        return ImageTuple(Image(t[0]/2+0.5),Image(t[1]/2+0.5))
+    
+    @classmethod
+    def from_folders(cls, path, folderA, folderB, **kwargs):
+        itemsB = ImageList.from_folder(path/folderB).items
+        res = super().from_folder(path/folderA, itemsB=itemsB, **kwargs)
+        res.path = path
+        return res
+    
+    def show_xys(self, xs, ys, figsize:Tuple[int,int]=(12,6), **kwargs):
+        "Show the `xs` and `ys` on a figure of `figsize`. `kwargs` are passed to the show method."
+        rows = int(math.sqrt(len(xs)))
+        fig, axs = plt.subplots(rows,rows,figsize=figsize)
+        for i, ax in enumerate(axs.flatten() if rows > 1 else [axs]):
+            xs[i].to_one().show(ax=ax, **kwargs)
+        plt.tight_layout()
+
+    def show_xyzs(self, xs, ys, zs, figsize:Tuple[int,int]=None, **kwargs):
+        """Show `xs` (inputs), `ys` (targets) and `zs` (predictions) on a figure of `figsize`.
+        `kwargs` are passed to the show method."""
+        figsize = ifnone(figsize, (12,3*len(xs)))
+        fig,axs = plt.subplots(len(xs), 2, figsize=figsize)
+        fig.suptitle('Ground truth / Predictions', weight='bold', size=14)
+        for i,(x,z) in enumerate(zip(xs,zs)):
+            x.to_one().show(ax=axs[i,0], **kwargs)
+            z.to_one().show(ax=axs[i,1], **kwargs)
+
+
+data = (ImageTupleList.from_folders(path, 'trainA', 'trainB')
+                      .split_none()
+                      .label_empty()
+                      .transform(get_transforms(), size=128)
+                      .databunch(bs=4))
+
+data.show_batch(rows=2)
+
+
+# ## Models
+
+# We use the models that were introduced in the [cycleGAN paper](https://arxiv.org/abs/1703.10593).
+
+def convT_norm_relu(ch_in:int, ch_out:int, norm_layer:nn.Module, ks:int=3, stride:int=2, bias:bool=True):
+    return [nn.ConvTranspose2d(ch_in, ch_out, kernel_size=ks, stride=stride, padding=1, output_padding=1, bias=bias),
+            norm_layer(ch_out), nn.ReLU(True)]
+
+
+def pad_conv_norm_relu(ch_in:int, ch_out:int, pad_mode:str, norm_layer:nn.Module, ks:int=3, bias:bool=True, 
+                       pad=1, stride:int=1, activ:bool=True, init:Callable=nn.init.kaiming_normal_)->List[nn.Module]:
+    layers = []
+    if pad_mode == 'reflection': layers.append(nn.ReflectionPad2d(pad))
+    elif pad_mode == 'border':   layers.append(nn.ReplicationPad2d(pad))
+    p = pad if pad_mode == 'zeros' else 0
+    conv = nn.Conv2d(ch_in, ch_out, kernel_size=ks, padding=p, stride=stride, bias=bias)
+    if init:
+        init(conv.weight)
+        if hasattr(conv, 'bias') and hasattr(conv.bias, 'data'): conv.bias.data.fill_(0.)
+    layers += [conv, norm_layer(ch_out)]
+    if activ: layers.append(nn.ReLU(inplace=True))
+    return layers
+
+
+class ResnetBlock(nn.Module):
+    def __init__(self, dim:int, pad_mode:str='reflection', norm_layer:nn.Module=None, dropout:float=0., bias:bool=True):
+        super().__init__()
+        assert pad_mode in ['zeros', 'reflection', 'border'], f'padding {pad_mode} not implemented.'
+        norm_layer = ifnone(norm_layer, nn.InstanceNorm2d)
+        layers = pad_conv_norm_relu(dim, dim, pad_mode, norm_layer, bias=bias)
+        if dropout != 0: layers.append(nn.Dropout(dropout))
+        layers += pad_conv_norm_relu(dim, dim, pad_mode, norm_layer, bias=bias, activ=False)
+        self.conv_block = nn.Sequential(*layers)
+
+    def forward(self, x): return x + self.conv_block(x)
+
+
+def resnet_generator(ch_in:int, ch_out:int, n_ftrs:int=64, norm_layer:nn.Module=None, 
+                     dropout:float=0., n_blocks:int=6, pad_mode:str='reflection')->nn.Module:
+    norm_layer = ifnone(norm_layer, nn.InstanceNorm2d)
+    bias = (norm_layer == nn.InstanceNorm2d)
+    layers = pad_conv_norm_relu(ch_in, n_ftrs, 'reflection', norm_layer, pad=3, ks=7, bias=bias)
+    for i in range(2):
+        layers += pad_conv_norm_relu(n_ftrs, n_ftrs *2, 'zeros', norm_layer, stride=2, bias=bias)
+        n_ftrs *= 2
+    layers += [ResnetBlock(n_ftrs, pad_mode, norm_layer, dropout, bias) for _ in range(n_blocks)]
+    for i in range(2):
+        layers += convT_norm_relu(n_ftrs, n_ftrs//2, norm_layer, bias=bias)
+        n_ftrs //= 2
+    layers += [nn.ReflectionPad2d(3), nn.Conv2d(n_ftrs, ch_out, kernel_size=7, padding=0), nn.Tanh()]
+    return nn.Sequential(*layers)
+
+
+resnet_generator(3, 3)
+
+
+def conv_norm_lr(ch_in:int, ch_out:int, norm_layer:nn.Module=None, ks:int=3, bias:bool=True, pad:int=1, stride:int=1, 
+                 activ:bool=True, slope:float=0.2, init:Callable=nn.init.kaiming_normal_)->List[nn.Module]:
+    conv = nn.Conv2d(ch_in, ch_out, kernel_size=ks, padding=pad, stride=stride, bias=bias)
+    if init:
+        init(conv.weight)
+        if hasattr(conv, 'bias') and hasattr(conv.bias, 'data'): conv.bias.data.fill_(0.)
+    layers = [conv]
+    if norm_layer is not None: layers.append(norm_layer(ch_out))
+    if activ: layers.append(nn.LeakyReLU(slope, inplace=True))
+    return layers
+
+
+def discriminator(ch_in:int, n_ftrs:int=64, n_layers:int=3, norm_layer:nn.Module=None, sigmoid:bool=False)->nn.Module:
+    norm_layer = ifnone(norm_layer, nn.InstanceNorm2d)
+    bias = (norm_layer == nn.InstanceNorm2d)
+    layers = conv_norm_lr(ch_in, n_ftrs, ks=4, stride=2, pad=1)
+    for i in range(n_layers-1):
+        new_ftrs = 2*n_ftrs if i <= 3 else n_ftrs
+        layers += conv_norm_lr(n_ftrs, new_ftrs, norm_layer, ks=4, stride=2, pad=1, bias=bias)
+        n_ftrs = new_ftrs
+    new_ftrs = 2*n_ftrs if n_layers <=3 else n_ftrs
+    layers += conv_norm_lr(n_ftrs, new_ftrs, norm_layer, ks=4, stride=1, pad=1, bias=bias)
+    layers.append(nn.Conv2d(new_ftrs, 1, kernel_size=4, stride=1, padding=1))
+    if sigmoid: layers.append(nn.Sigmoid())
+    return nn.Sequential(*layers)
+
+
+discriminator(3)
+
+
+# We group two discriminators and two generators in a single model, then a `Callback` will take care of training them properly.
+
+class CycleGAN(nn.Module):
+    
+    def __init__(self, ch_in:int, ch_out:int, n_features:int=64, disc_layers:int=3, gen_blocks:int=6, lsgan:bool=True, 
+                 drop:float=0., norm_layer:nn.Module=None):
+        super().__init__()
+        self.D_A = discriminator(ch_in, n_features, disc_layers, norm_layer, sigmoid=not lsgan)
+        self.D_B = discriminator(ch_in, n_features, disc_layers, norm_layer, sigmoid=not lsgan)
+        self.G_A = resnet_generator(ch_in, ch_out, n_features, norm_layer, drop, gen_blocks)
+        self.G_B = resnet_generator(ch_in, ch_out, n_features, norm_layer, drop, gen_blocks)
+        #G_A: takes real input B and generates fake input A
+        #G_B: takes real input A and generates fake input B
+        #D_A: trained to make the difference between real input A and fake input A
+        #D_B: trained to make the difference between real input B and fake input B
+    
+    def forward(self, real_A, real_B):
+        fake_A, fake_B = self.G_A(real_B), self.G_B(real_A)
+        if not self.training: return torch.cat([fake_A[:,None],fake_B[:,None]], 1)
+        idt_A, idt_B = self.G_A(real_A), self.G_B(real_B) #Needed for the identity loss during training.
+        return [fake_A, fake_B, idt_A, idt_B]
+
+
+# `AdaptiveLoss` is a wrapper around a PyTorch loss function to compare an output of any size with a single number (0. or 1.). It will generate a target with the same shape as the output. A discriminator returns a feature map, and we want it to predict zeros (or ones) for each feature.
+
+class AdaptiveLoss(nn.Module):
+    def __init__(self, crit):
+        super().__init__()
+        self.crit = crit
+    
+    def forward(self, output, target:bool, **kwargs):
+        targ = output.new_ones(*output.size()) if target else output.new_zeros(*output.size())
+        return self.crit(output, targ, **kwargs)
+
+
+# The main loss used to train the generators. It has three parts:
+# - the classic GAN loss: they must make the critics believe their images are real
+# - identity loss: if they are given an image from the set they are trying to imitate, they should return the same thing
+# - cycle loss: if an image from A goes through the generator that imitates B then through the generator that imitates A, it should be the same as the initial image. Same for B and switching the generators
+
+class CycleGanLoss(nn.Module):
+    
+    def __init__(self, cgan:nn.Module, lambda_A:float=10., lambda_B:float=10, lambda_idt:float=0.5, lsgan:bool=True):
+        super().__init__()
+        self.cgan,self.l_A,self.l_B,self.l_idt = cgan,lambda_A,lambda_B,lambda_idt
+        self.crit = AdaptiveLoss(F.mse_loss if lsgan else F.binary_cross_entropy)
+    
+    def set_input(self, input):
+        self.real_A,self.real_B = input
+    
+    def forward(self, output, target):
+        fake_A, fake_B, idt_A, idt_B = output
+        #Generators should return identity on the datasets they try to convert to
+        self.id_loss = self.l_idt * (self.l_A * F.l1_loss(idt_A, self.real_A) + self.l_B * F.l1_loss(idt_B, self.real_B))
+        #Generators are trained to trick the discriminators so the following should be ones
+        self.gen_loss = self.crit(self.cgan.D_A(fake_A), True) + self.crit(self.cgan.D_B(fake_B), True)
+        #Cycle loss
+        self.cyc_loss  = self.l_A * F.l1_loss(self.cgan.G_A(fake_B), self.real_A)
+        self.cyc_loss += self.l_B * F.l1_loss(self.cgan.G_B(fake_A), self.real_B)
+        return self.id_loss+self.gen_loss+self.cyc_loss
+
+
+# The main callback to train a cycle GAN. The training loop will train the generators (so `learn.opt` is given those parameters) while the critics are trained by the callback during `on_batch_end`.
+
+class CycleGANTrainer(LearnerCallback):
+    _order = -20 #Need to run before the Recorder
+    
+    def _set_trainable(self, D_A=False, D_B=False):
+        gen = (not D_A) and (not D_B)
+        requires_grad(self.learn.model.G_A, gen)
+        requires_grad(self.learn.model.G_B, gen)
+        requires_grad(self.learn.model.D_A, D_A)
+        requires_grad(self.learn.model.D_B, D_B)
+        if not gen:
+            self.opt_D_A.lr, self.opt_D_A.mom = self.learn.opt.lr, self.learn.opt.mom
+            self.opt_D_A.wd, self.opt_D_A.beta = self.learn.opt.wd, self.learn.opt.beta
+            self.opt_D_B.lr, self.opt_D_B.mom = self.learn.opt.lr, self.learn.opt.mom
+            self.opt_D_B.wd, self.opt_D_B.beta = self.learn.opt.wd, self.learn.opt.beta
+    
+    def on_train_begin(self, **kwargs):
+        self.G_A,self.G_B = self.learn.model.G_A,self.learn.model.G_B
+        self.D_A,self.D_B = self.learn.model.D_A,self.learn.model.D_B
+        self.crit = self.learn.loss_func.crit
+        if not getattr(self,'opt_G',None):
+            self.opt_G = self.learn.opt.new([nn.Sequential(*flatten_model(self.G_A), *flatten_model(self.G_B))])
+        else: 
+            self.opt_G.lr,self.opt_G.wd = self.opt.lr,self.opt.wd
+            self.opt_G.mom,self.opt_G.beta = self.opt.mom,self.opt.beta
+        if not getattr(self,'opt_D_A',None):
+            self.opt_D_A = self.learn.opt.new([nn.Sequential(*flatten_model(self.D_A))])
+        if not getattr(self,'opt_D_B',None):
+            self.opt_D_B = self.learn.opt.new([nn.Sequential(*flatten_model(self.D_B))])
+        self.learn.opt.opt = self.opt_G.opt
+        self._set_trainable()
+        self.id_smter,self.gen_smter,self.cyc_smter = SmoothenValue(0.98),SmoothenValue(0.98),SmoothenValue(0.98)
+        self.da_smter,self.db_smter = SmoothenValue(0.98),SmoothenValue(0.98)
+        self.recorder.add_metric_names(['id_loss', 'gen_loss', 'cyc_loss', 'D_A_loss', 'D_B_loss'])
+        
+    def on_batch_begin(self, last_input, **kwargs):
+        self.learn.loss_func.set_input(last_input)
+    
+    def on_backward_begin(self, **kwargs):
+        self.id_smter.add_value(self.loss_func.id_loss.detach().cpu())
+        self.gen_smter.add_value(self.loss_func.gen_loss.detach().cpu())
+        self.cyc_smter.add_value(self.loss_func.cyc_loss.detach().cpu())
+    
+    def on_batch_end(self, last_input, last_output, **kwargs):
+        self.G_A.zero_grad(); self.G_B.zero_grad()
+        fake_A, fake_B = last_output[0].detach(), last_output[1].detach()
+        real_A, real_B = last_input
+        self._set_trainable(D_A=True)
+        self.D_A.zero_grad()
+        loss_D_A = 0.5 * (self.crit(self.D_A(real_A), True) + self.crit(self.D_A(fake_A), False))
+        self.da_smter.add_value(loss_D_A.detach().cpu())
+        loss_D_A.backward()
+        self.opt_D_A.step()
+        self._set_trainable(D_B=True)
+        self.D_B.zero_grad()
+        loss_D_B = 0.5 * (self.crit(self.D_B(real_B), True) + self.crit(self.D_B(fake_B), False))
+        self.db_smter.add_value(loss_D_B.detach().cpu())
+        loss_D_B.backward()
+        self.opt_D_B.step()
+        self._set_trainable()
+        
+    def on_epoch_end(self, last_metrics, **kwargs):
+        return add_metrics(last_metrics, [s.smooth for s in [self.id_smter,self.gen_smter,self.cyc_smter,
+                                                             self.da_smter,self.db_smter]])
+
+
+# ## Training
+
+cycle_gan = CycleGAN(3,3, gen_blocks=9)
+learn = Learner(data, cycle_gan, loss_func=CycleGanLoss(cycle_gan), opt_func=partial(optim.Adam, betas=(0.5,0.99)),
+               callback_fns=[CycleGANTrainer])
+
+learn.lr_find()
+
+learn.recorder.plot()
+
+learn.fit(100, 1e-4)
+
+learn.save('100fit')
+
+learn = learn.load('100fit')
+
+# Let's look at some results using `Learner.show_results`.
+
+learn.show_results(ds_type=DatasetType.Train, rows=2)
+
+learn.show_results(ds_type=DatasetType.Train, rows=2)
+
+# Now let's go through all the images of the training set and find the ones that are the best converted (according to our critics) or the worst converted.
+
+len(learn.data.train_ds.items),len(learn.data.train_ds.itemsB)
+
+
+def get_batch(filenames, tfms, **kwargs):
+    samples = [open_image(fn) for fn in filenames]
+    for s in samples: s = s.apply_tfms(tfms, **kwargs)
+    batch = torch.stack([s.data for s in samples], 0).cuda()
+    return 2. * (batch - 0.5)
+
+
+fnames = learn.data.train_ds.items[:8]
+
+x = get_batch(fnames, get_transforms()[1], size=128)
+
+learn.model.eval()
+tfms = get_transforms()[1]
+bs = 16
+
+
+def get_losses(fnames, gen, crit, bs=16):
+    losses_in,losses_out = [],[]
+    with torch.no_grad():
+        for i in progress_bar(range(0, len(fnames), bs)):
+            xb = get_batch(fnames[i:i+bs], tfms, size=128)
+            fakes = gen(xb)
+            preds_in,preds_out = crit(xb),crit(fakes)
+            loss_in  = learn.loss_func.crit(preds_in, True,reduction='none')
+            loss_out = learn.loss_func.crit(preds_out,True,reduction='none')
+            losses_in.append(loss_in.view(loss_in.size(0),-1).mean(1))
+            losses_out.append(loss_out.view(loss_out.size(0),-1).mean(1))
+    return torch.cat(losses_in),torch.cat(losses_out)
+
+
+losses_A = get_losses(data.train_ds.x.items, learn.model.G_B, learn.model.D_B)
+
+losses_B = get_losses(data.train_ds.x.itemsB, learn.model.G_A, learn.model.D_A)
+
+
+def show_best(fnames, losses, gen, n=8):
+    sort_idx = losses.argsort().cpu()
+    _,axs = plt.subplots(n//2, 4, figsize=(12,2*n))
+    xb = get_batch(fnames[sort_idx][:n], tfms, size=128)
+    with torch.no_grad():
+        fakes = gen(xb)
+    xb,fakes = (1+xb.cpu())/2,(1+fakes.cpu())/2
+    for i in range(n):
+        axs.flatten()[2*i].imshow(xb[i].permute(1,2,0))
+        axs.flatten()[2*i].axis('off')
+        axs.flatten()[2*i+1].imshow(fakes[i].permute(1,2,0))
+        axs.flatten()[2*i+1].set_title(losses[sort_idx][i].item())
+        axs.flatten()[2*i+1].axis('off')
+
+
+show_best(data.train_ds.x.items, losses_A[1], learn.model.G_B)
+
+show_best(data.train_ds.x.itemsB, losses_B[1], learn.model.G_A)
+
+
diff --git a/nbs/dl2/cyclegan_ws.ipynb b/nbs/dl2/cyclegan_ws.ipynb
index b2f05cd..0cba645 100644
--- ./nbs/dl2/cyclegan_ws.ipynb
+++ ./nbs/dl2/cyclegan_ws.ipynb
@@ -720,6 +720,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/cyclegan_ws.py b/nbs/dl2/cyclegan_ws.py
new file mode 100644
index 0000000..800f058
--- /dev/null
+++ ./nbs/dl2/cyclegan_ws.py
@@ -0,0 +1,406 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+from fastai.vision import *
+
+# ## Data
+
+# One-time download, uncomment the next cells to get the data.
+
+# +
+#path = Config().data_path()
+
+# +
+# #! wget https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/summer2winter_yosemite.zip -P {path}
+# #! unzip -q -n {path}/summer2winter_yosemite.zip -d {path}
+# #! rm {path}/summer2winter_yosemite.zip
+# -
+
+path = Config().data_path()/'summer2winter_yosemite'
+path.ls()
+
+
+# See [this tutorial](https://docs.fast.ai/tutorial.itemlist.html) for a detailed walkthrough of how/why this custom `ItemList` was created.
+
+class ImageTuple(ItemBase):
+    def __init__(self, img1, img2):
+        self.img1,self.img2 = img1,img2
+        self.obj,self.data = (img1,img2),[-1+2*img1.data,-1+2*img2.data]
+    
+    def apply_tfms(self, tfms, **kwargs):
+        self.img1 = self.img1.apply_tfms(tfms, **kwargs)
+        self.img2 = self.img2.apply_tfms(tfms, **kwargs)
+        return self
+    
+    def to_one(self): return Image(0.5+torch.cat(self.data,2)/2)
+
+
+class TargetTupleList(ItemList):
+    def reconstruct(self, t:Tensor): 
+        if len(t.size()) == 0: return t
+        return ImageTuple(Image(t[0]/2+0.5),Image(t[1]/2+0.5))
+
+
+class ImageTupleList(ImageList):
+    _label_cls=TargetTupleList
+    def __init__(self, items, itemsB=None, **kwargs):
+        self.itemsB = itemsB
+        super().__init__(items, **kwargs)
+    
+    def new(self, items, **kwargs):
+        return super().new(items, itemsB=self.itemsB, **kwargs)
+    
+    def get(self, i):
+        img1 = super().get(i)
+        fn = self.itemsB[random.randint(0, len(self.itemsB)-1)]
+        return ImageTuple(img1, open_image(fn))
+    
+    def reconstruct(self, t:Tensor): 
+        return ImageTuple(Image(t[0]/2+0.5),Image(t[1]/2+0.5))
+    
+    @classmethod
+    def from_folders(cls, path, folderA, folderB, **kwargs):
+        itemsB = ImageList.from_folder(path/folderB).items
+        res = super().from_folder(path/folderA, itemsB=itemsB, **kwargs)
+        res.path = path
+        return res
+    
+    def show_xys(self, xs, ys, figsize:Tuple[int,int]=(12,6), **kwargs):
+        "Show the `xs` and `ys` on a figure of `figsize`. `kwargs` are passed to the show method."
+        rows = int(math.sqrt(len(xs)))
+        fig, axs = plt.subplots(rows,rows,figsize=figsize)
+        for i, ax in enumerate(axs.flatten() if rows > 1 else [axs]):
+            xs[i].to_one().show(ax=ax, **kwargs)
+        plt.tight_layout()
+
+    def show_xyzs(self, xs, ys, zs, figsize:Tuple[int,int]=None, **kwargs):
+        """Show `xs` (inputs), `ys` (targets) and `zs` (predictions) on a figure of `figsize`.
+        `kwargs` are passed to the show method."""
+        figsize = ifnone(figsize, (12,3*len(xs)))
+        fig,axs = plt.subplots(len(xs), 2, figsize=figsize)
+        fig.suptitle('Ground truth / Predictions', weight='bold', size=14)
+        for i,(x,z) in enumerate(zip(xs,zs)):
+            x.to_one().show(ax=axs[i,0], **kwargs)
+            z.to_one().show(ax=axs[i,1], **kwargs)
+
+
+data = (ImageTupleList.from_folders(path, 'trainA', 'trainB')
+                      .split_none()
+                      .label_empty()
+                      .transform(get_transforms(), size=128)
+                      .databunch(bs=4))
+
+data.show_batch(rows=2)
+
+
+# ## Models
+
+# We use the models that were introduced in the [cycleGAN paper](https://arxiv.org/abs/1703.10593).
+
+def convT_norm_relu(ch_in:int, ch_out:int, norm_layer:nn.Module, ks:int=3, stride:int=2, bias:bool=True):
+    return [nn.ConvTranspose2d(ch_in, ch_out, kernel_size=ks, stride=stride, padding=1, output_padding=1, bias=bias),
+            norm_layer(ch_out), nn.ReLU(True)]
+
+
+def pad_conv_norm_relu(ch_in:int, ch_out:int, pad_mode:str, norm_layer:nn.Module, ks:int=3, bias:bool=True, 
+                       pad=1, stride:int=1, activ:bool=True, init:Callable=nn.init.kaiming_normal_)->List[nn.Module]:
+    layers = []
+    if pad_mode == 'reflection': layers.append(nn.ReflectionPad2d(pad))
+    elif pad_mode == 'border':   layers.append(nn.ReplicationPad2d(pad))
+    p = pad if pad_mode == 'zeros' else 0
+    conv = nn.Conv2d(ch_in, ch_out, kernel_size=ks, padding=p, stride=stride, bias=bias)
+    if init:
+        init(conv.weight)
+        if hasattr(conv, 'bias') and hasattr(conv.bias, 'data'): conv.bias.data.fill_(0.)
+    layers += [conv, norm_layer(ch_out)]
+    if activ: layers.append(nn.ReLU(inplace=True))
+    return layers
+
+
+class ResnetBlock(nn.Module):
+    def __init__(self, dim:int, pad_mode:str='reflection', norm_layer:nn.Module=None, dropout:float=0., bias:bool=True):
+        super().__init__()
+        assert pad_mode in ['zeros', 'reflection', 'border'], f'padding {pad_mode} not implemented.'
+        norm_layer = ifnone(norm_layer, nn.InstanceNorm2d)
+        layers = pad_conv_norm_relu(dim, dim, pad_mode, norm_layer, bias=bias)
+        if dropout != 0: layers.append(nn.Dropout(dropout))
+        layers += pad_conv_norm_relu(dim, dim, pad_mode, norm_layer, bias=bias, activ=False)
+        self.conv_block = nn.Sequential(*layers)
+
+    def forward(self, x): return x + self.conv_block(x)
+
+
+def resnet_generator(ch_in:int, ch_out:int, n_ftrs:int=64, norm_layer:nn.Module=None, 
+                     dropout:float=0., n_blocks:int=6, pad_mode:str='reflection')->nn.Module:
+    norm_layer = ifnone(norm_layer, nn.InstanceNorm2d)
+    bias = (norm_layer == nn.InstanceNorm2d)
+    layers = pad_conv_norm_relu(ch_in, n_ftrs, 'reflection', norm_layer, pad=3, ks=7, bias=bias)
+    for i in range(2):
+        layers += pad_conv_norm_relu(n_ftrs, n_ftrs *2, 'zeros', norm_layer, stride=2, bias=bias)
+        n_ftrs *= 2
+    layers += [ResnetBlock(n_ftrs, pad_mode, norm_layer, dropout, bias) for _ in range(n_blocks)]
+    for i in range(2):
+        layers += convT_norm_relu(n_ftrs, n_ftrs//2, norm_layer, bias=bias)
+        n_ftrs //= 2
+    layers += [nn.ReflectionPad2d(3), nn.Conv2d(n_ftrs, ch_out, kernel_size=7, padding=0), nn.Tanh()]
+    return nn.Sequential(*layers)
+
+
+resnet_generator(3, 3)
+
+
+def conv_norm_lr(ch_in:int, ch_out:int, norm_layer:nn.Module=None, ks:int=3, bias:bool=True, pad:int=1, stride:int=1, 
+                 activ:bool=True, slope:float=0.2, init:Callable=nn.init.kaiming_normal_)->List[nn.Module]:
+    conv = nn.Conv2d(ch_in, ch_out, kernel_size=ks, padding=pad, stride=stride, bias=bias)
+    if init:
+        init(conv.weight)
+        if hasattr(conv, 'bias') and hasattr(conv.bias, 'data'): conv.bias.data.fill_(0.)
+    layers = [conv]
+    if norm_layer is not None: layers.append(norm_layer(ch_out))
+    if activ: layers.append(nn.LeakyReLU(slope, inplace=True))
+    return layers
+
+
+def discriminator(ch_in:int, n_ftrs:int=64, n_layers:int=3, norm_layer:nn.Module=None, sigmoid:bool=False)->nn.Module:
+    norm_layer = ifnone(norm_layer, nn.InstanceNorm2d)
+    bias = (norm_layer == nn.InstanceNorm2d)
+    layers = conv_norm_lr(ch_in, n_ftrs, ks=4, stride=2, pad=1)
+    for i in range(n_layers-1):
+        new_ftrs = 2*n_ftrs if i <= 3 else n_ftrs
+        layers += conv_norm_lr(n_ftrs, new_ftrs, norm_layer, ks=4, stride=2, pad=1, bias=bias)
+        n_ftrs = new_ftrs
+    new_ftrs = 2*n_ftrs if n_layers <=3 else n_ftrs
+    layers += conv_norm_lr(n_ftrs, new_ftrs, norm_layer, ks=4, stride=1, pad=1, bias=bias)
+    layers.append(nn.Conv2d(new_ftrs, 1, kernel_size=4, stride=1, padding=1))
+    if sigmoid: layers.append(nn.Sigmoid())
+    return nn.Sequential(*layers)
+
+
+discriminator(3)
+
+
+# We group two discriminators and two generators in a single model, then a `Callback` will take care of training them properly.
+
+class CycleGAN(nn.Module):
+    
+    def __init__(self, ch_in:int, ch_out:int, n_features:int=64, disc_layers:int=3, gen_blocks:int=6, lsgan:bool=True, 
+                 drop:float=0., norm_layer:nn.Module=None):
+        super().__init__()
+        self.D_A = discriminator(ch_in, n_features, disc_layers, norm_layer, sigmoid=not lsgan)
+        self.D_B = discriminator(ch_in, n_features, disc_layers, norm_layer, sigmoid=not lsgan)
+        self.G_A = resnet_generator(ch_in, ch_out, n_features, norm_layer, drop, gen_blocks)
+        self.G_B = resnet_generator(ch_in, ch_out, n_features, norm_layer, drop, gen_blocks)
+        #G_A: takes real input B and generates fake input A
+        #G_B: takes real input A and generates fake input B
+        #D_A: trained to make the difference between real input A and fake input A
+        #D_B: trained to make the difference between real input B and fake input B
+    
+    def forward(self, real_A, real_B):
+        fake_A, fake_B = self.G_A(real_B), self.G_B(real_A)
+        if not self.training: return torch.cat([fake_A[:,None],fake_B[:,None]], 1)
+        idt_A, idt_B = self.G_A(real_A), self.G_B(real_B) #Needed for the identity loss during training.
+        return [fake_A, fake_B, idt_A, idt_B]
+
+
+# `AdaptiveLoss` is a wrapper around a PyTorch loss function to compare an output of any size with a single number (0. or 1.). It will generate a target with the same shape as the output. A discriminator returns a feature map, and we want it to predict zeros (or ones) for each feature.
+
+class AdaptiveLoss(nn.Module):
+    def __init__(self, crit):
+        super().__init__()
+        self.crit = crit
+    
+    def forward(self, output, target:bool, **kwargs):
+        targ = output.new_ones(*output.size()) if target else output.new_zeros(*output.size())
+        return self.crit(output, targ, **kwargs)
+
+
+# The main loss used to train the generators. It has three parts:
+# - the classic GAN loss: they must make the critics believe their images are real
+# - identity loss: if they are given an image from the set they are trying to imitate, they should return the same thing
+# - cycle loss: if an image from A goes through the generator that imitates B then through the generator that imitates A, it should be the same as the initial image. Same for B and switching the generators
+
+class CycleGanLoss(nn.Module):
+    
+    def __init__(self, cgan:nn.Module, lambda_A:float=10., lambda_B:float=10, lambda_idt:float=0.5, lsgan:bool=True):
+        super().__init__()
+        self.cgan,self.l_A,self.l_B,self.l_idt = cgan,lambda_A,lambda_B,lambda_idt
+        self.crit = AdaptiveLoss(F.mse_loss if lsgan else F.binary_cross_entropy)
+    
+    def set_input(self, input):
+        self.real_A,self.real_B = input
+    
+    def forward(self, output, target):
+        fake_A, fake_B, idt_A, idt_B = output
+        #Generators should return identity on the datasets they try to convert to
+        self.id_loss = self.l_idt * (self.l_B * F.l1_loss(idt_A, self.real_B) + self.l_A * F.l1_loss(idt_B, self.real_A))
+        #Generators are trained to trick the discriminators so the following should be ones
+        self.gen_loss = self.crit(self.cgan.D_A(fake_A), True) + self.crit(self.cgan.D_B(fake_B), True)
+        #Cycle loss
+        self.cyc_loss  = self.l_A * F.l1_loss(self.cgan.G_A(fake_B), self.real_A)
+        self.cyc_loss += self.l_B * F.l1_loss(self.cgan.G_B(fake_A), self.real_B)
+        return self.id_loss+self.gen_loss+self.cyc_loss
+
+
+# The main callback to train a cycle GAN. The training loop will train the generators (so `learn.opt` is given those parameters) while the critics are trained by the callback during `on_batch_end`.
+
+class CycleGANTrainer(LearnerCallback):
+    _order = -20 #Need to run before the Recorder
+    
+    def _set_trainable(self, D_A=False, D_B=False):
+        gen = (not D_A) and (not D_B)
+        requires_grad(self.learn.model.G_A, gen)
+        requires_grad(self.learn.model.G_B, gen)
+        requires_grad(self.learn.model.D_A, D_A)
+        requires_grad(self.learn.model.D_B, D_B)
+        if not gen:
+            self.opt_D_A.lr, self.opt_D_A.mom = self.learn.opt.lr, self.learn.opt.mom
+            self.opt_D_A.wd, self.opt_D_A.beta = self.learn.opt.wd, self.learn.opt.beta
+            self.opt_D_B.lr, self.opt_D_B.mom = self.learn.opt.lr, self.learn.opt.mom
+            self.opt_D_B.wd, self.opt_D_B.beta = self.learn.opt.wd, self.learn.opt.beta
+    
+    def on_train_begin(self, **kwargs):
+        self.G_A,self.G_B = self.learn.model.G_A,self.learn.model.G_B
+        self.D_A,self.D_B = self.learn.model.D_A,self.learn.model.D_B
+        self.crit = self.learn.loss_func.crit
+        if not getattr(self,'opt_G',None):
+            self.opt_G = self.learn.opt.new([nn.Sequential(*flatten_model(self.G_A), *flatten_model(self.G_B))])
+        else: 
+            self.opt_G.lr,self.opt_G.wd = self.opt.lr,self.opt.wd
+            self.opt_G.mom,self.opt_G.beta = self.opt.mom,self.opt.beta
+        if not getattr(self,'opt_D_A',None):
+            self.opt_D_A = self.learn.opt.new([nn.Sequential(*flatten_model(self.D_A))])
+        if not getattr(self,'opt_D_B',None):
+            self.opt_D_B = self.learn.opt.new([nn.Sequential(*flatten_model(self.D_B))])
+        self.learn.opt.opt = self.opt_G.opt
+        self._set_trainable()
+        self.id_smter,self.gen_smter,self.cyc_smter = SmoothenValue(0.98),SmoothenValue(0.98),SmoothenValue(0.98)
+        self.da_smter,self.db_smter = SmoothenValue(0.98),SmoothenValue(0.98)
+        self.recorder.add_metric_names(['id_loss', 'gen_loss', 'cyc_loss', 'D_A_loss', 'D_B_loss'])
+        
+    def on_batch_begin(self, last_input, **kwargs):
+        self.learn.loss_func.set_input(last_input)
+    
+    def on_backward_begin(self, **kwargs):
+        self.id_smter.add_value(self.loss_func.id_loss.detach().cpu())
+        self.gen_smter.add_value(self.loss_func.gen_loss.detach().cpu())
+        self.cyc_smter.add_value(self.loss_func.cyc_loss.detach().cpu())
+    
+    def on_batch_end(self, last_input, last_output, **kwargs):
+        self.G_A.zero_grad(); self.G_B.zero_grad()
+        fake_A, fake_B = last_output[0].detach(), last_output[1].detach()
+        real_A, real_B = last_input
+        self._set_trainable(D_A=True)
+        self.D_A.zero_grad()
+        loss_D_A = 0.5 * (self.crit(self.D_A(real_A), True) + self.crit(self.D_A(fake_A), False))
+        self.da_smter.add_value(loss_D_A.detach().cpu())
+        loss_D_A.backward()
+        self.opt_D_A.step()
+        self._set_trainable(D_B=True)
+        self.D_B.zero_grad()
+        loss_D_B = 0.5 * (self.crit(self.D_B(real_B), True) + self.crit(self.D_B(fake_B), False))
+        self.db_smter.add_value(loss_D_B.detach().cpu())
+        loss_D_B.backward()
+        self.opt_D_B.step()
+        self._set_trainable()
+        
+    def on_epoch_end(self, last_metrics, **kwargs):
+        return add_metrics(last_metrics, [s.smooth for s in [self.id_smter,self.gen_smter,self.cyc_smter,
+                                                             self.da_smter,self.db_smter]])
+
+
+# ## Training
+
+cycle_gan = CycleGAN(3,3, gen_blocks=9)
+learn = Learner(data, cycle_gan, loss_func=CycleGanLoss(cycle_gan), opt_func=partial(optim.Adam, betas=(0.5,0.99)),
+               callback_fns=[CycleGANTrainer])
+
+learn.lr_find()
+
+learn.recorder.plot()
+
+learn.fit(100, 1e-4)
+
+learn.save('100fit')
+
+learn = learn.load('100fit')
+
+# Let's look at some results using `Learner.show_results`.
+
+learn.show_results(ds_type=DatasetType.Train, rows=10)
+
+learn.show_results(ds_type=DatasetType.Train, rows=10)
+
+# Now let's go through all the images of the training set and find the ones that are the best converted (according to our critics) or the worst converted.
+
+len(learn.data.train_ds.items),len(learn.data.train_ds.itemsB)
+
+
+def get_batch(filenames, tfms, **kwargs):
+    samples = [open_image(fn) for fn in filenames]
+    for s in samples: s = s.apply_tfms(tfms, **kwargs)
+    batch = torch.stack([s.data for s in samples], 0).cuda()
+    return 2. * (batch - 0.5)
+
+
+fnames = learn.data.train_ds.items[:8]
+
+x = get_batch(fnames, get_transforms()[1], size=128)
+
+learn.model.eval()
+tfms = get_transforms()[1]
+bs = 16
+
+
+def get_losses(fnames, gen, crit, bs=16):
+    losses_in,losses_out = [],[]
+    with torch.no_grad():
+        for i in progress_bar(range(0, len(fnames), bs)):
+            xb = get_batch(fnames[i:i+bs], tfms, size=128)
+            fakes = gen(xb)
+            preds_in,preds_out = crit(xb),crit(fakes)
+            loss_in  = learn.loss_func.crit(preds_in, True,reduction='none')
+            loss_out = learn.loss_func.crit(preds_out,True,reduction='none')
+            losses_in.append(loss_in.view(loss_in.size(0),-1).mean(1))
+            losses_out.append(loss_out.view(loss_out.size(0),-1).mean(1))
+    return torch.cat(losses_in),torch.cat(losses_out)
+
+
+losses_A = get_losses(data.train_ds.x.items, learn.model.G_B, learn.model.D_B)
+
+losses_B = get_losses(data.train_ds.x.itemsB, learn.model.G_A, learn.model.D_A)
+
+
+def show_best(fnames, losses, gen, n=8):
+    sort_idx = losses.argsort().cpu()
+    _,axs = plt.subplots(n//2, 4, figsize=(12,2*n))
+    xb = get_batch(fnames[sort_idx][:n], tfms, size=128)
+    with torch.no_grad():
+        fakes = gen(xb)
+    xb,fakes = (1+xb.cpu())/2,(1+fakes.cpu())/2
+    for i in range(n):
+        axs.flatten()[2*i].imshow(xb[i].permute(1,2,0))
+        axs.flatten()[2*i].axis('off')
+        axs.flatten()[2*i+1].imshow(fakes[i].permute(1,2,0))
+        axs.flatten()[2*i+1].set_title(losses[sort_idx][i].item())
+        axs.flatten()[2*i+1].axis('off')
+
+
+show_best(data.train_ds.x.items, losses_A[1], learn.model.G_B)
+
+show_best(data.train_ds.x.itemsB, losses_B[1], learn.model.G_A)
+
+show_best(data.train_ds.x.items, losses_A[1]-losses_A[0], learn.model.G_B)
+
+
diff --git a/nbs/dl2/devise.ipynb b/nbs/dl2/devise.ipynb
index acddd0e..cbafd4f 100644
--- ./nbs/dl2/devise.ipynb
+++ ./nbs/dl2/devise.ipynb
@@ -860,6 +860,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/devise.py b/nbs/dl2/devise.py
new file mode 100644
index 0000000..c7ade35
--- /dev/null
+++ ./nbs/dl2/devise.py
@@ -0,0 +1,310 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+from fastai.vision import *
+
+# ## Data
+
+path = Config().data_path()/'imagenet'
+
+# ### Inputs: precomputed activations
+
+# We will build a model on the whole of ImageNet, so we will compute once and for all the activations for the whole training and validation set. We use `presize` to set the images to 224 x 224 by just using PIL.
+
+src = (ImageList.from_folder(path)
+          .split_by_folder()
+          .label_from_folder())
+
+data = src.presize(size=224).databunch(bs=256).normalize(imagenet_stats)
+
+# This is the pretrained resnet50 with concat pool and flatten:
+
+body = create_body(models.resnet50)
+layers = list(body.children())
+layers += [AdaptiveConcatPool2d(), Flatten()]   
+body = nn.Sequential(*layers).to(defaults.device)
+
+# We will use bcolz to store our activations in an array that's saved to memory (all won't fit in RAM). Install with 
+# ```
+# pip install -U bcolz
+# ```
+
+import bcolz
+
+tmp_path = path/'tmp'
+
+
+# +
+#To clean-up previous tries
+#shutil.rmtree(tmp_path)
+# -
+
+# Those functions will store the precomputed activations in `tmp_path`.
+
+def precompute_activations_dl(dl, model, path:Path, force:bool=False):
+    model.eval()
+    if os.path.exists(path) and not force: return
+    arr = bcolz.carray(np.zeros((0,4096), np.float32), chunklen=1, mode='w', rootdir=path)
+    with torch.no_grad():
+        for x,y in progress_bar(dl):
+            z = model(x)
+            arr.append(z.cpu().numpy())
+            arr.flush()
+
+
+def precompute_activations(data, model, path:Path, force:bool=False):
+    os.makedirs(path, exist_ok=True)
+    precompute_activations_dl(data.fix_dl,   model, path/'train', force=force) #Use fix_dl and not train_dl for shuffle=False
+    precompute_activations_dl(data.valid_dl, model, path/'valid', force=force)
+
+
+precompute_activations(data, body, tmp_path)
+
+# Save the labels and the filenames in the same order as our activations.
+
+np.save(tmp_path/'trn_lbl.npy', data.train_ds.y.items)
+np.save(tmp_path/'val_lbl.npy', data.valid_ds.y.items)
+save_texts(tmp_path/'classes.txt', data.train_ds.classes)
+
+np.save(tmp_path/'trn_names.npy', data.train_ds.x.items)
+np.save(tmp_path/'val_names.npy', data.valid_ds.x.items)
+
+
+# To load our precomputed activations, we'll use the following `ItemList`
+
+class BcolzItemList(ItemList):
+    def __init__(self, path, **kwargs):
+        self.arr = bcolz.open(path)
+        super().__init__(range(len(self.arr)), **kwargs)
+    
+    def get(self, i): return self.arr[i]
+
+
+src = ItemLists(path, BcolzItemList(path/'tmp'/'train'), BcolzItemList(path/'tmp'/'valid'))
+
+# ### Targets: word vectors
+
+# We build a regression model that has to predict a vector from the image features. We need to associate a word vector to each one of our 1000 classes.
+
+classes = loadtxt_str(tmp_path/'classes.txt')
+
+classes, len(classes)
+
+# The labels in imagenet are codes that come from [wordnet](https://wordnet.princeton.edu/). So let's download the corresponding dictionary.
+
+WORDNET = 'classids.txt'
+download_url(f'http://files.fast.ai/data/{WORDNET}', path/'tmp'/WORDNET)
+
+class_ids = loadtxt_str(path/'tmp'/WORDNET)
+class_ids = dict([l.strip().split() for l in class_ids])
+
+named_classes = [class_ids[c] for c in classes]
+named_classes[:10]
+
+# We will train our model to predict not the label of its class, but the corresponding pretrained vector. There are plenty of word embeddings available, here we will use fastText.
+#
+# To install fastText:
+# ```
+# $ git clone https://github.com/facebookresearch/fastText.git
+# $ cd fastText
+# $ pip install .
+# ```
+#
+# To download the english embeddings:
+#
+# ```
+# $ wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz
+# ```
+
+import fasttext as ft
+en_vecs = ft.load_model(str((path/'cc.en.300.bin')))
+
+# A lot of our classes are actually composed of several words separated by a `_`. The pretrained word vectors from fastText won't know them directly, but it can still compute a word vector to represent them:
+
+vec_dog = en_vecs.get_sentence_vector('dog')
+vec_lab = en_vecs.get_sentence_vector('labrador')
+vec_gor = en_vecs.get_sentence_vector('golden retriever')
+vec_ban = en_vecs.get_sentence_vector('banana')
+
+# To check if two word vectors are close or not, we use cosine similarity.
+
+F.cosine_similarity(tensor(vec_dog[None]), tensor(vec_lab[None]))
+
+F.cosine_similarity(tensor(vec_dog[None]), tensor(vec_ban[None]))
+
+F.cosine_similarity(tensor(vec_lab[None]), tensor(vec_ban[None]))
+
+F.cosine_similarity(tensor(vec_dog[None]), tensor(vec_gor[None]))
+
+F.cosine_similarity(tensor(vec_lab[None]), tensor(vec_gor[None]))
+
+# So let's grab all the word vectors for all our classes:
+
+vecs = []
+for n in named_classes:
+    vecs.append(en_vecs.get_sentence_vector(n.replace('_', ' ')))
+
+# Then we label each feature map with the word vector of its target.
+
+train_labels = np.load(tmp_path/'trn_lbl.npy')
+valid_labels = np.load(tmp_path/'val_lbl.npy')
+train_vecs = [vecs[l] for l in train_labels]
+valid_vecs = [vecs[l] for l in valid_labels]
+
+# We use our custom `BcolzItemList` to gather the data:
+
+src = ItemLists(path, BcolzItemList(tmp_path/'train'), BcolzItemList(tmp_path/'valid'))
+src = src.label_from_lists(train_vecs, valid_vecs, label_cls=FloatList)
+
+data = src.databunch(bs=512, num_workers=16)
+
+model = create_head(4096, data.c, lin_ftrs = [1024], ps=[0.2,0.2])
+model = nn.Sequential(*list(model.children())[2:])
+
+
+def cos_loss(inp,targ): return 1 - F.cosine_similarity(inp,targ).mean()
+
+
+learn = Learner(data, model, loss_func=cos_loss)
+
+learn.lr_find()
+
+learn.recorder.plot()
+
+learn.fit_one_cycle(15,3e-2)
+
+learn.save('fit')
+
+learn.model.eval()
+preds = []
+with torch.no_grad():
+    for x,y in progress_bar(learn.data.fix_dl):
+        preds.append(learn.model(x).cpu().numpy())
+    for x,y in progress_bar(learn.data.valid_dl):
+        preds.append(learn.model(x).cpu().numpy())
+
+preds = np.concatenate(preds, 0)
+
+np.save(path/'preds.npy', preds)
+
+# ### Looking at predicted tags in image classes
+
+# Now we will check, for one given image, what are the word vectors that are the closes to it. To compute this very quickly, we use `nmslib` which is very fast (pip install nmslib).
+
+# +
+import nmslib
+
+def create_index(a):
+    index = nmslib.init(space='angulardist')
+    index.addDataPointBatch(a)
+    index.createIndex()
+    return index
+
+def get_knns(index, vecs):
+     return zip(*index.knnQueryBatch(vecs, k=10, num_threads=4))
+
+def get_knn(index, vec): return index.knnQuery(vec, k=10)
+
+
+# -
+
+# We first look in the word vectors of our given classes:
+
+nn_classes = create_index(vecs)
+
+valid_preds = preds[-len(data.valid_ds):]
+valid_names = np.load(tmp_path/'val_names.npy')
+
+idxs,dists = get_knns(nn_classes, valid_preds)
+
+ks = [0,10000,20000,30000]
+_,axs = plt.subplots(2,2,figsize=(12,8))
+for k,ax in zip(ks, axs.flatten()):
+    open_image(valid_names[k]).show(ax = ax)
+    title = ','.join([class_ids[classes[i]] for i in idxs[k][:3]])
+    title += f'\n{class_ids[classes[valid_labels[k]]]}'
+    ax.set_title(title)
+
+# ### Looking at predicted tags in all Wordnet
+
+# Now let's look at the words it finds in all Wordnet.
+
+words,wn_vecs = [],[]
+for k,n in class_ids.items():
+    words.append(n)
+    wn_vecs.append(en_vecs.get_sentence_vector(n.replace('_', ' ')))
+
+nn_wvs = create_index(wn_vecs)
+
+idxs,dists = get_knns(nn_wvs, valid_preds)
+
+ks = [0,10000,20000,30000]
+_,axs = plt.subplots(2,2,figsize=(12,8))
+for k,ax in zip(ks, axs.flatten()):
+    open_image(valid_names[k]).show(ax = ax)
+    title = ','.join([words[i] for i in idxs[k][:3]])
+    title += f'\n{class_ids[classes[valid_labels[k]]]}'
+    ax.set_title(title)
+
+# ### Text -> Image search
+
+# We can use the reverse approach: feed a word vector and find the image activations that match it the closest:
+
+nn_preds = create_index(valid_preds)
+
+
+def show_imgs_from_text(text):
+    vec = en_vecs.get_sentence_vector(text)
+    idxs,dists = get_knn(nn_preds, vec)
+    _,axs = plt.subplots(2,2,figsize=(12,8))
+    for i,ax in zip(idxs[:4], axs.flatten()):
+        open_image(valid_names[i]).show(ax = ax)
+
+
+# 'boat' isn't a label in ImageNet, yet if we ask the images whose vord vectors are the most similar to the word vector for boat...
+
+show_imgs_from_text('boat')
+
+# or even more precisely 'motor boat'
+
+show_imgs_from_text('motor boat')
+
+show_imgs_from_text('sail boat')
+
+# ### Image->image
+
+# We can also ask for the images with a word vector most similar to another image. This one was downloaded from Google and isn't in Imagenet.
+
+img = open_image('images/teddy_bear.jpg')
+img
+
+# To get the corresdponding vector, we need to feed it to the pretrained model (`body`, defined at the top) after normalizing it.
+
+img = img.data
+m,s = imagenet_stats
+x = (img - tensor(m)[:,None,None])/tensor(s)[:,None,None]
+
+activs = body.eval()(x[None].cuda())
+
+pred = learn.model.eval()(activs)
+
+pred = pred[0].detach().cpu().numpy()
+
+idxs,dists = get_knn(nn_preds, pred)
+_,axs = plt.subplots(2,2,figsize=(12,8))
+for i,ax in zip(idxs[:4], axs.flatten()):
+    open_image(valid_names[i]).show(ax = ax)
+
+
diff --git a/nbs/dl2/pascal.ipynb b/nbs/dl2/pascal.ipynb
index 0ce9613..644f47c 100644
--- ./nbs/dl2/pascal.ipynb
+++ ./nbs/dl2/pascal.ipynb
@@ -1665,6 +1665,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/pascal.py b/nbs/dl2/pascal.py
new file mode 100644
index 0000000..748f3c1
--- /dev/null
+++ ./nbs/dl2/pascal.py
@@ -0,0 +1,798 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+from fastai.vision import *
+
+# ## Data
+
+# We are going to use the [Pascal dataset](http://host.robots.ox.ac.uk/pascal/VOC/) for object detection. There is a version from 2007 and a bigger version from 2012. We'll use the 2007 version here. 
+
+path = untar_data(URLs.PASCAL_2007)
+
+# The annotations for the images are stored in json files that give the bounding boxes for each class.
+
+import json
+annots = json.load(open(path/'train.json'))
+
+annots.keys()
+
+annots['annotations'][0]
+
+# This first annotation is a bounding box on the image with id 12, and the corresponding object is the category with id 7. We can read the correspondance in the 'images' and the 'categories' keys.
+
+annots['categories']
+
+# There is a convenience method in fastai to extract all the annotations and map them with the right images/categories directly, as long as they are in the format we just saw (called the COCO format). 
+
+train_images, train_lbl_bbox = get_annotations(path/'train.json')
+val_images, val_lbl_bbox = get_annotations(path/'valid.json')
+#tst_images, tst_lbl_bbox = get_annotations(path/'test.json')
+
+# Here we will directly find the same image as before at the beginning of the training set, with the corresponding bounding box and category.
+
+train_images[0], train_lbl_bbox[0]
+
+# To see it, we open the image properly and we create an `ImageBBox` object from the list of bounding boxes. This will allow us to apply data augmentation to our bounding box. To create an `ImageBBox`, we need to give it the height and the width of the original picture, the list of bounding boxes, the list of category ids and the classes list (to map an id to a class).
+#
+# Here we don't have a class dictionary available (that will be done automatically behind the scenes with the data block API), so we just pass id 0 and `classes=['car']`.
+
+img = open_image(path/'train'/train_images[0])
+bbox = ImageBBox.create(*img.size, train_lbl_bbox[0][0], [0], classes=['car'])
+img.show(figsize=(6,4), y=bbox)
+
+# This works with one or several bounding boxes:
+
+train_images[1], train_lbl_bbox[1]
+
+img = open_image(path/'train'/train_images[1])
+bbox = ImageBBox.create(*img.size, train_lbl_bbox[1][0], [0, 1], classes=['person', 'horse'])
+img.show(figsize=(6,4), y=bbox)
+
+# And if we apply a transform to our image and the `ImageBBox` object, they stay aligned:
+
+img = img.rotate(-10)
+bbox = bbox.rotate(-10)
+img.show(figsize=(6,4), y=bbox)
+
+# We group all the image filenames and annotations together, to use the data block API to load the dataset in a `DataBunch`.
+
+images, lbl_bbox = train_images+val_images,train_lbl_bbox+val_lbl_bbox
+img2bbox = dict(zip(images, lbl_bbox))
+get_y_func = lambda o:img2bbox[o.name]
+
+
+def get_data(bs, size):
+    src = ObjectItemList.from_folder(path/'train')
+    src = src.split_by_files(val_images)
+    src = src.label_from_func(get_y_func)
+    src = src.transform(get_transforms(), size=size, tfm_y=True)
+    return src.databunch(path=path, bs=bs, collate_fn=bb_pad_collate)
+
+
+data = get_data(64,128)
+
+data.show_batch(rows=3)
+
+# ## Model
+
+# The architecture we will use is a [RetinaNet](https://arxiv.org/abs/1708.02002), which is based on a [Feature Pyramid Network](https://arxiv.org/abs/1612.03144). 
+#
+# ![Retina net](images/retinanet.png)
+#
+# This is a bit like a Unet in the sense we have a branch where the image is progressively reduced then another one where we upsample it again, and there are lateral connections, but we will use the feature maps produced at each level for our final predictions. Specifically, if we start with an image of size (256,256), the traditional resnet has intermediate features maps of sizes:
+# - C1 (128, 128)  
+# - C2 (64, 64)
+# - C3 (32, 32)
+# - C4 (16, 16)
+# - C5 (8, 8)
+# To which the authors add two other features maps C6 and C7 of sizes (4,4) and (2,2) by using stride-2 convolutions. (Note that the model requires an image size of 128 at the minimum because of this.)
+#
+# Then we have P7 = C7 and we go down from P7 to P2 by upsampling the result of the previous P-layer and adding a lateral connection. The idea is that the last feature map P7 will be responsible to detect big objects, while one like P3 will be responsible to detect smaller objects. 
+#
+# Each P-something feature map then goes through two subnet of four convolutional layers (with the same weights for all the feature maps), one that will be responsible for finding the category of the object and the other for drawing the bounding box. Each location in the feature map is assigned a given number of anchors (see below) so the classifier ends up with `n_anchors * n_classes` channels and the bounding box regressor with `n_anchors * 4` channels.
+
+#Grab the convenience functions that helps us buil the Unet
+from fastai.vision.models.unet import _get_sfs_idxs, model_sizes, hook_outputs
+
+
+class LateralUpsampleMerge(nn.Module):
+    "Merge the features coming from the downsample path (in `hook`) with the upsample path."
+    def __init__(self, ch, ch_lat, hook):
+        super().__init__()
+        self.hook = hook
+        self.conv_lat = conv2d(ch_lat, ch, ks=1, bias=True)
+    
+    def forward(self, x):
+        return self.conv_lat(self.hook.stored) + F.interpolate(x, self.hook.stored.shape[-2:], mode='nearest')
+
+
+class RetinaNet(nn.Module):
+    "Implements RetinaNet from https://arxiv.org/abs/1708.02002"
+    def __init__(self, encoder:nn.Module, n_classes, final_bias=0., chs=256, n_anchors=9, flatten=True):
+        super().__init__()
+        self.n_classes,self.flatten = n_classes,flatten
+        imsize = (256,256)
+        sfs_szs = model_sizes(encoder, size=imsize)
+        sfs_idxs = list(reversed(_get_sfs_idxs(sfs_szs)))
+        self.sfs = hook_outputs([encoder[i] for i in sfs_idxs])
+        self.encoder = encoder
+        self.c5top5 = conv2d(sfs_szs[-1][1], chs, ks=1, bias=True)
+        self.c5top6 = conv2d(sfs_szs[-1][1], chs, stride=2, bias=True)
+        self.p6top7 = nn.Sequential(nn.ReLU(), conv2d(chs, chs, stride=2, bias=True))
+        self.merges = nn.ModuleList([LateralUpsampleMerge(chs, sfs_szs[idx][1], hook) 
+                                     for idx,hook in zip(sfs_idxs[-2:-4:-1], self.sfs[-2:-4:-1])])
+        self.smoothers = nn.ModuleList([conv2d(chs, chs, 3, bias=True) for _ in range(3)])
+        self.classifier = self._head_subnet(n_classes, n_anchors, final_bias, chs=chs)
+        self.box_regressor = self._head_subnet(4, n_anchors, 0., chs=chs)
+        
+    def _head_subnet(self, n_classes, n_anchors, final_bias=0., n_conv=4, chs=256):
+        "Helper function to create one of the subnet for regression/classification."
+        layers = [conv_layer(chs, chs, bias=True, norm_type=None) for _ in range(n_conv)]
+        layers += [conv2d(chs, n_classes * n_anchors, bias=True)]
+        layers[-1].bias.data.zero_().add_(final_bias)
+        layers[-1].weight.data.fill_(0)
+        return nn.Sequential(*layers)
+    
+    def _apply_transpose(self, func, p_states, n_classes):
+        #Final result of the classifier/regressor is bs * (k * n_anchors) * h * w
+        #We make it bs * h * w * n_anchors * k then flatten in bs * -1 * k so we can contenate
+        #all the results in bs * anchors * k (the non flatten version is there for debugging only)
+        if not self.flatten: 
+            sizes = [[p.size(0), p.size(2), p.size(3)] for p in p_states]
+            return [func(p).permute(0,2,3,1).view(*sz,-1,n_classes) for p,sz in zip(p_states,sizes)]
+        else:
+            return torch.cat([func(p).permute(0,2,3,1).contiguous().view(p.size(0),-1,n_classes) for p in p_states],1)
+    
+    def forward(self, x):
+        c5 = self.encoder(x)
+        p_states = [self.c5top5(c5.clone()), self.c5top6(c5)]
+        p_states.append(self.p6top7(p_states[-1]))
+        for merge in self.merges: p_states = [merge(p_states[0])] + p_states
+        for i, smooth in enumerate(self.smoothers[:3]):
+            p_states[i] = smooth(p_states[i])
+        return [self._apply_transpose(self.classifier, p_states, self.n_classes), 
+                self._apply_transpose(self.box_regressor, p_states, 4),
+                [[p.size(2), p.size(3)] for p in p_states]]
+    
+    def __del__(self):
+        if hasattr(self, "sfs"): self.sfs.remove()
+
+
+# The model is a bit complex, but that's not the hardest part. It will spit out an absurdly high number of predictions: for the features P3 to P7 with an image size of 256, we have `32*32 + 16*16 + 8*8 + 4*4 +2*2` locations possible in one of the five feature maps, which gives 1,364 possible detections, multiplied by the number of anchors we choose to attribute to each location (9 below), which makes 12,276 possible hits.
+#
+# A lot of those aren't going to correspond to any object in the picture, and we need to somehow match all those predictions to either nothing or a given bounding box in the picture.
+
+# ## "Encore" boxes
+
+# If we look at the feature map of size `4*4`, we have 16 locations numbered like below: 
+
+torch.arange(0,16).long().view(4,4)
+
+
+# The most basic way to map one of these features with an actual area inside the image is to create the regular 4 by 4 grid. Our convention is that `y` is first (like in numpy or PyTorch), and that all coordinates are scaled from -1 to 1 (-1 being top/right, 1 being bottom/left). 
+
+def create_grid(size):
+    "Create a grid of a given `size`."
+    H, W = size if is_tuple(size) else (size,size)
+    grid = FloatTensor(H, W, 2)
+    linear_points = torch.linspace(-1+1/W, 1-1/W, W) if W > 1 else tensor([0.])
+    grid[:, :, 1] = torch.ger(torch.ones(H), linear_points).expand_as(grid[:, :, 0])
+    linear_points = torch.linspace(-1+1/H, 1-1/H, H) if H > 1 else tensor([0.])
+    grid[:, :, 0] = torch.ger(linear_points, torch.ones(W)).expand_as(grid[:, :, 1])
+    return grid.view(-1,2)
+
+
+# Let's use a helper function to draw those anchors:
+
+def show_anchors(ancs, size):
+    _,ax = plt.subplots(1,1, figsize=(5,5))
+    ax.set_xticks(np.linspace(-1,1, size[1]+1))
+    ax.set_yticks(np.linspace(-1,1, size[0]+1))
+    ax.grid()
+    ax.scatter(ancs[:,1], ancs[:,0]) #y is first
+    ax.set_yticklabels([])
+    ax.set_xticklabels([])
+    ax.set_xlim(-1,1)
+    ax.set_ylim(1,-1) #-1 is top, 1 is bottom
+    for i, (x, y) in enumerate(zip(ancs[:, 1], ancs[:, 0])): ax.annotate(i, xy = (x,y))
+
+
+size = (4,4)
+show_anchors(create_grid(size), size)
+
+
+# In practice, we use different ratios and scales of that basic grid to build our anchors, because bounding boxes aren't always a perfect square inside a grid. 
+
+def create_anchors(sizes, ratios, scales, flatten=True):
+    "Create anchor of `sizes`, `ratios` and `scales`."
+    aspects = [[[s*math.sqrt(r), s*math.sqrt(1/r)] for s in scales] for r in ratios]
+    aspects = torch.tensor(aspects).view(-1,2)
+    anchors = []
+    for h,w in sizes:
+        #4 here to have the anchors overlap.
+        sized_aspects = 4 * (aspects * torch.tensor([2/h,2/w])).unsqueeze(0)
+        base_grid = create_grid((h,w)).unsqueeze(1)
+        n,a = base_grid.size(0),aspects.size(0)
+        ancs = torch.cat([base_grid.expand(n,a,2), sized_aspects.expand(n,a,2)], 2)
+        anchors.append(ancs.view(h,w,a,4))
+    return torch.cat([anc.view(-1,4) for anc in anchors],0) if flatten else anchors
+
+
+ratios = [1/2,1,2]
+scales = [1,2**(-1/3), 2**(-2/3)] 
+#Paper used [1,2**(1/3), 2**(2/3)] but a bigger size (600) too, so the largest feature map gave anchors that cover less of the image.
+sizes = [(2**i,2**i) for i in range(5)]
+sizes.reverse() #Predictions come in the order of the smallest feature map to the biggest
+anchors = create_anchors(sizes, ratios, scales)
+
+anchors.size()
+
+# That's a bit less than in our computation earlier, but this is because it's for the case of (128,128) images (sizes go from (1,1) to (32,32) instead of (2,2) to (64,64)).
+
+# +
+import matplotlib.cm as cmx
+import matplotlib.colors as mcolors
+from cycler import cycler
+
+def get_cmap(N):
+    color_norm  = mcolors.Normalize(vmin=0, vmax=N-1)
+    return cmx.ScalarMappable(norm=color_norm, cmap='Set3').to_rgba
+
+num_color = 12
+cmap = get_cmap(num_color)
+color_list = [cmap(float(x)) for x in range(num_color)]
+
+def draw_outline(o, lw):
+    o.set_path_effects([patheffects.Stroke(
+        linewidth=lw, foreground='black'), patheffects.Normal()])
+
+def draw_rect(ax, b, color='white'):
+    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor=color, lw=2))
+    draw_outline(patch, 4)
+
+def draw_text(ax, xy, txt, sz=14, color='white'):
+    text = ax.text(*xy, txt,
+        verticalalignment='top', color=color, fontsize=sz, weight='bold')
+    draw_outline(text, 1)
+
+
+# -
+
+def show_boxes(boxes):
+    "Show the `boxes` (size by 4)"
+    _, ax = plt.subplots(1,1, figsize=(5,5))
+    ax.set_xlim(-1,1)
+    ax.set_ylim(1,-1)
+    for i, bbox in enumerate(boxes):
+        bb = bbox.numpy()
+        rect = [bb[1]-bb[3]/2, bb[0]-bb[2]/2, bb[3], bb[2]]
+        draw_rect(ax, rect, color=color_list[i%num_color])
+        draw_text(ax, [bb[1]-bb[3]/2,bb[0]-bb[2]/2], str(i), color=color_list[i%num_color])
+
+
+# Here is an example of the 9 anchor boxes with different scales/ratios on one region of the image. Now imagine we have this at every location of each of the feature maps.
+
+show_boxes(anchors[900:909])
+
+
+# For each anchor, we have one class predicted by the classifier and 4 floats `p_y,p_x,p_h,p_w` predicted by the regressor. If the corresponding anchor as a center in `anc_y`, `anc_x` with dimensions `anc_h`, `anc_w`, the predicted bounding box has those characteristics:
+# ```
+# center = [p_y * anc_h + anc_y, p_x * anc_w + anc_x]
+# height = anc_h * exp(p_h)
+# width  = anc_w * exp(p_w)
+# ```
+# The idea is that a prediction of `(0,0,0,0)` corresponds to the anchor itself.
+#
+# The next function converts the activations of the model in bounding boxes.
+
+def activ_to_bbox(acts, anchors, flatten=True):
+    "Extrapolate bounding boxes on anchors from the model activations."
+    if flatten:
+        acts.mul_(acts.new_tensor([[0.1, 0.1, 0.2, 0.2]])) #Can't remember where those scales come from, but they help regularize
+        centers = anchors[...,2:] * acts[...,:2] + anchors[...,:2]
+        sizes = anchors[...,2:] * torch.exp(acts[...,:2])
+        return torch.cat([centers, sizes], -1)
+    else: return [activ_to_bbox(act,anc) for act,anc in zip(acts, anchors)]
+    return res
+
+
+# Here is an example with the 3 by 4 regular grid and random predictions.
+
+size=(3,4)
+anchors = create_grid(size)
+anchors = torch.cat([anchors, torch.tensor([2/size[0],2/size[1]]).expand_as(anchors)], 1)
+activations = torch.randn(size[0]*size[1], 4) * 0.1
+bboxes = activ_to_bbox(activations, anchors)
+
+show_boxes(bboxes)
+
+
+# This helper function changes boxes in the format center/height/width to top/left/bottom/right.
+
+def cthw2tlbr(boxes):
+    "Convert center/size format `boxes` to top/left bottom/right corners."
+    top_left = boxes[:,:2] - boxes[:,2:]/2
+    bot_right = boxes[:,:2] + boxes[:,2:]/2
+    return torch.cat([top_left, bot_right], 1)
+
+
+# Now to decide which predicted bounding box will match a given ground truth object, we will compute the intersection over unions ratios between all the anchors and all the targets, then we will keep the ones that have an overlap greater than a given threshold (0.5).
+
+def intersection(anchors, targets):
+    "Compute the sizes of the intersections of `anchors` by `targets`."
+    ancs, tgts = cthw2tlbr(anchors), cthw2tlbr(targets)
+    a, t = ancs.size(0), tgts.size(0)
+    ancs, tgts = ancs.unsqueeze(1).expand(a,t,4), tgts.unsqueeze(0).expand(a,t,4)
+    top_left_i = torch.max(ancs[...,:2], tgts[...,:2])
+    bot_right_i = torch.min(ancs[...,2:], tgts[...,2:])
+    sizes = torch.clamp(bot_right_i - top_left_i, min=0) 
+    return sizes[...,0] * sizes[...,1]
+
+
+# Let's see some results, if we have our 12 anchors from before...
+
+show_boxes(anchors)
+
+# ... and those targets (0. is the whole image)
+
+targets = torch.tensor([[0.,0.,2.,2.], [-0.5,-0.5,1.,1.], [1/3,0.5,0.5,0.5]])
+show_boxes(targets)
+
+# Then the intersections of each bboxes by each targets are:
+
+intersection(anchors, targets)
+
+
+def IoU_values(anchors, targets):
+    "Compute the IoU values of `anchors` by `targets`."
+    inter = intersection(anchors, targets)
+    anc_sz, tgt_sz = anchors[:,2] * anchors[:,3], targets[:,2] * targets[:,3]
+    union = anc_sz.unsqueeze(1) + tgt_sz.unsqueeze(0) - inter
+    return inter/(union+1e-8)
+
+
+# And then the IoU values are.
+
+IoU_values(anchors, targets)
+
+
+# Then we match a anchor to targets with the following rules:
+# - for each anchor we take the maximum overlap possible with any of the targets.
+# - if that maximum overlap is less than 0.4, we match the anchor box to background, the classifier's target will be that class
+# - if the maximum overlap is greater than 0.5, we match the anchor box to that ground truth object. The classifier's target will be the category of that target
+# - if the maximum overlap is between 0.4 and 0.5, we ignore that anchor in our loss computation
+# - optionally, we force-match for each ground truth object the anchor that has the maximum overlap with it (not sure it helps)
+
+def match_anchors(anchors, targets, match_thr=0.5, bkg_thr=0.4):
+    "Match `anchors` to targets. -1 is match to background, -2 is ignore."
+    matches = anchors.new(anchors.size(0)).zero_().long() - 2
+    if targets.numel() == 0: return matches
+    ious = IoU_values(anchors, targets)
+    vals,idxs = torch.max(ious,1)
+    matches[vals < bkg_thr] = -1
+    matches[vals > match_thr] = idxs[vals > match_thr]
+    #Overwrite matches with each target getting the anchor that has the max IoU.
+    #vals,idxs = torch.max(ious,0)
+    #If idxs contains repetition, this doesn't bug and only the last is considered.
+    #matches[idxs] = targets.new_tensor(list(range(targets.size(0)))).long()
+    return matches
+
+
+# In our previous example, no one had an overlap > 0.5, so unless we use the special rule commented out, there are no matches.
+
+match_anchors(anchors, targets)
+
+# With anchors very close to the targets.
+
+size=(3,4)
+anchors = create_grid(size)
+anchors = torch.cat([anchors, torch.tensor([2/size[0],2/size[1]]).expand_as(anchors)], 1)
+activations = 0.1 * torch.randn(size[0]*size[1], 4)
+bboxes = activ_to_bbox(activations, anchors)
+match_anchors(anchors,bboxes)
+
+# With anchors in the grey area.
+
+anchors = create_grid((2,2))
+anchors = torch.cat([anchors, torch.tensor([1.,1.]).expand_as(anchors)], 1)
+targets = anchors.clone()
+anchors = torch.cat([anchors, torch.tensor([[-0.5,0.,1.,1.8]])], 0)
+match_anchors(anchors,targets)
+
+
+# Does the opposite of `cthw2tbr`.
+
+def tlbr2cthw(boxes):
+    "Convert top/left bottom/right format `boxes` to center/size corners."
+    center = (boxes[:,:2] + boxes[:,2:])/2
+    sizes = boxes[:,2:] - boxes[:,:2]
+    return torch.cat([center, sizes], 1)
+
+
+# Does the opposite of `activ_to_bbox`.
+
+def bbox_to_activ(bboxes, anchors, flatten=True):
+    "Return the target of the model on `anchors` for the `bboxes`."
+    if flatten:
+        t_centers = (bboxes[...,:2] - anchors[...,:2]) / anchors[...,2:] 
+        t_sizes = torch.log(bboxes[...,2:] / anchors[...,2:] + 1e-8) 
+        return torch.cat([t_centers, t_sizes], -1).div_(bboxes.new_tensor([[0.1, 0.1, 0.2, 0.2]]))
+    else: return [activ_to_bbox(act,anc) for act,anc in zip(acts, anchors)]
+    return res
+
+
+# We will one-hot encode our targets with the convention that the class of index 0 is the background, which is the absence of any other classes. That is coded by a row of zeros.
+
+def encode_class(idxs, n_classes):
+    target = idxs.new_zeros(len(idxs), n_classes).float()
+    mask = idxs != 0
+    i1s = LongTensor(list(range(len(idxs))))
+    target[i1s[mask],idxs[mask]-1] = 1
+    return target
+
+
+encode_class(LongTensor([1,2,0,1,3]),3)
+
+
+# And now we are ready to build the loss function. It has two parts, one for the classifier and one for the regressor. For the regression, we will use the L1 (potentially smoothed) loss between the predicted activations for an anchor that matches a given object (we ignore the no match or matches to background) and the corresponding bounding box (after going through `bbox2activ`).
+#
+# For the classification, we use the focal loss, which is a variant of the binary cross entropy used when we have a lot imbalance between the classes to predict (here we will very often have to predict 'background').
+
+class RetinaNetFocalLoss(nn.Module):
+    
+    def __init__(self, gamma:float=2., alpha:float=0.25,  pad_idx:int=0, scales:Collection[float]=None, 
+                 ratios:Collection[float]=None, reg_loss:LossFunction=F.smooth_l1_loss):
+        super().__init__()
+        self.gamma,self.alpha,self.pad_idx,self.reg_loss = gamma,alpha,pad_idx,reg_loss
+        self.scales = ifnone(scales, [1,2**(-1/3), 2**(-2/3)])
+        self.ratios = ifnone(ratios, [1/2,1,2])
+        
+    def _change_anchors(self, sizes:Sizes) -> bool:
+        if not hasattr(self, 'sizes'): return True
+        for sz1, sz2 in zip(self.sizes, sizes):
+            if sz1[0] != sz2[0] or sz1[1] != sz2[1]: return True
+        return False
+    
+    def _create_anchors(self, sizes:Sizes, device:torch.device):
+        self.sizes = sizes
+        self.anchors = create_anchors(sizes, self.ratios, self.scales).to(device)
+    
+    def _unpad(self, bbox_tgt, clas_tgt):
+        i = torch.min(torch.nonzero(clas_tgt-self.pad_idx))
+        return tlbr2cthw(bbox_tgt[i:]), clas_tgt[i:]-1+self.pad_idx
+    
+    def _focal_loss(self, clas_pred, clas_tgt):
+        encoded_tgt = encode_class(clas_tgt, clas_pred.size(1))
+        ps = torch.sigmoid(clas_pred.detach())
+        weights = encoded_tgt * (1-ps) + (1-encoded_tgt) * ps
+        alphas = (1-encoded_tgt) * self.alpha + encoded_tgt * (1-self.alpha)
+        weights.pow_(self.gamma).mul_(alphas)
+        clas_loss = F.binary_cross_entropy_with_logits(clas_pred, encoded_tgt, weights, reduction='sum')
+        return clas_loss
+        
+    def _one_loss(self, clas_pred, bbox_pred, clas_tgt, bbox_tgt):
+        bbox_tgt, clas_tgt = self._unpad(bbox_tgt, clas_tgt)
+        matches = match_anchors(self.anchors, bbox_tgt)
+        bbox_mask = matches>=0
+        if bbox_mask.sum() != 0:
+            bbox_pred = bbox_pred[bbox_mask]
+            bbox_tgt = bbox_tgt[matches[bbox_mask]]
+            bb_loss = self.reg_loss(bbox_pred, bbox_to_activ(bbox_tgt, self.anchors[bbox_mask]))
+        else: bb_loss = 0.
+        matches.add_(1)
+        clas_tgt = clas_tgt + 1
+        clas_mask = matches>=0
+        clas_pred = clas_pred[clas_mask]
+        clas_tgt = torch.cat([clas_tgt.new_zeros(1).long(), clas_tgt])
+        clas_tgt = clas_tgt[matches[clas_mask]]
+        return bb_loss + self._focal_loss(clas_pred, clas_tgt)/torch.clamp(bbox_mask.sum(), min=1.)
+    
+    def forward(self, output, bbox_tgts, clas_tgts):
+        clas_preds, bbox_preds, sizes = output
+        if self._change_anchors(sizes): self._create_anchors(sizes, clas_preds.device)
+        n_classes = clas_preds.size(2)
+        return sum([self._one_loss(cp, bp, ct, bt)
+                    for (cp, bp, ct, bt) in zip(clas_preds, bbox_preds, clas_tgts, bbox_tgts)])/clas_tgts.size(0)
+
+
+# This is a variant of the L1 loss used in several implementations:
+
+class SigmaL1SmoothLoss(nn.Module):
+
+    def forward(self, output, target):
+        reg_diff = torch.abs(target - output)
+        reg_loss = torch.where(torch.le(reg_diff, 1/9), 4.5 * torch.pow(reg_diff, 2), reg_diff - 1/18)
+        return reg_loss.mean()
+
+
+# ## Defining the Learner
+
+ratios = [1/2,1,2]
+scales = [1,2**(-1/3), 2**(-2/3)]
+#scales = [1,2**(1/3), 2**(2/3)] for bigger size
+
+encoder = create_body(models.resnet50, cut=-2)
+model = RetinaNet(encoder, data.c, final_bias=-4)
+crit = RetinaNetFocalLoss(scales=scales, ratios=ratios)
+learn = Learner(data, model, loss_func=crit)
+
+# Why `final_bias=-4`? That's because we want the network to predict background easily at the beginning (since it's the most common class). At first the final convolution of the classifier is initialized with weights=0 and that bias, so it will return -4 for everyone. If go though a sigmoid 
+
+torch.sigmoid(tensor([-4.]))
+
+
+# We see it'll give a corresponding probability of 0.02 roughly. 
+#
+# Then, for transfer learning/discriminative LRs, we need to define how to split between body and custom head.
+
+def retina_net_split(model):
+    groups = [list(model.encoder.children())[:6], list(model.encoder.children())[6:]]
+    return groups + [list(model.children())[1:]]
+
+
+learn = learn.split(retina_net_split)
+
+# And now we can train as usual!
+
+learn.freeze()
+
+learn.lr_find()
+
+learn.recorder.plot(skip_end=5)
+
+learn.fit_one_cycle(5, 1e-4)
+
+learn.save('stage1-128')
+
+learn.unfreeze()
+
+learn.fit_one_cycle(10, slice(1e-6, 5e-5))
+
+learn.save('stage2-128')
+
+learn.data = get_data(32,192)
+
+learn.freeze()
+learn.lr_find()
+
+learn.recorder.plot()
+
+learn.fit_one_cycle(5, 1e-4)
+
+learn.save('stage1-192')
+
+learn.unfreeze()
+
+learn.fit_one_cycle(10, slice(1e-6, 5e-5))
+
+learn.save('stage2-192')
+
+learn.data = get_data(24,256)
+
+learn.freeze()
+learn.fit_one_cycle(5, 1e-4)
+
+learn.save('stage1-256')
+
+learn.unfreeze()
+learn.fit_one_cycle(10, slice(1e-6, 5e-5))
+
+learn.save('stage2-256')
+
+# ## Results
+
+learn = learn.load('stage2-256')
+
+img,target = next(iter(data.valid_dl))
+with torch.no_grad():
+    output = learn.model(img)
+
+
+# First we need to remove the padding that was added to collate our targets together.
+
+def unpad(tgt_bbox, tgt_clas, pad_idx=0):
+    i = torch.min(torch.nonzero(tgt_clas-pad_idx))
+    return tlbr2cthw(tgt_bbox[i:]), tgt_clas[i:]-1+pad_idx
+
+
+# Then we process the outputs of the model: we convert the activations of the regressor to bounding boxes and the predictions to probabilities, only keeping those above a given threshold.
+
+def process_output(output, i, detect_thresh=0.25):
+    "Process `output[i]` and return the predicted bboxes above `detect_thresh`."
+    clas_pred,bbox_pred,sizes = output[0][i], output[1][i], output[2]
+    anchors = create_anchors(sizes, ratios, scales).to(clas_pred.device)
+    bbox_pred = activ_to_bbox(bbox_pred, anchors)
+    clas_pred = torch.sigmoid(clas_pred)
+    detect_mask = clas_pred.max(1)[0] > detect_thresh
+    bbox_pred, clas_pred = bbox_pred[detect_mask], clas_pred[detect_mask]
+    bbox_pred = tlbr2cthw(torch.clamp(cthw2tlbr(bbox_pred), min=-1, max=1))    
+    scores, preds = clas_pred.max(1)
+    return bbox_pred, scores, preds
+
+
+# Helper functions to plot the results
+
+# +
+def _draw_outline(o:Patch, lw:int):
+    "Outline bounding box onto image `Patch`."
+    o.set_path_effects([patheffects.Stroke(
+        linewidth=lw, foreground='black'), patheffects.Normal()])
+
+def draw_rect(ax:plt.Axes, b:Collection[int], color:str='white', text=None, text_size=14):
+    "Draw bounding box on `ax`."
+    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor=color, lw=2))
+    _draw_outline(patch, 4)
+    if text is not None:
+        patch = ax.text(*b[:2], text, verticalalignment='top', color=color, fontsize=text_size, weight='bold')
+        _draw_outline(patch,1)
+
+
+# -
+
+def show_preds(img, output, idx, detect_thresh=0.25, classes=None):
+    bbox_pred, scores, preds = process_output(output, idx, detect_thresh)
+    bbox_pred, preds, scores = bbox_pred.cpu(), preds.cpu(), scores.cpu()
+    t_sz = torch.Tensor([*img.size])[None].float()
+    bbox_pred[:,:2] = bbox_pred[:,:2] - bbox_pred[:,2:]/2
+    bbox_pred[:,:2] = (bbox_pred[:,:2] + 1) * t_sz/2
+    bbox_pred[:,2:] = bbox_pred[:,2:] * t_sz
+    bbox_pred = bbox_pred.long()
+    _, ax = plt.subplots(1,1)
+    for bbox, c, scr in zip(bbox_pred, preds, scores):
+        img.show(ax=ax)
+        txt = str(c.item()) if classes is None else classes[c.item()+1]
+        draw_rect(ax, [bbox[1],bbox[0],bbox[3],bbox[2]], text=f'{txt} {scr:.2f}')
+
+
+# And let's have a look at one picture.
+
+idx = 0
+img = data.valid_ds[idx][0]
+show_preds(img, output, idx, detect_thresh=0.3, classes=data.classes)
+
+
+# It looks like a lot of our anchors are detecting kind of the same object. We use an algorithm called Non-Maximum Suppression to remove near-duplicates: going from the biggest score predicted to the lowest, we take the corresponding bounding boxes and remove all the bounding boxes down the list that have an IoU > 0.5 with this one. We continue the process until we have reached the end of the list.
+
+def nms(boxes, scores, thresh=0.3):
+    idx_sort = scores.argsort(descending=True)
+    boxes, scores = boxes[idx_sort], scores[idx_sort]
+    to_keep, indexes = [], torch.LongTensor(range_of(scores))
+    while len(scores) > 0:
+        to_keep.append(idx_sort[indexes[0]])
+        iou_vals = IoU_values(boxes, boxes[:1]).squeeze()
+        mask_keep = iou_vals < thresh
+        if len(mask_keep.nonzero()) == 0: break
+        boxes, scores, indexes = boxes[mask_keep], scores[mask_keep], indexes[mask_keep]
+    return LongTensor(to_keep)
+
+
+def process_output(output, i, detect_thresh=0.25):
+    clas_pred,bbox_pred,sizes = output[0][i], output[1][i], output[2]
+    anchors = create_anchors(sizes, ratios, scales).to(clas_pred.device)
+    bbox_pred = activ_to_bbox(bbox_pred, anchors)
+    clas_pred = torch.sigmoid(clas_pred)
+    detect_mask = clas_pred.max(1)[0] > detect_thresh
+    bbox_pred, clas_pred = bbox_pred[detect_mask], clas_pred[detect_mask]
+    bbox_pred = tlbr2cthw(torch.clamp(cthw2tlbr(bbox_pred), min=-1, max=1))    
+    if clas_pred.numel() == 0: return [],[],[]
+    scores, preds = clas_pred.max(1)
+    return bbox_pred, scores, preds
+
+
+def show_preds(img, output, idx, detect_thresh=0.25, classes=None, ax=None):
+    bbox_pred, scores, preds = process_output(output, idx, detect_thresh)
+    if len(scores) != 0:
+        to_keep = nms(bbox_pred, scores)
+        bbox_pred, preds, scores = bbox_pred[to_keep].cpu(), preds[to_keep].cpu(), scores[to_keep].cpu()
+        t_sz = torch.Tensor([*img.size])[None].float()
+        bbox_pred[:,:2] = bbox_pred[:,:2] - bbox_pred[:,2:]/2
+        bbox_pred[:,:2] = (bbox_pred[:,:2] + 1) * t_sz/2
+        bbox_pred[:,2:] = bbox_pred[:,2:] * t_sz
+        bbox_pred = bbox_pred.long()
+    if ax is None: _, ax = plt.subplots(1,1)
+    img.show(ax=ax)
+    for bbox, c, scr in zip(bbox_pred, preds, scores):
+        txt = str(c.item()) if classes is None else classes[c.item()+1]
+        draw_rect(ax, [bbox[1],bbox[0],bbox[3],bbox[2]], text=f'{txt} {scr:.2f}')
+
+
+def show_results(learn, start=0, n=5, detect_thresh=0.35, figsize=(10,25)):
+    x,y = learn.data.one_batch(DatasetType.Valid, cpu=False)
+    with torch.no_grad():
+        z = learn.model.eval()(x)
+    _,axs = plt.subplots(n, 2, figsize=figsize)
+    for i in range(n):
+        img,bbox = learn.data.valid_ds[start+i]
+        img.show(ax=axs[i,0], y=bbox)
+        show_preds(img, z, start+i, detect_thresh=detect_thresh, classes=learn.data.classes, ax=axs[i,1])
+
+
+learn = learn.load('stage2-256')
+
+show_results(learn, start=10)
+
+
+# ## mAP
+
+# A metric often used for this kind of task is the mean Average Precision (our mAP). It relies on computing the cumulated precision and recall for each class, then tries to compute the area under the precision/recall curve we can draw.
+
+def get_predictions(output, idx, detect_thresh=0.05):
+    bbox_pred, scores, preds = process_output(output, idx, detect_thresh)
+    if len(scores) == 0: return [],[],[]
+    to_keep = nms(bbox_pred, scores)
+    return bbox_pred[to_keep], preds[to_keep], scores[to_keep]
+
+
+def compute_ap(precision, recall):
+    "Compute the average precision for `precision` and `recall` curve."
+    recall = np.concatenate(([0.], list(recall), [1.]))
+    precision = np.concatenate(([0.], list(precision), [0.]))
+    for i in range(len(precision) - 1, 0, -1):
+        precision[i - 1] = np.maximum(precision[i - 1], precision[i])
+    idx = np.where(recall[1:] != recall[:-1])[0]
+    ap = np.sum((recall[idx + 1] - recall[idx]) * precision[idx + 1])
+    return ap
+
+
+def compute_class_AP(model, dl, n_classes, iou_thresh=0.5, detect_thresh=0.35, num_keep=100):
+    tps, clas, p_scores = [], [], []
+    classes, n_gts = LongTensor(range(n_classes)),torch.zeros(n_classes).long()
+    with torch.no_grad():
+        for input,target in progress_bar(dl):
+            output = model(input)
+            for i in range(target[0].size(0)):
+                bbox_pred, preds, scores = get_predictions(output, i, detect_thresh)
+                tgt_bbox, tgt_clas = unpad(target[0][i], target[1][i])
+                if len(bbox_pred) != 0 and len(tgt_bbox) != 0:
+                    ious = IoU_values(bbox_pred, tgt_bbox)
+                    max_iou, matches = ious.max(1)
+                    detected = []
+                    for i in range_of(preds):
+                        if max_iou[i] >= iou_thresh and matches[i] not in detected and tgt_clas[matches[i]] == preds[i]:
+                            detected.append(matches[i])
+                            tps.append(1)
+                        else: tps.append(0)
+                    clas.append(preds.cpu())
+                    p_scores.append(scores.cpu())
+                n_gts += (tgt_clas.cpu()[:,None] == classes[None,:]).sum(0)
+    tps, p_scores, clas = torch.tensor(tps), torch.cat(p_scores,0), torch.cat(clas,0)
+    fps = 1-tps
+    idx = p_scores.argsort(descending=True)
+    tps, fps, clas = tps[idx], fps[idx], clas[idx]
+    aps = []
+    #return tps, clas
+    for cls in range(n_classes):
+        tps_cls, fps_cls = tps[clas==cls].float().cumsum(0), fps[clas==cls].float().cumsum(0)
+        if tps_cls.numel() != 0 and tps_cls[-1] != 0:
+            precision = tps_cls / (tps_cls + fps_cls + 1e-8)
+            recall = tps_cls / (n_gts[cls] + 1e-8)
+            aps.append(compute_ap(precision, recall))
+        else: aps.append(0.)
+    return aps
+
+
+L = compute_class_AP(learn.model, data.valid_dl, data.c-1)
+
+for ap,cl in zip(L, data.classes[1:]): print(f'{cl}: {ap:.6f}')
+
+for ap,cl in zip(L, data.classes[1:]): print(f'{cl}: {ap:.6f}')
+
+for ap,cl in zip(L, data.classes[1:]): print(f'{cl}: {ap:.6f}')
+
+for ap,cl in zip(L, data.classes[1:]): print(f'{cl}: {ap:.6f}')
+
+for ap,cl in zip(L, data.classes[1:]): print(f'{cl}: {ap:.6f}')
+
+
diff --git a/nbs/dl2/timing.ipynb b/nbs/dl2/timing.ipynb
index 2351f29..e38654e 100644
--- ./nbs/dl2/timing.ipynb
+++ ./nbs/dl2/timing.ipynb
@@ -106,6 +106,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/timing.py b/nbs/dl2/timing.py
new file mode 100644
index 0000000..6d9ed84
--- /dev/null
+++ ./nbs/dl2/timing.py
@@ -0,0 +1,43 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+from exp.nb_08 import *
+
+path = datasets.untar_data(datasets.URLs.IMAGENETTE_320)
+
+# +
+tfms = [make_rgb, ResizeFixed(224), to_byte_tensor, to_float_tensor]
+
+il = ImageList.from_files(path, tfms=tfms)
+sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='val'))
+ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())
+# -
+
+bs=256
+
+train_dl,valid_dl = get_dls(ll.train,ll.valid,bs, num_workers=4)
+
+# %time x,y = next(iter(train_dl))
+
+# %time for x,y in train_dl: x,y = x.cuda(),y.cuda()
+
+
diff --git a/nbs/dl2/transforms-dali.ipynb b/nbs/dl2/transforms-dali.ipynb
index 1108e6b..3fb0a58 100644
--- ./nbs/dl2/transforms-dali.ipynb
+++ ./nbs/dl2/transforms-dali.ipynb
@@ -278,6 +278,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/transforms-dali.py b/nbs/dl2/transforms-dali.py
new file mode 100644
index 0000000..1d002d3
--- /dev/null
+++ ./nbs/dl2/transforms-dali.py
@@ -0,0 +1,106 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Data augmentation
+
+# +
+# %load_ext autoreload
+# %autoreload 2
+
+# %matplotlib inline
+# -
+
+#export
+from exp.nb_09 import *
+
+# ## PIL transforms
+
+path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)
+tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor]
+
+from nvidia.dali.pipeline import Pipeline
+
+import nvidia.dali.ops as ops
+import nvidia.dali.types as types
+
+bs=8
+
+
+class SimplePipeline(Pipeline):
+    def __init__(self, batch_size=8, num_threads=8, device_id=0):
+        super(SimplePipeline, self).__init__(batch_size, num_threads, device_id, seed = 12)
+        self.input = ops.FileReader(file_root = path/'train')
+        # self.input = ops.FileReader(file_root = image_dir, file_list = image_dir + '/file_list.txt')
+        self.resize = ops.Resize(device = "cpu", resize_x=128, resize_y=128, image_type = types.RGB,
+            interp_type = types.INTERP_LINEAR)
+        self.decode = ops.HostDecoder(output_type = types.RGB)
+
+    def define_graph(self):
+        jpegs, labels = self.input(name='r')
+        images = self.decode(jpegs)
+        images = self.resize(images)
+        return (images, labels)
+
+
+pipe = SimplePipeline()
+pipe.build()
+
+pipe_out = pipe.run()
+print(pipe_out)
+
+images, labels = pipe_out
+images.is_dense_tensor(), labels.is_dense_tensor()
+
+t = images.as_tensor()
+
+from nvidia.dali.plugin.pytorch import DALIGenericIterator,DALIClassificationIterator,feed_ndarray
+
+it = DALIGenericIterator(pipe, ['data','label'], pipe.epoch_size('r'))
+
+it = DALIClassificationIterator(pipe, pipe.epoch_size('r'))
+
+its = iter(it)
+
+t = next(it)[0]
+
+t['label'].cuda().long().type()
+
+t['data'].type()
+
+import numpy as np
+
+labels_tensor = labels.as_tensor()
+
+labels_tensor.shape()
+
+np.array(labels_tensor)
+
+import matplotlib.gridspec as gridspec
+
+
+def show_images(image_batch):
+    columns = 4
+    rows = len(image_batch) // (columns)
+    fig = plt.figure(figsize = (32,(32 // columns) * rows))
+    gs = gridspec.GridSpec(rows, columns)
+    for j in range(rows*columns):
+        plt.subplot(gs[j])
+        plt.axis("off")
+        plt.imshow(image_batch.at(j))
+
+
+show_images(images)
+
+
diff --git a/nbs/dl2/translation.ipynb b/nbs/dl2/translation.ipynb
index 1c0b71a..5b8fefc 100644
--- ./nbs/dl2/translation.ipynb
+++ ./nbs/dl2/translation.ipynb
@@ -2180,6 +2180,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/translation.py b/nbs/dl2/translation.py
new file mode 100644
index 0000000..634b7e2
--- /dev/null
+++ ./nbs/dl2/translation.py
@@ -0,0 +1,578 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+from fastai.text import *
+
+# ## Reduce original dataset to questions
+
+path = Config().data_path()/'giga-fren'
+
+# You only need to execute the setup cells once, uncomment to run. The dataset can be downloaded [here](https://s3.amazonaws.com/fast-ai-nlp/giga-fren.tgz).
+
+# +
+# #! wget https://s3.amazonaws.com/fast-ai-nlp/giga-fren.tgz -P {path}
+# #! tar xf {path}/giga-fren.tgz -C {path} 
+
+# with open(path/'giga-fren.release2.fixed.fr') as f:
+#    fr = f.read().split('\n')
+
+# with open(path/'giga-fren.release2.fixed.en') as f:
+#    en = f.read().split('\n')
+
+# re_eq = re.compile('^(Wh[^?.!]+\?)')
+# re_fq = re.compile('^([^?.!]+\?)')
+# en_fname = path/'giga-fren.release2.fixed.en'
+# fr_fname = path/'giga-fren.release2.fixed.fr'
+
+# lines = ((re_eq.search(eq), re_fq.search(fq)) 
+#         for eq, fq in zip(open(en_fname, encoding='utf-8'), open(fr_fname, encoding='utf-8')))
+# qs = [(e.group(), f.group()) for e,f in lines if e and f]
+
+# qs = [(q1,q2) for q1,q2 in qs]
+# df = pd.DataFrame({'fr': [q[1] for q in qs], 'en': [q[0] for q in qs]}, columns = ['en', 'fr'])
+# df.to_csv(path/'questions_easy.csv', index=False)
+
+# del en, fr, lines, qs, df # free RAM or restart the nb 
+
+# +
+### fastText pre-trained word vectors https://fasttext.cc/docs/en/crawl-vectors.html
+# #! wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.bin.gz -P {path}
+# #! wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz -P {path}
+# #! gzip -d {path}/cc.fr.300.bin.gz 
+# #! gzip -d {path}/cc.en.300.bin.gz
+# -
+
+path.ls()
+
+# ## Put them in a DataBunch
+
+# Our questions look like this now:
+
+df = pd.read_csv(path/'questions_easy.csv')
+df.head()
+
+# To make it simple, we lowercase everything.
+
+df['en'] = df['en'].apply(lambda x:x.lower())
+df['fr'] = df['fr'].apply(lambda x:x.lower())
+
+
+# The first thing is that we will need to collate inputs and targets in a batch: they have different lengths so we need to add padding to make the sequence length the same;
+
+def seq2seq_collate(samples:BatchSamples, pad_idx:int=1, pad_first:bool=True, backwards:bool=False) -> Tuple[LongTensor, LongTensor]:
+    "Function that collect samples and adds padding. Flips token order if needed"
+    samples = to_data(samples)
+    max_len_x,max_len_y = max([len(s[0]) for s in samples]),max([len(s[1]) for s in samples])
+    res_x = torch.zeros(len(samples), max_len_x).long() + pad_idx
+    res_y = torch.zeros(len(samples), max_len_y).long() + pad_idx
+    if backwards: pad_first = not pad_first
+    for i,s in enumerate(samples):
+        if pad_first: 
+            res_x[i,-len(s[0]):],res_y[i,-len(s[1]):] = LongTensor(s[0]),LongTensor(s[1])
+        else:         
+            res_x[i,:len(s[0]):],res_y[i,:len(s[1]):] = LongTensor(s[0]),LongTensor(s[1])
+    if backwards: res_x,res_y = res_x.flip(1),res_y.flip(1)
+    return res_x,res_y
+
+
+# Then we create a special `DataBunch` that uses this collate function.
+
+class Seq2SeqDataBunch(TextDataBunch):
+    "Create a `TextDataBunch` suitable for training an RNN classifier."
+    @classmethod
+    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs:int=32, val_bs:int=None, pad_idx=1,
+               pad_first=False, device:torch.device=None, no_check:bool=False, backwards:bool=False, **dl_kwargs) -> DataBunch:
+        "Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`"
+        datasets = cls._init_ds(train_ds, valid_ds, test_ds)
+        val_bs = ifnone(val_bs, bs)
+        collate_fn = partial(seq2seq_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)
+        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs//2)
+        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)
+        dataloaders = [train_dl]
+        for ds in datasets[1:]:
+            lengths = [len(t) for t in ds.x.items]
+            sampler = SortSampler(ds.x, key=lengths.__getitem__)
+            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))
+        return cls(*dataloaders, path=path, device=device, collate_fn=collate_fn, no_check=no_check)
+
+
+# And a subclass of `TextList` that will use this `DataBunch` class in the call `.databunch` and will use `TextList` to label (since our targets are other texts).
+
+class Seq2SeqTextList(TextList):
+    _bunch = Seq2SeqDataBunch
+    _label_cls = TextList
+
+
+# Thats all we need to use the data block API!
+
+src = Seq2SeqTextList.from_df(df, path = path, cols='fr').split_by_rand_pct().label_from_df(cols='en', label_cls=TextList)
+
+np.percentile([len(o) for o in src.train.x.items] + [len(o) for o in src.valid.x.items], 90)
+
+np.percentile([len(o) for o in src.train.y.items] + [len(o) for o in src.valid.y.items], 90)
+
+# We remove the items where one of the target is more than 30 tokens long.
+
+src = src.filter_by_func(lambda x,y: len(x) > 30 or len(y) > 30)
+
+len(src.train) + len(src.valid)
+
+data = src.databunch()
+
+data.save()
+
+data = load_data(path)
+
+data.show_batch()
+
+# ## Model
+
+# ### Pretrained embeddings
+
+# To install fastText:
+# ```
+# $ git clone https://github.com/facebookresearch/fastText.git
+# $ cd fastText
+# $ pip install .
+# ```
+
+# Installation: https://github.com/facebookresearch/fastText#building-fasttext-for-python
+import fastText as ft
+
+fr_vecs = ft.load_model(str((path/'cc.fr.300.bin')))
+en_vecs = ft.load_model(str((path/'cc.en.300.bin')))
+
+
+# We create an embedding module with the pretrained vectors and random data for the missing parts.
+
+def create_emb(vecs, itos, em_sz=300, mult=1.):
+    emb = nn.Embedding(len(itos), em_sz, padding_idx=1)
+    wgts = emb.weight.data
+    vec_dic = {w:vecs.get_word_vector(w) for w in vecs.get_words()}
+    miss = []
+    for i,w in enumerate(itos):
+        try: wgts[i] = tensor(vec_dic[w])
+        except: miss.append(w)
+    return emb
+
+
+emb_enc = create_emb(fr_vecs, data.x.vocab.itos)
+emb_dec = create_emb(en_vecs, data.y.vocab.itos)
+
+torch.save(emb_enc, path/'models'/'fr_emb.pth')
+torch.save(emb_dec, path/'models'/'en_emb.pth')
+
+# Free some RAM
+
+del fr_vecs
+del en_vecs
+
+# ### QRNN seq2seq
+
+# Our model we use QRNNs at its base (you can use GRUs or LSTMs by adapting a little bit). Using QRNNs require you have properly installed cuda (a version that matches your PyTorch install). 
+
+from fastai.text.models.qrnn import QRNN, QRNNLayer
+
+
+# The model in itself consists in an encoder and a decoder
+#
+# ![Seq2seq model](images/seq2seq.png)
+#
+# The encoder is a (quasi) recurrent neural net and we feed it our input sentence, producing an output (that we discard for now) and a hidden state. That hidden state is then given to the decoder (an other RNN) which uses it in conjunction with the outputs it predicts to get produce the translation. We loop until the decoder produces a padding token (or at 30 iterations to make sure it's not an infinite loop at the beginning of training). 
+
+class Seq2SeqQRNN(nn.Module):
+    def __init__(self, emb_enc, emb_dec, n_hid, max_len, n_layers=2, p_inp:float=0.15, p_enc:float=0.25, 
+                 p_dec:float=0.1, p_out:float=0.35, p_hid:float=0.05, bos_idx:int=0, pad_idx:int=1):
+        super().__init__()
+        self.n_layers,self.n_hid,self.max_len,self.bos_idx,self.pad_idx = n_layers,n_hid,max_len,bos_idx,pad_idx
+        self.emb_enc = emb_enc
+        self.emb_enc_drop = nn.Dropout(p_inp)
+        self.encoder = QRNN(emb_enc.weight.size(1), n_hid, n_layers=n_layers, dropout=p_enc)
+        self.out_enc = nn.Linear(n_hid, emb_enc.weight.size(1), bias=False)
+        self.hid_dp  = nn.Dropout(p_hid)
+        self.emb_dec = emb_dec
+        self.decoder = QRNN(emb_dec.weight.size(1), emb_dec.weight.size(1), n_layers=n_layers, dropout=p_dec)
+        self.out_drop = nn.Dropout(p_out)
+        self.out = nn.Linear(emb_dec.weight.size(1), emb_dec.weight.size(0))
+        self.out.weight.data = self.emb_dec.weight.data
+        
+    def forward(self, inp):
+        bs,sl = inp.size()
+        self.encoder.reset()
+        self.decoder.reset()
+        hid = self.initHidden(bs)
+        emb = self.emb_enc_drop(self.emb_enc(inp))
+        enc_out, hid = self.encoder(emb, hid)
+        hid = self.out_enc(self.hid_dp(hid))
+
+        dec_inp = inp.new_zeros(bs).long() + self.bos_idx
+        outs = []
+        for i in range(self.max_len):
+            emb = self.emb_dec(dec_inp).unsqueeze(1)
+            out, hid = self.decoder(emb, hid)
+            out = self.out(self.out_drop(out[:,0]))
+            outs.append(out)
+            dec_inp = out.max(1)[1]
+            if (dec_inp==self.pad_idx).all(): break
+        return torch.stack(outs, dim=1)
+    
+    def initHidden(self, bs): return one_param(self).new_zeros(self.n_layers, bs, self.n_hid)
+
+
+# #### Loss function
+
+# The loss pads output and target so that they are of the same size before using the usual flattened version of cross entropy. We do the same for accuracy.
+
+def seq2seq_loss(out, targ, pad_idx=1):
+    bs,targ_len = targ.size()
+    _,out_len,vs = out.size()
+    if targ_len>out_len: out  = F.pad(out,  (0,0,0,targ_len-out_len,0,0), value=pad_idx)
+    if out_len>targ_len: targ = F.pad(targ, (0,out_len-targ_len,0,0), value=pad_idx)
+    return CrossEntropyFlat()(out, targ)
+
+
+def seq2seq_acc(out, targ, pad_idx=1):
+    bs,targ_len = targ.size()
+    _,out_len,vs = out.size()
+    if targ_len>out_len: out  = F.pad(out,  (0,0,0,targ_len-out_len,0,0), value=pad_idx)
+    if out_len>targ_len: targ = F.pad(targ, (0,out_len-targ_len,0,0), value=pad_idx)
+    out = out.argmax(2)
+    return (out==targ).float().mean()
+
+
+# #### Bleu metric (see dedicated notebook)
+
+# In translation, the metric usually used is BLEU, see the corresponding notebook for the details.
+
+class NGram():
+    def __init__(self, ngram, max_n=5000): self.ngram,self.max_n = ngram,max_n
+    def __eq__(self, other):
+        if len(self.ngram) != len(other.ngram): return False
+        return np.all(np.array(self.ngram) == np.array(other.ngram))
+    def __hash__(self): return int(sum([o * self.max_n**i for i,o in enumerate(self.ngram)]))
+
+
+def get_grams(x, n, max_n=5000):
+    return x if n==1 else [NGram(x[i:i+n], max_n=max_n) for i in range(len(x)-n+1)]
+
+
+def get_correct_ngrams(pred, targ, n, max_n=5000):
+    pred_grams,targ_grams = get_grams(pred, n, max_n=max_n),get_grams(targ, n, max_n=max_n)
+    pred_cnt,targ_cnt = Counter(pred_grams),Counter(targ_grams)
+    return sum([min(c, targ_cnt[g]) for g,c in pred_cnt.items()]),len(pred_grams)
+
+
+class CorpusBLEU(Callback):
+    def __init__(self, vocab_sz):
+        self.vocab_sz = vocab_sz
+        self.name = 'bleu'
+    
+    def on_epoch_begin(self, **kwargs):
+        self.pred_len,self.targ_len,self.corrects,self.counts = 0,0,[0]*4,[0]*4
+    
+    def on_batch_end(self, last_output, last_target, **kwargs):
+        last_output = last_output.argmax(dim=-1)
+        for pred,targ in zip(last_output.cpu().numpy(),last_target.cpu().numpy()):
+            self.pred_len += len(pred)
+            self.targ_len += len(targ)
+            for i in range(4):
+                c,t = get_correct_ngrams(pred, targ, i+1, max_n=self.vocab_sz)
+                self.corrects[i] += c
+                self.counts[i]   += t
+    
+    def on_epoch_end(self, last_metrics, **kwargs):
+        precs = [c/t for c,t in zip(self.corrects,self.counts)]
+        len_penalty = exp(1 - self.targ_len/self.pred_len) if self.pred_len < self.targ_len else 1
+        bleu = len_penalty * ((precs[0]*precs[1]*precs[2]*precs[3]) ** 0.25)
+        return add_metrics(last_metrics, bleu)
+
+
+# We load our pretrained embeddings to create the model.
+
+emb_enc = torch.load(path/'models'/'fr_emb.pth')
+emb_dec = torch.load(path/'models'/'en_emb.pth')
+
+model = Seq2SeqQRNN(emb_enc, emb_dec, 256, 30, n_layers=2)
+learn = Learner(data, model, loss_func=seq2seq_loss, metrics=[seq2seq_acc, CorpusBLEU(len(data.y.vocab.itos))])
+
+learn.lr_find()
+
+learn.recorder.plot()
+
+learn.fit_one_cycle(8, 1e-2)
+
+
+# So how good is our model? Let's see a few predictions.
+
+def get_predictions(learn, ds_type=DatasetType.Valid):
+    learn.model.eval()
+    inputs, targets, outputs = [],[],[]
+    with torch.no_grad():
+        for xb,yb in progress_bar(learn.dl(ds_type)):
+            out = learn.model(xb)
+            for x,y,z in zip(xb,yb,out):
+                inputs.append(learn.data.train_ds.x.reconstruct(x))
+                targets.append(learn.data.train_ds.y.reconstruct(y))
+                outputs.append(learn.data.train_ds.y.reconstruct(z.argmax(1)))
+    return inputs, targets, outputs
+
+
+inputs, targets, outputs = get_predictions(learn)
+
+inputs[700], targets[700], outputs[700]
+
+inputs[701], targets[701], outputs[701]
+
+inputs[2513], targets[2513], outputs[2513]
+
+inputs[4000], targets[4000], outputs[4000]
+
+
+# It's usually beginning well, but falls into easy word at the end of the question.
+
+# ### Teacher forcing
+
+# One way to help training is to help the decoder by feeding it the real targets instead of its predictions (if it starts with wrong words, it's very unlikely to give us the right translation). We do that all the time at the beginning, then progressively reduce the amount of teacher forcing.
+
+class TeacherForcing(LearnerCallback):
+    
+    def __init__(self, learn, end_epoch):
+        super().__init__(learn)
+        self.end_epoch = end_epoch
+    
+    def on_batch_begin(self, last_input, last_target, train, **kwargs):
+        if train: return {'last_input': [last_input, last_target]}
+    
+    def on_epoch_begin(self, epoch, **kwargs):
+        self.learn.model.pr_force = 1 - 0.5 * epoch/self.end_epoch
+
+
+class Seq2SeqQRNN(nn.Module):
+    def __init__(self, emb_enc, emb_dec, n_hid, max_len, n_layers=2, p_inp:float=0.15, p_enc:float=0.25, 
+                 p_dec:float=0.1, p_out:float=0.35, p_hid:float=0.05, bos_idx:int=0, pad_idx:int=1):
+        super().__init__()
+        self.n_layers,self.n_hid,self.max_len,self.bos_idx,self.pad_idx = n_layers,n_hid,max_len,bos_idx,pad_idx
+        self.emb_enc = emb_enc
+        self.emb_enc_drop = nn.Dropout(p_inp)
+        self.encoder = QRNN(emb_enc.weight.size(1), n_hid, n_layers=n_layers, dropout=p_enc)
+        self.out_enc = nn.Linear(n_hid, emb_enc.weight.size(1), bias=False)
+        self.hid_dp  = nn.Dropout(p_hid)
+        self.emb_dec = emb_dec
+        self.decoder = QRNN(emb_dec.weight.size(1), emb_dec.weight.size(1), n_layers=n_layers, dropout=p_dec)
+        self.out_drop = nn.Dropout(p_out)
+        self.out = nn.Linear(emb_dec.weight.size(1), emb_dec.weight.size(0))
+        self.out.weight.data = self.emb_dec.weight.data
+        self.pr_force = 0.
+        
+    def forward(self, inp, targ=None):
+        bs,sl = inp.size()
+        hid = self.initHidden(bs)
+        emb = self.emb_enc_drop(self.emb_enc(inp))
+        enc_out, hid = self.encoder(emb, hid)
+        hid = self.out_enc(self.hid_dp(hid))
+
+        dec_inp = inp.new_zeros(bs).long() + self.bos_idx
+        res = []
+        for i in range(self.max_len):
+            emb = self.emb_dec(dec_inp).unsqueeze(1)
+            outp, hid = self.decoder(emb, hid)
+            outp = self.out(self.out_drop(outp[:,0]))
+            res.append(outp)
+            dec_inp = outp.data.max(1)[1]
+            if (dec_inp==self.pad_idx).all(): break
+            if (targ is not None) and (random.random()<self.pr_force):
+                if i>=targ.shape[1]: break
+                dec_inp = targ[:,i]
+        return torch.stack(res, dim=1)
+    
+    def initHidden(self, bs): return one_param(self).new_zeros(self.n_layers, bs, self.n_hid)
+
+
+emb_enc = torch.load(path/'models'/'fr_emb.pth')
+emb_dec = torch.load(path/'models'/'en_emb.pth')
+
+model = Seq2SeqQRNN(emb_enc, emb_dec, 256, 30, n_layers=2)
+learn = Learner(data, model, loss_func=seq2seq_loss, metrics=[seq2seq_acc, CorpusBLEU(len(data.y.vocab.itos))],
+                callback_fns=partial(TeacherForcing, end_epoch=8))
+
+learn.fit_one_cycle(8, 1e-2)
+
+inputs, targets, outputs = get_predictions(learn)
+
+inputs[700],targets[700],outputs[700]
+
+inputs[2513], targets[2513], outputs[2513]
+
+inputs[4000], targets[4000], outputs[4000]
+
+
+# +
+#get_bleu(learn)
+# -
+
+# ## Bidir
+
+# A second things that might help is to use a bidirectional model for the encoder.
+
+class Seq2SeqQRNN(nn.Module):
+    def __init__(self, emb_enc, emb_dec, n_hid, max_len, n_layers=2, p_inp:float=0.15, p_enc:float=0.25, 
+                 p_dec:float=0.1, p_out:float=0.35, p_hid:float=0.05, bos_idx:int=0, pad_idx:int=1):
+        super().__init__()
+        self.n_layers,self.n_hid,self.max_len,self.bos_idx,self.pad_idx = n_layers,n_hid,max_len,bos_idx,pad_idx
+        self.emb_enc = emb_enc
+        self.emb_enc_drop = nn.Dropout(p_inp)
+        self.encoder = QRNN(emb_enc.weight.size(1), n_hid, n_layers=n_layers, dropout=p_enc, bidirectional=True)
+        self.out_enc = nn.Linear(2*n_hid, emb_enc.weight.size(1), bias=False)
+        self.hid_dp  = nn.Dropout(p_hid)
+        self.emb_dec = emb_dec
+        self.decoder = QRNN(emb_dec.weight.size(1), emb_dec.weight.size(1), n_layers=n_layers, dropout=p_dec)
+        self.out_drop = nn.Dropout(p_out)
+        self.out = nn.Linear(emb_dec.weight.size(1), emb_dec.weight.size(0))
+        self.out.weight.data = self.emb_dec.weight.data
+        self.pr_force = 0.
+        
+    def forward(self, inp, targ=None):
+        bs,sl = inp.size()
+        hid = self.initHidden(bs)
+        emb = self.emb_enc_drop(self.emb_enc(inp))
+        enc_out, hid = self.encoder(emb, hid)
+        
+        hid = hid.view(2,self.n_layers, bs, self.n_hid).permute(1,2,0,3).contiguous()
+        hid = self.out_enc(self.hid_dp(hid).view(self.n_layers, bs, 2*self.n_hid))
+
+        dec_inp = inp.new_zeros(bs).long() + self.bos_idx
+        res = []
+        for i in range(self.max_len):
+            emb = self.emb_dec(dec_inp).unsqueeze(1)
+            outp, hid = self.decoder(emb, hid)
+            outp = self.out(self.out_drop(outp[:,0]))
+            res.append(outp)
+            dec_inp = outp.data.max(1)[1]
+            if (dec_inp==self.pad_idx).all(): break
+            if (targ is not None) and (random.random()<self.pr_force):
+                if i>=targ.shape[1]: break
+                dec_inp = targ[:,i]
+        return torch.stack(res, dim=1)
+    
+    def initHidden(self, bs): return one_param(self).new_zeros(2*self.n_layers, bs, self.n_hid)
+
+
+emb_enc = torch.load(path/'models'/'fr_emb.pth')
+emb_dec = torch.load(path/'models'/'en_emb.pth')
+
+model = Seq2SeqQRNN(emb_enc, emb_dec, 256, 30, n_layers=2)
+learn = Learner(data, model, loss_func=seq2seq_loss, metrics=[seq2seq_acc, CorpusBLEU(len(data.y.vocab.itos))],
+                callback_fns=partial(TeacherForcing, end_epoch=8))
+
+learn.lr_find()
+
+learn.recorder.plot()
+
+learn.fit_one_cycle(8, 1e-2)
+
+inputs, targets, outputs = get_predictions(learn)
+
+inputs[700], targets[700], outputs[700]
+
+inputs[701], targets[701], outputs[701]
+
+inputs[4001], targets[4001], outputs[4001]
+
+
+# +
+#get_bleu(learn)
+# -
+
+# ## Attention
+
+# Attention is a technique that uses the output of our encoder: instead of discarding it entirely, we use it with our hidden state to pay attention to specific words in the input sentence for the predictions in the output sentence. Specifically, we compute attention weights, then add to the input of the decoder the linear combination of the output of the encoder, with those attention weights.
+
+def init_param(*sz): return nn.Parameter(torch.randn(sz)/math.sqrt(sz[0]))
+
+
+class Seq2SeqQRNN(nn.Module):
+    def __init__(self, emb_enc, emb_dec, n_hid, max_len, n_layers=2, p_inp:float=0.15, p_enc:float=0.25, 
+                 p_dec:float=0.1, p_out:float=0.35, p_hid:float=0.05, bos_idx:int=0, pad_idx:int=1):
+        super().__init__()
+        self.n_layers,self.n_hid,self.max_len,self.bos_idx,self.pad_idx = n_layers,n_hid,max_len,bos_idx,pad_idx
+        self.emb_enc = emb_enc
+        self.emb_enc_drop = nn.Dropout(p_inp)
+        self.encoder = QRNN(emb_enc.weight.size(1), n_hid, n_layers=n_layers, dropout=p_enc, bidirectional=True)
+        self.out_enc = nn.Linear(2*n_hid, emb_enc.weight.size(1), bias=False)
+        self.hid_dp  = nn.Dropout(p_hid)
+        self.emb_dec = emb_dec
+        emb_sz = emb_dec.weight.size(1)
+        self.decoder = QRNN(emb_sz + 2*n_hid, emb_dec.weight.size(1), n_layers=n_layers, dropout=p_dec)
+        self.out_drop = nn.Dropout(p_out)
+        self.out = nn.Linear(emb_sz, emb_dec.weight.size(0))
+        self.out.weight.data = self.emb_dec.weight.data #Try tying
+        self.enc_att = nn.Linear(2*n_hid, emb_sz, bias=False)
+        self.hid_att = nn.Linear(emb_sz, emb_sz)
+        self.V =  init_param(emb_sz)
+        self.pr_force = 0.
+        
+    def forward(self, inp, targ=None):
+        bs,sl = inp.size()
+        hid = self.initHidden(bs)
+        emb = self.emb_enc_drop(self.emb_enc(inp))
+        enc_out, hid = self.encoder(emb, hid)
+        
+        hid = hid.view(2,self.n_layers, bs, self.n_hid).permute(1,2,0,3).contiguous()
+        hid = self.out_enc(self.hid_dp(hid).view(self.n_layers, bs, 2*self.n_hid))
+
+        dec_inp = inp.new_zeros(bs).long() + self.bos_idx
+        res = []
+        enc_att = self.enc_att(enc_out)
+        for i in range(self.max_len):
+            hid_att = self.hid_att(hid[-1])
+            u = torch.tanh(enc_att + hid_att[:,None])
+            attn_wgts = F.softmax(u @ self.V, 1)
+            ctx = (attn_wgts[...,None] * enc_out).sum(1)
+            emb = self.emb_dec(dec_inp)
+            outp, hid = self.decoder(torch.cat([emb, ctx], 1)[:,None], hid)
+            outp = self.out(self.out_drop(outp[:,0]))
+            res.append(outp)
+            dec_inp = outp.data.max(1)[1]
+            if (dec_inp==self.pad_idx).all(): break
+            if (targ is not None) and (random.random()<self.pr_force):
+                if i>=targ.shape[1]: break
+                dec_inp = targ[:,i]
+        return torch.stack(res, dim=1)
+    
+    def initHidden(self, bs): return one_param(self).new_zeros(2*self.n_layers, bs, self.n_hid)
+
+
+emb_enc = torch.load(path/'models'/'fr_emb.pth')
+emb_dec = torch.load(path/'models'/'en_emb.pth')
+
+model = Seq2SeqQRNN(emb_enc, emb_dec, 256, 30, n_layers=2)
+learn = Learner(data, model, loss_func=seq2seq_loss, metrics=[seq2seq_acc, CorpusBLEU(len(data.y.vocab.itos))],
+                callback_fns=partial(TeacherForcing, end_epoch=8))
+
+learn.lr_find()
+
+learn.recorder.plot()
+
+learn.fit_one_cycle(8, 3e-3)
+
+inputs, targets, outputs = get_predictions(learn)
+
+inputs[700], targets[700], outputs[700]
+
+inputs[701], targets[701], outputs[701]
+
+inputs[4002], targets[4002], outputs[4002]
+
+
diff --git a/nbs/dl2/translation_transformer.ipynb b/nbs/dl2/translation_transformer.ipynb
index bd2bddd..660cfa2 100644
--- ./nbs/dl2/translation_transformer.ipynb
+++ ./nbs/dl2/translation_transformer.ipynb
@@ -1552,6 +1552,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/nbs/dl2/translation_transformer.py b/nbs/dl2/translation_transformer.py
new file mode 100644
index 0000000..82ee11b
--- /dev/null
+++ ./nbs/dl2/translation_transformer.py
@@ -0,0 +1,416 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+from fastai.text import *
+
+# +
+# Note: You need to run translation.ipynb nb before running this one to get the data set up
+
+path = Config().data_path()/'giga-fren'
+path.ls()
+
+
+# -
+
+# ## Load data
+
+# We reuse the same functions as in the translation notebook to load our data.
+
+def seq2seq_collate(samples:BatchSamples, pad_idx:int=1, pad_first:bool=True, backwards:bool=False) -> Tuple[LongTensor, LongTensor]:
+    "Function that collect samples and adds padding. Flips token order if needed"
+    samples = to_data(samples)
+    max_len_x,max_len_y = max([len(s[0]) for s in samples]),max([len(s[1]) for s in samples])
+    res_x = torch.zeros(len(samples), max_len_x).long() + pad_idx
+    res_y = torch.zeros(len(samples), max_len_y).long() + pad_idx
+    if backwards: pad_first = not pad_first
+    for i,s in enumerate(samples):
+        if pad_first: 
+            res_x[i,-len(s[0]):],res_y[i,-len(s[1]):] = LongTensor(s[0]),LongTensor(s[1])
+        else:         
+            res_x[i,:len(s[0]):],res_y[i,:len(s[1]):] = LongTensor(s[0]),LongTensor(s[1])
+    if backwards: res_x,res_y = res_x.flip(1),res_y.flip(1)
+    return res_x, res_y
+
+
+class Seq2SeqDataBunch(TextDataBunch):
+    "Create a `TextDataBunch` suitable for training an RNN classifier."
+    @classmethod
+    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs:int=32, val_bs:int=None, pad_idx=1,
+               pad_first=False, device:torch.device=None, no_check:bool=False, backwards:bool=False, **dl_kwargs) -> DataBunch:
+        "Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`"
+        datasets = cls._init_ds(train_ds, valid_ds, test_ds)
+        val_bs = ifnone(val_bs, bs)
+        collate_fn = partial(seq2seq_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)
+        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs//2)
+        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)
+        dataloaders = [train_dl]
+        for ds in datasets[1:]:
+            lengths = [len(t) for t in ds.x.items]
+            sampler = SortSampler(ds.x, key=lengths.__getitem__)
+            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))
+        return cls(*dataloaders, path=path, device=device, collate_fn=collate_fn, no_check=no_check)
+
+
+class Seq2SeqTextList(TextList):
+    _bunch = Seq2SeqDataBunch
+    _label_cls = TextList
+
+
+# Refer to the translation notebook for creation of 'questions_easy.csv'.
+
+df = pd.read_csv(path/'questions_easy.csv')
+
+src = Seq2SeqTextList.from_df(df, path = path, cols='fr').split_by_rand_pct().label_from_df(cols='en', label_cls=TextList)
+
+np.percentile([len(o) for o in src.train.x.items] + [len(o) for o in src.valid.x.items], 90)
+
+np.percentile([len(o) for o in src.train.y.items] + [len(o) for o in src.valid.y.items], 90)
+
+# As before, we remove questions with more than 30 tokens.
+
+src = src.filter_by_func(lambda x,y: len(x) > 30 or len(y) > 30)
+
+len(src.train) + len(src.valid)
+
+data = src.databunch()
+
+data.save()
+
+# Can load from here when restarting.
+
+data = load_data(path)
+
+data.show_batch()
+
+
+# ## Transformer model
+
+# ![Transformer model](images/Transformer.png)
+
+# ### Shifting
+
+# We add a transform to the dataloader that shifts the targets right and adds a padding at the beginning.
+
+def shift_tfm(b):
+    x,y = b
+    y = F.pad(y, (1, 0), value=1)
+    return [x,y[:,:-1]], y[:,1:]
+
+
+data.add_tfm(shift_tfm)
+
+
+# ### Embeddings
+
+# The input and output embeddings are traditional PyTorch embeddings (and we can use pretrained vectors if we want to). The transformer model isn't a recurrent one, so it has no idea of the relative positions of the words. To help it with that, they had to the input embeddings a positional encoding which is cosine of a certain frequency:
+
+class PositionalEncoding(nn.Module):
+    "Encode the position with a sinusoid."
+    def __init__(self, d:int):
+        super().__init__()
+        self.register_buffer('freq', 1 / (10000 ** (torch.arange(0., d, 2.)/d)))
+    
+    def forward(self, pos:Tensor):
+        inp = torch.ger(pos, self.freq)
+        enc = torch.cat([inp.sin(), inp.cos()], dim=-1)
+        return enc
+
+
+tst_encoding = PositionalEncoding(20)
+res = tst_encoding(torch.arange(0,100).float())
+_, ax = plt.subplots(1,1)
+for i in range(1,5): ax.plot(res[:,i])
+
+
+class TransformerEmbedding(nn.Module):
+    "Embedding + positional encoding + dropout"
+    def __init__(self, vocab_sz:int, emb_sz:int, inp_p:float=0.):
+        super().__init__()
+        self.emb_sz = emb_sz
+        self.embed = embedding(vocab_sz, emb_sz)
+        self.pos_enc = PositionalEncoding(emb_sz)
+        self.drop = nn.Dropout(inp_p)
+    
+    def forward(self, inp): 
+        pos = torch.arange(0, inp.size(1), device=inp.device).float()
+        return self.drop(self.embed(inp) * math.sqrt(self.emb_sz) + self.pos_enc(pos))
+
+
+# ### Feed forward
+
+# The feed forward cell is easy: it's just two linear layers with a skip connection and a LayerNorm.
+
+def feed_forward(d_model:int, d_ff:int, ff_p:float=0., double_drop:bool=True):
+    layers = [nn.Linear(d_model, d_ff), nn.ReLU()]
+    if double_drop: layers.append(nn.Dropout(ff_p))
+    return SequentialEx(*layers, nn.Linear(d_ff, d_model), nn.Dropout(ff_p), MergeLayer(), nn.LayerNorm(d_model))
+
+
+# ### Multi-head attention
+
+# ![Multi head attention](images/attention.png)
+
+class MultiHeadAttention(nn.Module):
+    "MutiHeadAttention."
+    
+    def __init__(self, n_heads:int, d_model:int, d_head:int=None, resid_p:float=0., attn_p:float=0., bias:bool=True,
+                 scale:bool=True):
+        super().__init__()
+        d_head = ifnone(d_head, d_model//n_heads)
+        self.n_heads,self.d_head,self.scale = n_heads,d_head,scale
+        self.q_wgt = nn.Linear(d_model, n_heads * d_head, bias=bias)
+        self.k_wgt = nn.Linear(d_model, n_heads * d_head, bias=bias)
+        self.v_wgt = nn.Linear(d_model, n_heads * d_head, bias=bias)
+        self.out = nn.Linear(n_heads * d_head, d_model, bias=bias)
+        self.drop_att,self.drop_res = nn.Dropout(attn_p),nn.Dropout(resid_p)
+        self.ln = nn.LayerNorm(d_model)
+        
+    def forward(self, q:Tensor, k:Tensor, v:Tensor, mask:Tensor=None):
+        return self.ln(q + self.drop_res(self.out(self._apply_attention(q, k, v, mask=mask))))
+    
+    def _apply_attention(self, q:Tensor, k:Tensor, v:Tensor, mask:Tensor=None):
+        bs,seq_len = q.size(0),q.size(1)
+        wq,wk,wv = self.q_wgt(q),self.k_wgt(k),self.v_wgt(v)
+        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))
+        wq,wk,wv = wq.permute(0, 2, 1, 3),wk.permute(0, 2, 3, 1),wv.permute(0, 2, 1, 3)
+        attn_score = torch.matmul(wq, wk)
+        if self.scale: attn_score = attn_score.div_(self.d_head ** 0.5)
+        if mask is not None: 
+            attn_score = attn_score.float().masked_fill(mask, -float('inf')).type_as(attn_score)
+        attn_prob = self.drop_att(F.softmax(attn_score, dim=-1))
+        attn_vec = torch.matmul(attn_prob, wv)
+        return attn_vec.permute(0, 2, 1, 3).contiguous().contiguous().view(bs, seq_len, -1)
+        
+    def _attention_einsum(self, q:Tensor, k:Tensor, v:Tensor, mask:Tensor=None):
+        # Permute and matmul is a little bit faster but this implementation is more readable
+        bs,seq_len = q.size(0),q.size(1)
+        wq,wk,wv = self.q_wgt(q),self.k_wgt(k),self.v_wgt(v)
+        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))
+        attn_score = torch.einsum('bind,bjnd->bijn', (wq, wk))
+        if self.scale: attn_score = attn_score.mul_(1/(self.d_head ** 0.5))
+        if mask is not None: 
+            attn_score = attn_score.float().masked_fill(mask, -float('inf')).type_as(attn_score)
+        attn_prob = self.drop_att(F.softmax(attn_score, dim=2))
+        attn_vec = torch.einsum('bijn,bjnd->bind', (attn_prob, wv))
+        return attn_vec.contiguous().view(bs, seq_len, -1)
+
+
+# ### Masking
+
+# The attention layer uses a mask to avoid paying attention to certain timesteps. The first thing is that we don't really want the network to pay attention to the padding, so we're going to mask it. The second thing is that since this model isn't recurrent, we need to mask (in the output) all the tokens we're not supposed to see yet (otherwise it would be cheating).
+
+def get_padding_mask(inp, pad_idx:int=1):
+    return None
+    return (inp == pad_idx)[:,None,:,None]
+
+
+def get_output_mask(inp, pad_idx:int=1):
+    return torch.triu(inp.new_ones(inp.size(1),inp.size(1)), diagonal=1)[None,None].byte()
+    return ((inp == pad_idx)[:,None,:,None].long() + torch.triu(inp.new_ones(inp.size(1),inp.size(1)), diagonal=1)[None,None] != 0)
+
+
+# Example of mask for the future tokens:
+
+torch.triu(torch.ones(10,10), diagonal=1).byte()
+
+
+# ### Encoder and decoder blocks
+
+# We are now ready to regroup these layers in the blocks we add in the model picture:
+#
+# ![Transformer model](images/Transformer.png)
+
+class EncoderBlock(nn.Module):
+    "Encoder block of a Transformer model."
+    #Can't use Sequential directly cause more than one input...
+    def __init__(self, n_heads:int, d_model:int, d_head:int, d_inner:int, resid_p:float=0., attn_p:float=0., ff_p:float=0.,
+                 bias:bool=True, scale:bool=True, double_drop:bool=True):
+        super().__init__()
+        self.mha = MultiHeadAttention(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)
+        self.ff  = feed_forward(d_model, d_inner, ff_p=ff_p, double_drop=double_drop)
+    
+    def forward(self, x:Tensor, mask:Tensor=None): return self.ff(self.mha(x, x, x, mask=mask))
+
+
+class DecoderBlock(nn.Module):
+    "Decoder block of a Transformer model."
+    #Can't use Sequential directly cause more than one input...
+    def __init__(self, n_heads:int, d_model:int, d_head:int, d_inner:int, resid_p:float=0., attn_p:float=0., ff_p:float=0.,
+                 bias:bool=True, scale:bool=True, double_drop:bool=True):
+        super().__init__()
+        self.mha1 = MultiHeadAttention(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)
+        self.mha2 = MultiHeadAttention(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)
+        self.ff   = feed_forward(d_model, d_inner, ff_p=ff_p, double_drop=double_drop)
+    
+    def forward(self, x:Tensor, enc:Tensor, mask_in:Tensor=None, mask_out:Tensor=None): 
+        y = self.mha1(x, x, x, mask_out)
+        return self.ff(self.mha2(y, enc, enc, mask=mask_in))
+
+
+# ### The whole model
+
+class Transformer(nn.Module):
+    "Transformer model"
+    
+    def __init__(self, inp_vsz:int, out_vsz:int, n_layers:int=6, n_heads:int=8, d_model:int=256, d_head:int=32, 
+                 d_inner:int=1024, inp_p:float=0.1, resid_p:float=0.1, attn_p:float=0.1, ff_p:float=0.1, bias:bool=True, 
+                 scale:bool=True, double_drop:bool=True, pad_idx:int=1):
+        super().__init__()
+        self.enc_emb = TransformerEmbedding(inp_vsz, d_model, inp_p)
+        self.dec_emb = TransformerEmbedding(out_vsz, d_model, 0.)
+        self.encoder = nn.ModuleList([EncoderBlock(n_heads, d_model, d_head, d_inner, resid_p, attn_p, 
+                                                   ff_p, bias, scale, double_drop) for _ in range(n_layers)])
+        self.decoder = nn.ModuleList([DecoderBlock(n_heads, d_model, d_head, d_inner, resid_p, attn_p, 
+                                                   ff_p, bias, scale, double_drop) for _ in range(n_layers)])
+        self.out = nn.Linear(d_model, out_vsz)
+        self.out.weight = self.dec_emb.embed.weight
+        self.pad_idx = pad_idx
+        
+    def forward(self, inp, out):
+        mask_in  = get_padding_mask(inp, self.pad_idx)
+        mask_out = get_output_mask (out, self.pad_idx)
+        enc,out = self.enc_emb(inp),self.dec_emb(out)
+        for enc_block in self.encoder: enc = enc_block(enc, mask_in)
+        for dec_block in self.decoder: out = dec_block(out, enc, mask_in, mask_out)
+        return self.out(out)
+
+
+# #### Bleu metric (see dedicated notebook)
+
+class NGram():
+    def __init__(self, ngram, max_n=5000): self.ngram,self.max_n = ngram,max_n
+    def __eq__(self, other):
+        if len(self.ngram) != len(other.ngram): return False
+        return np.all(np.array(self.ngram) == np.array(other.ngram))
+    def __hash__(self): return int(sum([o * self.max_n**i for i,o in enumerate(self.ngram)]))
+
+
+def get_grams(x, n, max_n=5000):
+    return x if n==1 else [NGram(x[i:i+n], max_n=max_n) for i in range(len(x)-n+1)]
+
+
+def get_correct_ngrams(pred, targ, n, max_n=5000):
+    pred_grams,targ_grams = get_grams(pred, n, max_n=max_n),get_grams(targ, n, max_n=max_n)
+    pred_cnt,targ_cnt = Counter(pred_grams),Counter(targ_grams)
+    return sum([min(c, targ_cnt[g]) for g,c in pred_cnt.items()]),len(pred_grams)
+
+
+class CorpusBLEU(Callback):
+    def __init__(self, vocab_sz):
+        self.vocab_sz = vocab_sz
+        self.name = 'bleu'
+    
+    def on_epoch_begin(self, **kwargs):
+        self.pred_len,self.targ_len,self.corrects,self.counts = 0,0,[0]*4,[0]*4
+    
+    def on_batch_end(self, last_output, last_target, **kwargs):
+        last_output = last_output.argmax(dim=-1)
+        for pred,targ in zip(last_output.cpu().numpy(),last_target.cpu().numpy()):
+            self.pred_len += len(pred)
+            self.targ_len += len(targ)
+            for i in range(4):
+                c,t = get_correct_ngrams(pred, targ, i+1, max_n=self.vocab_sz)
+                self.corrects[i] += c
+                self.counts[i]   += t
+    
+    def on_epoch_end(self, last_metrics, **kwargs):
+        precs = [c/t for c,t in zip(self.corrects,self.counts)]
+        len_penalty = exp(1 - self.targ_len/self.pred_len) if self.pred_len < self.targ_len else 1
+        bleu = len_penalty * ((precs[0]*precs[1]*precs[2]*precs[3]) ** 0.25)
+        return add_metrics(last_metrics, bleu)
+
+
+# ### Training
+
+model = Transformer(len(data.train_ds.x.vocab.itos), len(data.train_ds.y.vocab.itos), d_model=256)
+
+learn = Learner(data, model, metrics=[accuracy, CorpusBLEU(len(data.train_ds.y.vocab.itos))], 
+                loss_func = CrossEntropyFlat())
+
+learn.lr_find()
+
+learn.recorder.plot()
+
+learn.fit_one_cycle(8, 5e-4, div_factor=5)
+
+
+def get_predictions(learn, ds_type=DatasetType.Valid):
+    learn.model.eval()
+    inputs, targets, outputs = [],[],[]
+    with torch.no_grad():
+        for xb,yb in progress_bar(learn.dl(ds_type)):
+            out = learn.model(*xb)
+            for x,y,z in zip(xb[0],xb[1],out):
+                inputs.append(learn.data.train_ds.x.reconstruct(x))
+                targets.append(learn.data.train_ds.y.reconstruct(y))
+                outputs.append(learn.data.train_ds.y.reconstruct(z.argmax(1)))
+    return inputs, targets, outputs
+
+
+inputs, targets, outputs = get_predictions(learn)
+
+inputs[10],targets[10],outputs[10]
+
+inputs[700],targets[700],outputs[700]
+
+inputs[701],targets[701],outputs[701]
+
+inputs[2500],targets[2500],outputs[2500]
+
+inputs[4002],targets[4002],outputs[4002]
+
+# ### Label smoothing
+
+# They point out in the paper that using label smoothing helped getting a better BLEU/accuracy, even if it made the loss worse.
+
+model = Transformer(len(data.train_ds.x.vocab.itos), len(data.train_ds.y.vocab.itos), d_model=256)
+
+learn = Learner(data, model, metrics=[accuracy, CorpusBLEU(len(data.train_ds.y.vocab.itos))], 
+                loss_func=FlattenedLoss(LabelSmoothingCrossEntropy, axis=-1))
+
+learn.fit_one_cycle(8, 5e-4, div_factor=5)
+
+learn.fit_one_cycle(8, 5e-4, div_factor=5)
+
+print("Quels sont les atouts particuliers du Canada en recherche sur l'obésité sur la scène internationale ?")
+print("What are Specific strengths canada strengths in obesity - ? are up canada ? from international international stage ?")
+print("Quelles sont les répercussions politiques à long terme de cette révolution scientifique mondiale ?")
+print("What are the long the long - term policies implications of this global scientific ? ?")
+
+inputs[10],targets[10],outputs[10]
+
+inputs[700],targets[700],outputs[700]
+
+inputs[701],targets[701],outputs[701]
+
+inputs[4001],targets[4001],outputs[4001]
+
+# ### Test leakage
+
+# If we change a token in the targets at position n, it shouldn't impact the predictions before that.
+
+learn.model.eval();
+
+xb,yb = data.one_batch(cpu=False)
+
+inp1,out1 = xb[0][:1],xb[1][:1]
+inp2,out2 = inp1.clone(),out1.clone()
+out2[0,15] = 10
+
+y1 = learn.model(inp1, out1)
+y2 = learn.model(inp2, out2)
+
+(y1[0,:15] - y2[0,:15]).abs().mean()
diff --git a/nbs/swift/00_load_data.ipynb b/nbs/swift/00_load_data.ipynb
index eabdccc..37a17b9 100644
--- ./nbs/swift/00_load_data.ipynb
+++ ./nbs/swift/00_load_data.ipynb
@@ -719,6 +719,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/00_load_data.py b/nbs/swift/00_load_data.py
new file mode 100644
index 0000000..f798b80
--- /dev/null
+++ ./nbs/swift/00_load_data.py
@@ -0,0 +1,307 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+%install-location $cwd/swift-install
+%install '.package(url: "https://github.com/mxcl/Path.swift", from: "0.16.1")' Path
+%install '.package(url: "https://github.com/saeta/Just", from: "0.7.2")' Just
+%install '.package(url: "https://github.com/latenitesoft/NotebookExport", from: "0.5.0")' NotebookExport
+
+# Currently there's a bug in swift-jupyter which requires we define any custom operators here:
+
+# +
+// export
+precedencegroup ExponentiationPrecedence {
+    associativity: right
+    higherThan: MultiplicationPrecedence
+}
+infix operator ** : ExponentiationPrecedence
+
+precedencegroup CompositionPrecedence { associativity: left }
+infix operator >| : CompositionPrecedence
+# -
+
+# ## Getting the MNIST dataset
+
+# We will use a couple of libraries here:
+# - [Just](https://github.com/JustHTTP/Just) does the equivalent of requests in python
+# - [Path](https://github.com/mxcl/Path.swift) is even better than its python counterpart
+# - Foundation is the base library from Apple
+
+//export
+import Foundation
+import Just
+import Path
+
+# We will need to gunzip, untar or unzip files we download, so instead of grabbing one library for each, we implement a function that can execute any shell command.
+
+//export
+public extension String {
+    @discardableResult
+    func shell(_ args: String...) -> String
+    {
+        let (task,pipe) = (Process(),Pipe())
+        task.executableURL = URL(fileURLWithPath: self)
+        (task.arguments,task.standardOutput) = (args,pipe)
+        do    { try task.run() }
+        catch { print("Unexpected error: \(error).") }
+
+        let data = pipe.fileHandleForReading.readDataToEndOfFile()
+        return String(data: data, encoding: String.Encoding.utf8) ?? ""
+    }
+}
+
+print("/bin/ls".shell("-lh"))
+
+# To download a file, we use the `Just` library.
+
+//export
+public func downloadFile(_ url: String, dest: String? = nil, force: Bool = false) {
+    let dest_name = dest ?? (Path.cwd/url.split(separator: "/").last!).string
+    let url_dest = URL(fileURLWithPath: (dest ?? (Path.cwd/url.split(separator: "/").last!).string))
+    if !force && Path(dest_name)!.exists { return }
+
+    print("Downloading \(url)...")
+
+    if let cts = Just.get(url).content {
+        do    {try cts.write(to: URL(fileURLWithPath:dest_name))}
+        catch {print("Can't write to \(url_dest).\n\(error)")}
+    } else {
+        print("Can't reach \(url)")
+    }
+}
+
+downloadFile("https://storage.googleapis.com/cvdf-datasets/mnist/train-images-idx3-ubyte.gz")
+
+# Then we will need to read our data and convert it into a `Tensor`:
+
+//export
+import TensorFlow
+
+# The following is generic over the element type on the return value.  We could define two functions like this:
+#
+# ```swift
+# func loadMNIST(training: Bool, labels: Bool, path: Path, flat: Bool) -> Tensor<Float> {
+# func loadMNIST(training: Bool, labels: Bool, path: Path, flat: Bool) -> Tensor<Int32> {
+# ```
+#
+# but that would be boring.  So we make loadMNIST take a "type parameter" `T` which indicates what sort of element type to load a tensor into.
+
+func loadMNIST<T>(training: Bool, labels: Bool, path: Path, flat: Bool) -> Tensor<T> {
+    let split = training ? "train" : "t10k"
+    let kind = labels ? "labels" : "images"
+    let batch = training ? 60000 : 10000
+    let shape: TensorShape = labels ? [batch] : (flat ? [batch, 784] : [batch, 28, 28])
+    let dropK = labels ? 8 : 16
+    let baseUrl = "https://storage.googleapis.com/cvdf-datasets/mnist/"
+    let fname = split + "-" + kind + "-idx\(labels ? 1 : 3)-ubyte"
+    let file = path/fname
+    if !file.exists {
+        downloadFile("\(baseUrl)\(fname).gz", dest:(path/"\(fname).gz").string)
+        "/bin/gunzip".shell("-fq", (path/"\(fname).gz").string)
+    }
+    let data = try! Data(contentsOf: URL(fileURLWithPath: file.string)).dropFirst(dropK)
+    if labels { return Tensor(data.map(T.init)) }
+    else      { return Tensor(data.map(T.init)).reshaped(to: shape)}
+}
+
+# But this doesn't work because S4TF can't just put any type of data inside a `Tensor`. We have to tell it that this type:
+# - is a type that TF can understand and deal with
+# - is a type that can be applied to the data we read in the byte format
+#
+# We do this by defining a protocol called `ConvertibleFromByte` that inherits from `TensorFlowScalar`. That takes care of the first requirement. The second requirement is dealt with by asking for an `init` method that takes `UInt8`:
+
+//export
+protocol ConvertibleFromByte: TensorFlowScalar {
+    init(_ d:UInt8)
+}
+
+# Then we need to say that `Float` and `Int32` conform to that protocol. They already have the right initializer so we don't have to code anything.
+
+//export
+extension Float : ConvertibleFromByte {}
+extension Int32 : ConvertibleFromByte {}
+
+# Lastly, we write a convenience method for all types that conform to the `ConvertibleFromByte` protocol, that will convert some raw data to a `Tensor` of that type.
+
+//export
+extension Data {
+    func asTensor<T:ConvertibleFromByte>() -> Tensor<T> {
+        return Tensor(map(T.init))
+    }
+}
+
+# And now we can write a generic `loadMNIST` function that can returns tensors of `Float` or `Int32`.
+
+# +
+//export
+func loadMNIST<T: ConvertibleFromByte>
+            (training: Bool, labels: Bool, path: Path, flat: Bool) -> Tensor<T> {
+    let split = training ? "train" : "t10k"
+    let kind = labels ? "labels" : "images"
+    let batch = training ? 60000 : 10000
+    let shape: TensorShape = labels ? [batch] : (flat ? [batch, 784] : [batch, 28, 28])
+    let dropK = labels ? 8 : 16
+    let baseUrl = "https://storage.googleapis.com/cvdf-datasets/mnist/"
+    let fname = split + "-" + kind + "-idx\(labels ? 1 : 3)-ubyte"
+    let file = path/fname
+    if !file.exists {
+        downloadFile("\(baseUrl)\(fname).gz", dest:(path/"\(fname).gz").string)
+        "/bin/gunzip".shell("-fq", (path/"\(fname).gz").string)
+    }
+    let data = try! Data(contentsOf: URL(fileURLWithPath: file.string)).dropFirst(dropK)
+    if labels { return data.asTensor() }
+    else      { return data.asTensor().reshaped(to: shape)}
+}
+
+public func loadMNIST(path:Path, flat:Bool = false)
+        -> (Tensor<Float>, Tensor<Int32>, Tensor<Float>, Tensor<Int32>) {
+    try! path.mkdir(.p)
+    return (
+        loadMNIST(training: true,  labels: false, path: path, flat: flat) / 255.0,
+        loadMNIST(training: true,  labels: true,  path: path, flat: flat),
+        loadMNIST(training: false, labels: false, path: path, flat: flat) / 255.0,
+        loadMNIST(training: false, labels: true,  path: path, flat: flat)
+    )
+}
+# -
+
+# We will store mnist in this folder so that we don't download it each time we run a notebook:
+
+//export
+public let mnistPath = Path.home/".fastai"/"data"/"mnist_tst"
+
+# The default returns mnist in the image format:
+
+let (xTrain, yTrain, xValid, yValid) = loadMNIST(path: mnistPath)
+xTrain.shape
+
+# We can also ask for it in its flattened form:
+
+let (xTrain, yTrain, xValid, yValid) = loadMNIST(path: mnistPath, flat: true)
+xTrain.shape
+
+# ## Timing
+
+# Here is our time function:
+
+# +
+//export 
+import Dispatch
+
+// ⏰Time how long it takes to run the specified function, optionally taking
+// the average across a number of repetitions.
+public func time(repeating: Int = 1, _ f: () -> ()) {
+    guard repeating > 0 else { return }
+    
+    // Warmup
+    if repeating > 1 { f() }
+    
+    var times = [Double]()
+    for _ in 1...repeating {
+        let start = DispatchTime.now()
+        f()
+        let end = DispatchTime.now()
+        let nanoseconds = Double(end.uptimeNanoseconds - start.uptimeNanoseconds)
+        let milliseconds = nanoseconds / 1e6
+        times.append(milliseconds)
+    }
+    print("average: \(times.reduce(0.0, +)/Double(times.count)) ms,   " +
+          "min: \(times.reduce(times[0], min)) ms,   " +
+          "max: \(times.reduce(times[0], max)) ms")
+}
+# -
+
+time(repeating: 10) {
+    _ = loadMNIST(training: false, labels: false, path: mnistPath, flat: false) as Tensor<Float>
+}
+
+# ## Export
+
+# Searching for a specific pattern with a regular expression isn't easy in swift. The good thing is that with an extension, we can make it easy for us!
+
+// export
+public extension String {
+    func findFirst(pat: String) -> Range<String.Index>? {
+        return range(of: pat, options: .regularExpression)
+    }
+    func hasMatch(pat: String) -> Bool {
+        return findFirst(pat:pat) != nil
+    }
+}
+
+# The foundation library isn't always the most convenient to use... This is how the first line of the following cell is written in it.
+#
+# ```swift
+# let url_fname = URL(fileURLWithPath: fname)
+# let last = fname.lastPathComponent
+# let out_fname = (url_fname.deletingLastPathComponent().appendingPathComponent("FastaiNotebooks", isDirectory: true)
+#      .appendingPathComponent("Sources", isDirectory: true)
+#      .appendingPathComponent("FastaiNotebooks", isDirectory: true).appendingPathComponent(last)
+#      .deletingPathExtension().appendingPathExtension("swift"))
+# ```
+
+# This function parses the underlying json behind a notebook to keep the code in the cells marked with `//export`.
+
+//export
+public func notebookToScript(fname: Path){
+    let newname = fname.basename(dropExtension: true)+".swift"
+    let url = fname.parent/"FastaiNotebooks/Sources/FastaiNotebooks"/newname
+    do {
+        let data = try Data(contentsOf: fname.url)
+        let jsonData = try JSONSerialization.jsonObject(with: data, options: .allowFragments) as! [String: Any]
+        let cells = jsonData["cells"] as! [[String:Any]]
+        var module = """
+/*
+THIS FILE WAS AUTOGENERATED! DO NOT EDIT!
+file to edit: \(fname.lastPathComponent)
+
+*/
+        
+"""
+        for cell in cells {
+            if let source = cell["source"] as? [String], !source.isEmpty, 
+                   source[0].hasMatch(pat: #"^\s*//\s*export\s*$"#) {
+                module.append("\n" + source[1...].joined() + "\n")
+            }
+        }
+        try module.write(to: url, encoding: .utf8)
+    } catch {
+        print("Can't read the content of \(fname)")
+    }
+}
+
+# And this will do all the notebooks in a given folder.
+
+// export
+public func exportNotebooks(_ path: Path) {
+    for entry in try! path.ls()
+    where entry.kind == Entry.Kind.file && 
+          entry.path.basename().hasMatch(pat: #"^\d*_.*ipynb$"#) {
+        print("Converting \(entry)")
+        notebookToScript(fname: entry.path)
+    }
+}
+
+notebookToScript(fname: Path.cwd/"00_load_data.ipynb")
+
+# But now that we implemented it from scratch we're allowed to use it as a package ;). NotebookExport has been written by pcuenq
+# and will make our lives easier.
+
+import NotebookExport
+let exporter = NotebookExport(Path.cwd/"00_load_data.ipynb")
+print(exporter.export(usingPrefix: "FastaiNotebook_"))
+
+
diff --git a/nbs/swift/00a_intro_and_float.ipynb b/nbs/swift/00a_intro_and_float.ipynb
index d726e0f..a508fce 100644
--- ./nbs/swift/00a_intro_and_float.ipynb
+++ ./nbs/swift/00a_intro_and_float.ipynb
@@ -1316,6 +1316,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/00a_intro_and_float.py b/nbs/swift/00a_intro_and_float.py
new file mode 100644
index 0000000..4f692f2
--- /dev/null
+++ ./nbs/swift/00a_intro_and_float.py
@@ -0,0 +1,419 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/FastaiNotebook_00_load_data")' FastaiNotebook_00_load_data
+
+import FastaiNotebook_00_load_data
+
+# # Hello Swift
+
+# ### Constants, variables and types
+
+# The first thing you'll notice about Swift is that it has variables (`var`) and constants (`let`).
+
+// These are integers
+let batchSize = 6
+var b = 1 + (4 * batchSize)
+print("some integers:", batchSize, b)
+
+// These are floating point values.
+var myPi = 3.1
+let π    = Float.pi
+print("some floats: ", myPi, π)
+
+# Constants cannot be changed after they are set:
+
+batchSize = 8
+
+# Swift also has types... and type inference.  This means you often don't have to specify the types, but they are there, and can be used to override type inference:
+#
+
+# +
+// Type inference would produce an integer, but we make this be a float.
+var someFloat : Float = 1 + 4*9
+
+// Jeremy might not like greek letters, but he surely loves emoji 😬
+var 🐶💩 = "dog poo"
+print(🐶💩)
+
+
+// This is the equivalent of the Python 'type' operator.
+print("some types: ", type(of: someFloat), type(of: 🐶💩))
+# -
+
+# ### Whitespace, functions, tuples, and structs in Swift 
+
+# Functions are declare with `func` instead of `def` in Swift.  They include types, and default to having argument labels.
+
+# +
+func distance(x: Float, y: Float) -> Float {
+    return sqrt(x*x+y*y)
+}
+
+// Functions default to having argument labels:
+distance(x: 1.0, y: 2.0)
+# -
+
+# Sometimes argument labels are the wrong thing, and sometimes you want the caller to use a different argument label than you want to use as the name inside of a function.  To support this, Swift allows you to use two names for each argument, and the underscore means "ignore".
+#
+# Swift also has tuples just like Python, and you can use them to return multiple results:
+
+# +
+func sincos(_ value: Float) -> (Float, Float) {
+    return (sin(value), cos(value))
+}
+
+sincos(2*π)
+# -
+
+# Destructuring works like in Python:
+
+let (s, c) = sincos(42)
+s + c
+
+# And you can also access the elements with `.0`, `.1`...
+
+let tupleValue = sincos(213)
+tupleValue.1
+
+# Structures are a little bit like a class in Python.  You can define structures with the `struct` keyword, which gives you a name for a type.  Structs are super efficient (no memory allocation etc) and allow you to access the fields conveniently:
+
+# +
+struct ComplexF {
+    var real, imag : Float
+}
+
+var someComplex = ComplexF(real: 1.0, imag: 12.0)
+print(someComplex)
+# -
+
+# This is very similar to a [dataclass](https://docs.python.org/3/library/dataclasses.html) in Python:
+# ```python
+# @dataclass
+# class ComplexF:
+#     real:float
+#     imag:float
+#
+# someComplex = ComplexF(1.0, 12.0)
+# print(someComplex)
+# ```
+
+# Once you have types, you want to write generic code.  Generics in Swift work very differently than Java or C++ generics, but the syntax is similar.  Let's make this type generic, and add some computed properties:
+#
+#
+
+struct Complex<T : SignedNumeric> {
+    var real, imag : T
+    
+    // This is a read only computed property.
+    var conj : Complex { return Complex(real: real, imag: -imag) }
+    
+    // Here's a computed property with a setter, that returns the imaginary
+    // component negated, just to show how to do this.  A more realistic
+    // use case would be to provide a polar coordinate projection.
+    var imagNegated : T {
+        get { return -imag }
+        set { imag = -newValue }
+    }
+}
+
+# We can then define a `Complex` structure that takes `Int` or `Double`. Swift automatically infers the type.
+
+var complexInt = Complex(real: 1, imag: 12)
+var complexDouble = Complex(real: 1.0, imag: π)
+print(complexInt, complexDouble, separator: "\n")
+
+print("Conj:     ", complexInt.conj)
+
+# The `imagNegated` propery we defined is a *computed property*. It's not a stored value in memory but is calculated when asked to by the user from the structure state. We can define a setter for it that will adapt the stored properties of the structure with respect to the `newValue` we pass.
+
+print("property: ", complexInt.imag, complexInt.imagNegated)
+complexInt.imagNegated = 4
+print(complexInt)
+
+# Swift lets you add things to types that are already defined with **Extensions** to types, using the `extension` keyword.  You can even extend a type implemented by someone else, no problem. Here we give an `add` method to `Complex`:
+
+extension Complex {
+    func add(_ other: Complex) -> Complex {
+        return Complex(real: real + other.real,
+                       imag: imag + other.imag)
+    }
+}
+
+print("☑️Original: ", complexDouble)
+print("⤴️Moved:    ", complexDouble.add(Complex(real: 10, imag: 10)))
+print("2️⃣Doubled:  ", complexDouble.add(complexDouble))
+
+
+# Defining an 'add' method makes me sad though, because this is math! Fortunately **operators** are just functions in Swift, and are defined with `func`. 
+
+# +
+extension Complex {
+    static func + (lhs: Complex, rhs: Complex) -> Complex {
+        return Complex(real: lhs.real + lhs.real, 
+                       imag: lhs.imag + rhs.imag)
+    }
+}
+
+print("added: ", complexInt + complexInt)
+# -
+
+# You can even define your own operators if you feel like it. Please don't get too crazy.
+
+# +
+prefix operator √
+
+// Complex square root returns two different complex numbers.
+prefix func √(value: Complex<Float>) -> (Complex<Float>, Complex<Float>) {
+    // Implemention omitted - just return the same thing for simplicity.
+    return (value, value)
+}
+
+let (root1, root2) = √complexDouble
+print(root1)
+# -
+
+#
+#
+# **Wrapping up:** Okay, that's your basic introduction to Swift.  If you'd like a longer tour about high level Swift language features and concepts, there is an online [guided tour to Swift](https://docs.swift.org/swift-book/GuidedTour/GuidedTour.html) on swift.org.
+#
+# Now lets talk about ... Python?!
+#
+#
+
+# # Swift loves Python 🐍 too
+
+# One of the cool things about Swift for TensorFlow is that we can directly call into Python.  First we import Python into Swift.
+
+import Python
+
+# You can import arbitrary Python modules and directly use them. No wrappers, interface libraries, code generators, or build steps.
+
+public let np = Python.import("numpy")
+public let plt = Python.import("matplotlib.pyplot")
+
+# Err, Python in Swift??  Yep, Swift is super dynamic too, so it can directly talk to the Python runtime.  We'll talk about that later.
+
+let npArray = np.array([1,2,3,4])
+npArray
+
+# Why do we want this?  Well it turns out that the entire datascience ecosystem is in Python, and many of you are comfortable with Python APIs.  Swift can do lots of things, but Python is just fine for basic things like reading files, so there's no need to change what isn't broken!
+#
+
+# # Using Python and matplotlib to visualize data
+
+# `loadMNIST` is defined in workbook 00_load_data. It loads the data into TensorFlow Tensors, which is why we need to import TensorFlow.
+
+import TensorFlow
+let (xTrain, yTrain, xValid, yValid) = loadMNIST(path: mnistPath, flat: true)
+
+# That just loaded a TensorFlow Tensor:
+
+print(type(of: xTrain))
+print(xTrain.shape, yTrain.shape, xValid.shape, yValid.shape, separator: "\n")
+
+
+# For now though we won't talk much about the Tensor API, come back to it later.
+
+# ### Using matplotlib
+#
+# Let's take a look at an example from MNIST. We have already imported numpy and matplotlib.pyplot. This is the equivalent of the magic `%matplotlib inline` in a python notebook, it will let us see the plots:
+
+%include "EnableIPythonDisplay.swift"
+IPythonDisplay.shell.enable_matplotlib("inline")
+
+# It's super easy to turn an example from the Tensor into a NumPy array.
+
+let img = xTrain[0].makeNumpyArray().reshape(28, 28)
+
+# Then we can plot it like this:
+
+plt.figure(figsize: [5,5])
+plt.show(plt.imshow(X: img, cmap: "gray"))
+
+# ### How does this work?
+#
+# Python is a dynamically typed language ... but another way to look at it is that it has exactly one static type.  Swift calls it `PythonObject`.
+
+# +
+print("NumPy Module Type: ", type(of: np))
+print("NumPy Function Type: ", type(of: np.array))
+print("NumPy Array Type: ", type(of: npArray))
+print(npArray*2, npArray[1])
+
+print(Python.type(npArray))
+print(npArray.__class__)
+# -
+
+# This works really well in practice, because Swift is talking to Python dynamically - exactly as it was designed to be used.  You can even import and use the entire [fastai PyTorch framework and use it from Swift](https://github.com/Omarsf/swiftTransit/blob/master/SwiftWrappedLesson1Preliminary.ipynb)!
+
+# # "Impractical" Programming Languages
+#
+# We now have a bit of a grasp on how Swift basics work, but we're taking a lot for granted here!  We are supposed to be building an entire machine learning framework from scratch!
+#
+# Jeremy started by showing you how to implement a MatMul using an array of floating point scalars, "from the foundations".  Apparently, Jeremy thinks this is the foundations:
+#
+# ```python
+# def matmul(a,b):
+#     ar,ac = a.shape # n_rows * n_cols
+#     br,bc = b.shape
+#     assert ac==br
+#     c = torch.zeros(ar, bc)
+#     for i in range(ar):
+#         for j in range(bc):
+#             for k in range(ac): # or br
+#                 c[i,j] += a[i,k] * b[k,j]
+#     return c
+# ```
+#
+# Let's do it properly, this time... by going down to the bedrock.  First let's talk more about what programming languages are, how compilers work, and then we can understand what Swift really is.
+#
+# **Slides**: [What is a Compiler](https://docs.google.com/presentation/d/1dc6o2o-uYGnJeCeyvgsgyk05dBMneArxdICW5vF75oU/edit#slide=id.g5674d3ead7_0_83)
+#
+
+# ## Building Float
+#
+# Ok, now we know how to build Float, Double, Int, etc.  One really nice thing is that this generates really great code.
+
+// Really simple math
+func squareAdd(n: Float) -> Float {
+    let tmp = n * n
+    let result = tmp + 1.0
+    return result
+}
+
+# You can see the generated X86 assembly the [Compiler Explorer](https://godbolt.org/z/Xns8IA)!  It is optimal, and exactly what you get from the Clang C compiler, because it is built on top of the same LLVM code generator.
+#
+# Because `Int` and `Float` are implemented in the standard library, you can actually see the underlying field if you know where to look:
+
+var someInt = 42
+print(type(of: someInt._value))
+print(type(of: π._value))
+
+
+# That said you can't do anything useful with it - only the Swift standard library gets to use the functionality defined in the Builtin module. It is a private interface between the standard library and the compiler, and the stdlib reexports all of its functionality.
+
+import Builtin
+
+# A cool thing is that since these things are normal types, you can add your own operators and methods to them, just like you can with any other type:
+
+# +
+extension Int {
+  var isOdd : Bool { return self & 1 != 0 }
+}
+
+extension Bool {
+  var symbol : String { return self ? "👍" : "👎" }
+}
+
+# -
+
+# String literals can be multi-line, and also support interpolation with `\()`.
+
+print("""
+      Feeling odd?
+        Lets check  4: \(     4.isOdd         )
+        what about 17: \(    17.isOdd         )
+        Lets check  4: \(     4.isOdd.symbol  )
+        What about 17: \(    17.isOdd.symbol  )
+      """)
+
+# A lot of the Swift "language" is implemented in the standard library, including primitive types like `Bool` as well.  Check out how short circuiting `&&` and `||` operators are [implemented right in the standard library](https://github.com/apple/swift/blob/master/stdlib/public/core/Bool.swift#L245).  Even primitives like `assert` are just [functions implemented in Swift](https://github.com/apple/swift/blob/master/stdlib/public/core/Assert.swift#L13).
+#
+#
+# ```swift
+#   public static func && (lhs: Bool, rhs: @autoclosure () -> Bool) -> Bool {
+#     return lhs ? rhs() : false
+#   }
+# ```
+
+# ## Looking inside Array
+#
+# Of course, `Array` and `String` are also written in Swift and provided by the standard library.  Array in Swift contains a pointer to the elements and a size and a reserved capacity.  Let's try it out to see how it works.
+#
+# Arrays work with type inference:
+
+var myArray = [1,2,3,4,5,6]
+
+# You can write array types with `[]` syntax, which is what most people do...
+
+var myArray2 : [Int] = myArray
+
+# But this is just synactic sugar for the `Array` type.
+
+var myArray3 : Array<Int> = myArray
+
+# You can see that all of these have the exact same type, even though they are written in different ways:
+
+print(type(of: myArray), type(of: myArray2), type(of: myArray3))
+
+# Swift arrays support all the normal stuff you'd expect, like iteration, indexing, slicing etc. 
+#
+# Here is a standard `for` loop:
+
+// Standard `for` loop
+for x in myArray {
+    print(x)
+}
+
+# You index or slice with brackets. The Swift slicing operators are `...` for inclusive (including the endpoint) and `..<` for exclusive ranges.  You'll see `..<` the most often:
+
+print("element:", myArray[0])
+print("slice:  ", myArray[1...3])  // inclusive - includes "3"
+print("slice:  ", myArray[1..<3])  // exclusive
+
+# `map` applies a function (closure) to every element in the array. Swift closures are like Python lambda's. You can name the argument to the lambda if you'd like:
+
+print("mapped:   ", myArray.map({ arg in arg + 10 }))
+
+# Arguments can also be anonymous, and default to `$0`, `$1`, `$2`, which are nice for concise functional algorithms. 
+#
+# `filter` returns an array with the elements for which the function returns `true`
+
+print("filtered: ", myArray.filter({ $0.isOdd }))
+
+# Functions with trailing closures can omit the parens or move the closure after the parens, which allows you to write nice and fluent code:
+
+print("oddity:   ", myArray.map{ $0.isOdd.symbol })
+print("processed:", myArray.map{ $0*3 }.filter{ $0.isOdd })
+
+# `map` and `filter` are extremely important functions that we never use in Python because they are inefficient, so you'll have to learn to use them in Swift. The last important function is `reduce` that will compute a value from your array by going from an initial value and applying an operator:
+
+print("sum: ", myArray.reduce(0, +))
+
+# Since `Array` is a type like any other, of course you can put your own methods on it with an extension:
+
+# +
+extension Array where Element : Numeric {
+    func doubleElements() -> Array {
+        return self.map { $0 * 2 }
+    }
+}
+
+print([1,2,3].doubleElements())
+# -
+
+# You might be wondering what the `where Element : Numeric` thing is on that extension.  That is saying that the `doubleElements` method only exists on arrays whose elements are numeric.  Other sorts of arrays don't get this method because they can't multiply the elements by two.  For example, try it out on an array of strings or bool (which aren't considered to be numeric) to see what happens:
+
+print([true, false, true].doubleElements())
+
+# If you are curious about `Array` please dive [into the code in the standard library](https://github.com/apple/swift/blob/master/stdlib/public/core/Array.swift#L300).  It is all written in Swift, but uses somewhat more advanced features than we've introduced so far.  For example, you can see how it [implements subscripting of an element](https://github.com/apple/swift/blob/master/stdlib/public/core/Array.swift#L677).
+#
+# If you're interested in diving into other things, [`Dictionary`](https://github.com/apple/swift/blob/tensorflow/stdlib/public/core/Dictionary.swift#L13) and [`Set`](https://github.com/apple/swift/blob/tensorflow/stdlib/public/core/Set.swift#L13) are also interesting and have some massive doc comments explaining how they work.
+#
+#
+# Now that we have seen how Float and Arrays are defined, we're allowed to use them - so we can define a matmul!  Onward to [01_matmul](https://github.com/fastai/fastai_docs/tree/master/dev_swift).
diff --git a/nbs/swift/01_matmul.ipynb b/nbs/swift/01_matmul.ipynb
index 0e748af..9e960eb 100644
--- ./nbs/swift/01_matmul.ipynb
+++ ./nbs/swift/01_matmul.ipynb
@@ -1439,6 +1439,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/01_matmul.py b/nbs/swift/01_matmul.py
new file mode 100644
index 0000000..b9d708d
--- /dev/null
+++ ./nbs/swift/01_matmul.py
@@ -0,0 +1,486 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/FastaiNotebook_00_load_data")' FastaiNotebook_00_load_data
+
+//export
+import Path
+import TensorFlow
+
+import FastaiNotebook_00_load_data
+
+# ## Get some Tensors to play with
+
+# We can initialize a tensor in lots of different ways because in swift, two functions with the same name can coexist as long as they don't have the same signatures. Different named arguments give different signatures, so all of those are different `init` functions of `Tensor`:
+
+let zeros = Tensor<Float>(zeros: [1,4,5])
+let ones  = Tensor<Float>(ones: [12,4,5])
+let twos  = Tensor<Float>(repeating: 2.0, shape: [2,3,4,5])
+let range = Tensor<Int32>(rangeFrom: 0, to: 32, stride: 1)
+
+# Those are just some examples and there are many more! Here we grab random numbers
+
+let xTrain = Tensor<Float>(randomNormal: [5, 784])
+var weights = Tensor<Float>(randomNormal: [784, 10]) / sqrt(784)
+print(weights[0])
+
+# # Building Matmul
+
+# Ok, now that we know how floating point types and arrays work, we can finally build our own matmul from scratch, using a few loops.  We will take the two input matrices as single dimensional arrays so we can show manual indexing into them, the hard way:
+
+// a and b are the flattened array elements, aDims/bDims are the #rows/columns of the arrays.
+func swiftMatmul(a: [Float], b: [Float], aDims: (Int,Int), bDims: (Int,Int)) -> [Float] {
+    assert(aDims.1 == bDims.0, "matmul shape mismatch")
+    
+    var res = Array(repeating: Float(0.0), count: aDims.0 * bDims.1)
+    for i in 0 ..< aDims.0 {
+        for j in 0 ..< bDims.1 {
+            for k in 0 ..< aDims.1 {
+                res[i*bDims.1+j] += a[i*aDims.1+k] * b[k*bDims.1+j]
+            }
+        }
+    }
+    return res
+}
+
+# To try this out, we extract the scalars out of our MNIST data as an array.
+
+let flatA = xTrain[0..<5].scalars
+let flatB = weights.scalars
+let (aDims,bDims) = ((5, 784), (784, 10))
+
+# Now that we've got everything together, we can try it out!
+
+var resultArray = swiftMatmul(a: flatA, b: flatB, aDims: aDims, bDims: bDims)
+
+time(repeating: 100) {
+    _ = swiftMatmul(a: flatA, b: flatB, aDims: aDims, bDims: bDims)
+}
+
+# Awesome, that is pretty fast - compare that to **835 ms** with Python!
+#
+# You might be wondering what that `time(repeating:)` builtin is.  As you might guess, this is actually a Swift function - one that is using "trailing closure" syntax to specify the body of the timing block.  Trailing closures are passed as arguments to the function, and in this case, the function was defined in our ✅**00_load_data** workbook.  Let's take a look!
+#
+
+# # Getting the performance of C 💯
+
+# This performance is pretty great, but we can do better.  Swift is a memory safe language (like Python), which means it has to do array bounds checks and some other stuff.  Fortunately, Swift is a pragmatic language that allows you to drop through this to get peak performance - check out Jeremy's article [High Performance Numeric Programming with Swift: Explorations and Reflections](https://www.fast.ai/2019/01/10/swift-numerics/) for a deep dive.
+#
+# One thing you can do is use `UnsafePointer` (which is basically a raw C pointer) instead of using a bounds checked array.  This isn't memory safe, but gives us about a 2x speedup in this case!
+
+// a and b are the flattened array elements, aDims/bDims are the #rows/columns of the arrays.
+func swiftMatmulUnsafe(a: UnsafePointer<Float>, b: UnsafePointer<Float>, aDims: (Int,Int), bDims: (Int,Int)) -> [Float] {
+    assert(aDims.1 == bDims.0, "matmul shape mismatch")
+    
+    var res = Array(repeating: Float(0.0), count: aDims.0 * bDims.1)
+    res.withUnsafeMutableBufferPointer { res in 
+        for i in 0 ..< aDims.0 {
+            for j in 0 ..< bDims.1 {
+                for k in 0 ..< aDims.1 {
+                    res[i*bDims.1+j] += a[i*aDims.1+k] * b[k*bDims.1+j]
+                }
+            }
+        }
+    }
+    return res
+}
+
+time(repeating: 100) {
+    _ = swiftMatmulUnsafe(a: flatA, b: flatB, aDims: aDims, bDims: bDims)
+}
+
+# One of the other cool things about this is that we can provide a nice idiomatic API to the caller of this, and keep all the unsafe shenanigans inside the implementation of this function.
+#
+# If you really want to fall down the rabbit hole, you can look at the [implementation of `UnsafePointer`](https://github.com/apple/swift/blob/tensorflow/stdlib/public/core/UnsafePointer.swift), which is of written in Swift wrapping LLVM pointer operations.  This means you can literally get the performance of C code directly in Swift, while providing easy to use high level APIs!
+
+# ## Swift 💖 C APIs too: you get the full utility of the C ecosystem
+
+# Swift even lets you transparently work with C APIs, just like it does with Python.  This can be used for both good and evil.  For example, here we directly call the `malloc` function, dereference the uninitialized pointer, and print it out:
+
+# +
+import Glibc
+
+let ptr : UnsafeMutableRawPointer = malloc(42)
+
+print("☠️☠️ Uninitialized garbage =", ptr.load(as: UInt8.self))
+
+free(ptr)
+# -
+
+# An `UnsafeMutableRawPointer` ([implementation](https://github.com/apple/swift/blob/tensorflow/stdlib/public/core/UnsafeRawPointer.swift)) isn't something you should use lightly, but when you work with C APIs, you'll see various types like that in the function signatures.
+#
+# Calling `malloc` and `free` directly aren't recommended in Swift, but is useful and important when you're working with C APIs that expect to get malloc'd memory, which comes up when you're written a safe Swift wrapper for some existing code.
+#
+# Speaking of existing code, let's take a look at that **Python interop** we touched on before:
+#
+
+# +
+import Python
+let np = Python.import("numpy")
+let pickle = Python.import("pickle")
+let sys = Python.import("sys")
+
+print("🐍list =    ", pickle.dumps([1, 2, 3]))
+print("🐍ndarray = ", pickle.dumps(np.array([[1, 2], [3, 4]])))
+
+# -
+
+# Of course this is [all written in Swift](https://github.com/apple/swift/tree/tensorflow/stdlib/public/Python) as well.  You can probably guess how this works now: `PythonObject` is a Swift struct that wraps a pointer to the Python interpreter's notion of a Python object.
+#
+# ```swift
+# @dynamicCallable
+# @dynamicMemberLookup
+# public struct PythonObject {
+#   var reference: PyReference
+#   ...
+# }
+# ```
+#
+# The `@dynamicMemberLookup` attribute allows it to dynamically handle all member lookups (like `x.y`) by calling into [the `PyObject_GetAttrString` runtime call](https://github.com/apple/swift/blob/tensorflow/stdlib/public/Python/Python.swift#L427).  Similarly, the `@dynamicCallable` attribute allows the type to intercept all calls to a PythonObject (like `x()`), which it implements using the [`PyObject_Call` runtime call](https://github.com/apple/swift/blob/tensorflow/stdlib/public/Python/Python.swift#L324).  
+#
+# Because Swift has such simple and transparent access to C, it allows building very nice first-class Swift APIs that talk directly to the lower level implementation, and these implementations can have very little overhead.
+
+# # Working with Tensor
+
+# Lets get back into matmul and explore more of the `Tensor` type as provided by the TensorFlow module.  You can see all things `Tensor` can do in the [official documentation](https://www.tensorflow.org/swift/api_docs/Structs/Tensor).
+#
+# Here are some highlights. We saw how you can get zeros or random data:
+
+# +
+var bias = Tensor<Float>(zeros: [10])
+
+let m1 = Tensor<Float>(randomNormal: [5, 784])
+let m2 = Tensor<Float>(randomNormal: [784, 10])
+# -
+
+# Tensors carry data and a shape.
+
+print("m1: ", m1.shape)
+print("m2: ", m2.shape)
+
+# The `Tensor` type provides all the normal stuff you'd expect as methods.  Including arithmetic, convolutions, etc and this includes full support for broadcasting:
+#
+
+# +
+let small = Tensor<Float>([[1, 2],
+                           [3, 4]])
+
+print("🔢2x2:\n", small)
+# -
+
+# **MatMul Operator:** In addition to using the global `matmul(a, b)` function, you can also use the `a • b` operator to matmul together two things.  This is just like the `@` operator in Python.  You can get it with the <kbd>option</kbd>-<kbd>8</kbd> on Mac or <kbd>compose</kbd>-<kbd>.</kbd>-<kbd>=</kbd> elsewhere. Or if you prefer, just use the `matmul()` function we've seen already.
+
+print("⊞ matmul:\n",  matmul(small, small))
+print("\n⊞ again:\n", small • small)
+
+
+# **Reshaping** works the way you'd expect:
+
+var m = Tensor([1.0, 2, 3, 4, 5, 6, 7, 8, 9]).reshaped(to: [3, 3])
+print(m)
+
+# You have the basic mathematical functions:
+
+sqrt((m * m).sum())
+
+# ## Elementwise ops and comparisons
+
+# Standard math operators (`+`,`-`,`*`,`/`) are all element-wise, and there are a bunch of standard math functions like `sqrt` and `pow`.  Here are some examples:
+
+var a = Tensor([10.0, 6, -4])
+var b = Tensor([2.0, 8, 7])
+(a,b)
+
+print("add:  ", a + b)
+print("mul:  ", a * b)
+print("sqrt: ", sqrt(a))
+print("pow:  ", pow(a, b))
+
+#
+# **Comparison operators** (`>`,`<`,`==`,`!=`,...) in Swift are supposed to return a single `Bool` value, so they are `true` if all the elements of the tensors satisfy the comparison.
+#
+# Elementwise versions have the `.` prefix, which is read as "pointwise": `.>`, `.<`, `.==`, etc.  You can merge a tensor of bools into a single Bool with the `any()` and `all()` methods.
+
+a < b
+
+a .< b
+
+print((a .> 0).all())
+
+print((a .> 0).any())
+
+# ## Broadcasting
+
+# Broadcasting with a scalar works just like in Python:
+
+var a = Tensor([10.0, 6.0, -4.0])
+
+print(a+1)
+
+2 * m
+
+# #### Broadcasting a vector with a matrix
+
+let c = Tensor([10.0,20.0,30.0])
+
+# By default, broadcasting is done by adding 1 dimensions to the beginning until dimensions of both objects match.
+
+m + c
+
+c + m
+
+# To broadcast on the other dimensions, one has to use `expandingShape` to add the dimension.
+
+m + c.expandingShape(at: 1)
+
+c.expandingShape(at: 1)
+
+# #### Broadcasting rules
+
+print(c.expandingShape(at: 0).shape)
+
+print(c.expandingShape(at: 1).shape)
+
+c.expandingShape(at: 0) * c.expandingShape(at: 1)
+
+c.expandingShape(at: 0) .> c.expandingShape(at: 1)
+
+# # Matmul using `Tensor`
+#
+# Coming back to our matmul algorithm, we can implement exactly what we had before by using subscripting into a tensor, instead of subscripting into an array.  Let's see how that works:
+
+# +
+func tensorMatmul(_ a: Tensor<Float>, _ b: Tensor<Float>) -> Tensor<Float> {
+    var res = Tensor<Float>(zeros: [a.shape[0], b.shape[1]])
+
+    for i in 0 ..< a.shape[0] {
+        for j in 0 ..< b.shape[1] {
+            for k in 0 ..< a.shape[1] {
+                res[i, j] += a[i, k] * b[k, j]
+            }
+        }
+    }
+    return res
+}
+
+_ = tensorMatmul(m1, m2)
+# -
+
+time { 
+    let tmp = tensorMatmul(m1, m2)
+    
+    // Copy a scalar back to the host to force a GPU sync.
+    _ = tmp[0, 0].scalar
+}
+
+# What, what just happened?? We used to be less than a **tenth of a millisecond**, now we're taking **multiple seconds**.  It turns out that Tensor's are very good at bulk data processing, but they are not good at doing one float at a time.  Make sure to use the coarse-grained operations.  We can make this faster by vectorizing each loop in turn.
+#
+# **Slides:** [Granularity of Tensor Operations](https://docs.google.com/presentation/d/1dc6o2o-uYGnJeCeyvgsgyk05dBMneArxdICW5vF75oU/edit#slide=id.g58253914c1_0_380). 
+
+# ## Vectorize the inner loop into a multiply + sum
+
+# +
+func elementWiseMatmul(_ a:Tensor<Float>, _ b:Tensor<Float>) -> Tensor<Float>{
+    let (ar, ac) = (a.shape[0], a.shape[1])
+    let (br, bc) = (b.shape[0], b.shape[1])
+    var res = Tensor<Float>(zeros: [ac, br])
+    
+    for i in 0 ..< ar {
+        let row = a[i]
+        for j in 0 ..< bc {
+            res[i, j] = (row * b.slice(lowerBounds: [0,j], upperBounds: [ac,j+1]).squeezingShape(at: 1)).sum()
+        }
+    }
+    return res
+}
+
+_ = elementWiseMatmul(m1, m2)
+# -
+
+time { 
+    let tmp = elementWiseMatmul(m1, m2)
+
+    // Copy a scalar back to the host to force a GPU sync.
+    _ = tmp[0, 0].scalar
+}
+
+# ## Vectorize the inner two loops with broadcasting
+
+# +
+func broadcastMatmult(_ a:Tensor<Float>, _ b:Tensor<Float>) -> Tensor<Float>{
+    var res = Tensor<Float>(zeros: [a.shape[0], b.shape[1]])
+    for i in 0..<a.shape[0] {
+        res[i] = (a[i].expandingShape(at: 1) * b).sum(squeezingAxes: 0)
+    }
+    return res
+}
+
+_ = broadcastMatmult(m1, m2)
+# -
+
+time(repeating: 100) {
+    let tmp = broadcastMatmult(m1, m2)
+
+    // Copy a scalar back to the host to force a GPU sync.
+    _ = tmp[0, 0].scalar
+}
+
+# ## Vectorize the whole thing with one Tensorflow op
+
+time(repeating: 100) { _ = m1 • m2 }
+
+# Ok, now that we have matmul, we can continue to build out our framework.
+#
+# To complete today's lesson, let's jump way way up the stack to see **Workbook 11**.
+#
+#
+#
+# ## Tensorflow vectorizes, parallelizes, and scales
+#
+# The reason that TensorFlow works in practice is that it can scale way up to large matrices, for example, lets try some thing a bit larger:
+
+# +
+func timeMatmulTensor(size: Int) {
+    var matrix = Tensor<Float>(randomNormal: [size, size])
+    print("\n\(size)x\(size):\n  ⏰", terminator: "")
+    time(repeating: 10) { 
+        let matrix = matrix • matrix 
+        _ = matrix[0, 0].scalar
+    }
+}
+
+timeMatmulTensor(size: 1)     // Tiny
+timeMatmulTensor(size: 10)    // Bigger
+timeMatmulTensor(size: 100)   // Even Bigger
+timeMatmulTensor(size: 1000)  // Biggerest
+timeMatmulTensor(size: 5000)  // Even Biggerest
+# -
+
+# In constrast, our simple CPU implementation takes a lot longer to do the same work.  For example:
+
+# +
+func timeMatmulSwift(size: Int, repetitions: Int = 10) {
+    var matrix = Tensor<Float>(randomNormal: [size, size])
+    let matrixFlatArray = matrix.scalars
+
+    print("\n\(size)x\(size):\n  ⏰", terminator: "")
+    time(repeating: repetitions) { 
+       _ = swiftMatmulUnsafe(a: matrixFlatArray, b: matrixFlatArray, aDims: (size,size), bDims: (size,size))
+    }
+}
+
+timeMatmulSwift(size: 1)     // Tiny
+timeMatmulSwift(size: 10)    // Bigger
+timeMatmulSwift(size: 100)   // Even Bigger
+timeMatmulSwift(size: 1000, repetitions: 1)  // Biggerest
+
+print("\n5000x5000: skipped, it takes tooo long!")
+# -
+
+# Why is TensorFlow *so so so* much faster than our CPU implementation?  Well there are two reasons: the first of which is that it uses GPU hardware, which is much faster for math like this.  That said, there are a ton of tricks (involving memory hierarchies, cache blocking, and other tricks) that make matrix multiplications go fast on CPUs and other hardware.
+#
+# For example, try using TensorFlow on the CPU to do the same computation as above:
+
+withDevice(.cpu) {
+    timeMatmulTensor(size: 1)     // Tiny
+    timeMatmulTensor(size: 10)    // Bigger
+    timeMatmulTensor(size: 100)   // Even Bigger
+    timeMatmulTensor(size: 1000)  // Biggerest
+    timeMatmulTensor(size: 5000)  // Even Biggerest
+}
+
+# This is a pretty big difference.  On my hardware, it takes 2287ms for Swift to do a 1000x1000 multiply on the CPU, it takes TensorFlow 6.7ms to do the same work on the CPU, and takes TensorFlow 0.49ms to do it on a GPU.
+#
+# # Hardware Accelerators vs Flexibility
+#
+# One of the big challenges with machine learning frameworks today is that they provide a fixed set of "ops" that you can use with high performance.  There is a lot of work underway to fix this.  The [XLA compiler in TensorFlow](https://www.tensorflow.org/xla) is an important piece of this, which allows more flexibility in the programming model while still providing high performance by using compilers to target the hardware accelerator.  If you're interested in the details, there is a [great video by the creator of Halide](https://www.youtube.com/watch?v=3uiEyEKji0M) explaining why this is challenging.
+#
+# TensorFlow internals are undergoing [significant changes (slide)](https://docs.google.com/presentation/d/1dc6o2o-uYGnJeCeyvgsgyk05dBMneArxdICW5vF75oU/edit#slide=id.g58253914c1_3_0) including the introduction of the XLA compiler, and the introduction of [MLIR compiler technology](https://github.com/tensorflow/mlir).
+#
+#
+# # Tensor internals and Raw TensorFlow operations
+#
+# TensorFlow provides hundreds of different operators, and they sort of grew organically over time.  This means that there are some deprecated operators, they aren't particularly consistent, and there are other oddities.  As such, the `Tensor` type provides a curated set of these operators as methods.
+#
+# Whereas `Int` and `Float` are syntactic sugar for LLVM, and `PythonObject` is syntactic sugar for the Python interpreter, `Tensor` ends up being syntactic sugar for the TensorFlow operator set.  You can dive in and see its implementation in Swift in [the S4TF `TensorFlow` module](https://github.com/apple/swift/blob/tensorflow/stdlib/public/TensorFlow/Tensor.swift), e.g.:
+#
+# ```swift
+# public struct Tensor<Scalar : TensorFlowScalar> : TensorProtocol {
+#   /// The underlying `TensorHandle`.
+#   /// - Note: `handle` is public to allow user defined ops, but should not
+#   /// normally be used otherwise.
+#   public let handle: TensorHandle<Scalar>
+#   ... 
+# }
+# ```
+#
+# Here we see the internal implementation details of `Tensor`, which stores a `TensorHandle` - the internal implementation detail of the TensorFlow Eager runtime.
+#
+# Methods are defined on Tensor just like you'd expect, here [is the basic addition operator](https://github.com/apple/swift/blob/tensorflow/stdlib/public/TensorFlow/Ops.swift#L88), defined over all numeric tensors (i.e., not tensors of `Bool`):
+#
+# ```swift
+# extension Tensor : AdditiveArithmetic where Scalar : Numeric {
+#   /// Adds two tensors and produces their sum.
+#   /// - Note: `+` supports broadcasting.
+#   public static func + (lhs: Tensor, rhs: Tensor) -> Tensor {
+#     return Raw.add(lhs, rhs)
+#   }
+# }
+# ```
+#
+# But wait, what is this Raw thing?
+
+# ### Raw TensorFlow ops
+#
+# TensorFlow has a database of the operators it defines, which gets encoded into a [protocol buffer](https://developers.google.com/protocol-buffers/).  From this protobuf, *all* of the operators automatically get a Raw operator (implemented in terms of a lower level `#tfop` primitive).
+#
+
+# +
+// Explore the contents of the Raw namespace by typing Raw.<tab>
+print(Raw.zerosLike(c))
+
+// Raw.
+# -
+
+# There is an [entire tutorial on Raw operators](https://colab.research.google.com/github/tensorflow/swift/blob/master/docs/site/tutorials/raw_tensorflow_operators.ipynb) on github/TensorFlow/swift.  The key thing to know is that TensorFlow can do almost anything, so if there is no obvious method on `Tensor` to do what you need it is worth checking out the tutorial to see how to do this.
+#
+# As one example, later parts of the tutorial need the ability to load files and decode JPEGs.  Swift for TensorFlow doesn't have these as methods on `StringTensor` yet, but we can add them like this:
+
+//export
+public extension StringTensor {
+    // Read a file into a Tensor.
+    init(readFile filename: String) {
+        self.init(readFile: StringTensor(filename))
+    }
+    init(readFile filename: StringTensor) {
+        self = Raw.readFile(filename: filename)
+    }
+
+    // Decode a StringTensor holding a JPEG file into a Tensor<UInt8>.
+    func decodeJpeg(channels: Int = 0) -> Tensor<UInt8> {
+        return Raw.decodeJpeg(contents: self, channels: Int64(channels), dctMethod: "") 
+    }
+}
+
+
+# ### Export
+
+import NotebookExport
+let exporter = NotebookExport(Path.cwd/"01_matmul.ipynb")
+print(exporter.export(usingPrefix: "FastaiNotebook_"))
+
+
diff --git a/nbs/swift/01a_fastai_layers.ipynb b/nbs/swift/01a_fastai_layers.ipynb
index 71048e3..400cef1 100644
--- ./nbs/swift/01a_fastai_layers.ipynb
+++ ./nbs/swift/01a_fastai_layers.ipynb
@@ -766,6 +766,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/01a_fastai_layers.py b/nbs/swift/01a_fastai_layers.py
new file mode 100644
index 0000000..7227709
--- /dev/null
+++ ./nbs/swift/01a_fastai_layers.py
@@ -0,0 +1,549 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+# # "fast.ai" layers
+#
+# This notebook defines layers similar to those defined in the [Swift for TensorFlow Deep Learning Library](https://github.com/tensorflow/swift-apis), but with some experimental extra features for the fast.ai course.
+
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/FastaiNotebook_01_matmul")' FastaiNotebook_01_matmul
+
+//export
+import Path
+import TensorFlow
+
+import FastaiNotebook_01_matmul
+
+# ## Tensor extensions
+
+# All the fastai layers will use kaiming initialization as default, so we write an extension to do this fast.
+
+// export
+public extension Tensor where Scalar: TensorFlowFloatingPoint {
+    init(kaimingNormal shape: TensorShape, negativeSlope: Double = 1.0) {
+        // Assumes Leaky ReLU nonlinearity
+        let gain = Scalar.init(TensorFlow.sqrt(2.0 / (1.0 + TensorFlow.pow(negativeSlope, 2))))
+        let spatialDimCount = shape.count - 2
+        let receptiveField = shape[0..<spatialDimCount].contiguousSize
+        let fanIn = shape[spatialDimCount] * receptiveField
+        self.init(randomNormal: shape)
+        self *= Tensor<Scalar>(gain/TensorFlow.sqrt(Scalar(fanIn)))
+    }
+}
+
+# We shorten the name of standardDeviation to match numpy/PyTorch since we'll have to write it a lot in the notebooks.
+
+// export
+public extension Tensor where Scalar: TensorFlowFloatingPoint {
+    func std() -> Tensor<Scalar> { return standardDeviation() }
+    func std(alongAxes a: [Int]) -> Tensor<Scalar> { return standardDeviation(alongAxes: a) }
+    func std(alongAxes a: Tensor<Int32>) -> Tensor<Scalar> { return standardDeviation(alongAxes: a) }
+    func std(alongAxes a: Int...) -> Tensor<Scalar> { return standardDeviation(alongAxes: a) }
+    func std(squeezingAxes a: [Int]) -> Tensor<Scalar> { return standardDeviation(squeezingAxes: a) }
+    func std(squeezingAxes a: Tensor<Int32>) -> Tensor<Scalar> { return standardDeviation(squeezingAxes: a) }
+    func std(squeezingAxes a: Int...) -> Tensor<Scalar> { return standardDeviation(squeezingAxes: a) }
+}
+
+# # Introducing Protocols
+#
+# Just like PyTorch, the S4TF base library provides a standard layer type.  It is defined in the [TensorFlow/swift-apis](https://github.com/tensorflow/swift-apis/) repository on GitHub in [Layer.swift](https://github.com/tensorflow/swift-apis/blob/master/Sources/TensorFlow/Layer.swift) and it looks like this:
+#
+# ```swift
+# protocol Layer: Differentiable {
+#     /// The input type of the layer.
+#     associatedtype Input: Differentiable
+#     /// The output type of the layer.
+#     associatedtype Output: Differentiable
+#
+#     /// Returns the output obtained from applying the layer to the given input.
+#     ///
+#     /// - Parameter input: The input to the layer.
+#     /// - Returns: The output.
+#     @differentiable
+#     func call(_ input: Input) -> Output
+# }
+# ```
+#
+# There is a lot going on here - before we dive into the details of Layers, lets talk about what protocols are:
+#
+# **Slides**: [Protocols in Swift](https://docs.google.com/presentation/d/1dc6o2o-uYGnJeCeyvgsgyk05dBMneArxdICW5vF75oU/edit#slide=id.g5674d3ead7_0_474)
+#
+
+# # Explaining `Layer`
+#  
+# Let's break down the `Layer` protocol into its pieces:
+#
+# ```swift
+# protocol Layer: Differentiable {
+#     ...
+# }
+# ```
+#
+# This declares a protocol named `Layer` and says that all layers are also `Differentiable`.  `Differentiable` is a yet-another protocol [defined in the Swift standard library](https://github.com/apple/swift/blob/tensorflow/stdlib/public/core/AutoDiff.swift#L99).
+#
+# ```swift
+#     /// Returns the output obtained from applying the layer to the given input.
+#     ///
+#     /// - Parameter input: The input to the layer.
+#     /// - Returns: The output.
+#     @differentiable
+#     func call(_ input: Input) -> Output
+# ```
+#
+# These lines say that all `Layer`s are required to have a method named `call`, that it must be differentiable, and that it can take an return an arbitrary `Input` and `Output` type.
+#
+# ```swift
+#     /// The input type of the layer.
+#     associatedtype Input: Differentiable
+#     /// The output type of the layer.
+#     associatedtype Output: Differentiable
+# ```
+#
+# These two lines say that each `Layer` has to have an `Input` and `Output` type, and that those types must furthermore be differentiable.
+#
+
+# # Taking `Layer` further: `FALayer`
+#
+# Jeremy likes to have nice things, and in particular wants to be able to install delegate callbacks on `Layer`s (to mimic PyTorch's [hooks](https://pytorch.org/tutorials/beginner/former_torchies/nn_tutorial.html#forward-and-backward-function-hooks)).  It might seem that we have to give up on using the builtin layer, but that isn't so!  We can extend the existing `Layer` and give it new functionality with `FALayer`.  First we define the type we want to use for our callbacks:
+
+# We can now define `FALayer`, which is just a layer that has a delegate.  Types that implement `FALayer` will implement a new `forward` method instead of `func call`:
+
+# +
+//export
+
+// FALayer is a layer that supports callbacks through its LayerDelegate.
+public protocol FALayer: Layer {
+    var delegates: [(Output) -> ()] { get set }
+    
+    // FALayer's will implement this instead of `func call`.
+    @differentiable
+    func forward(_ input: Input) -> Output
+    
+    associatedtype Input
+    associatedtype Output
+}
+# -
+
+# It would be a tremendous amount of boilerplate to require all `FALayer`s to implement `func call` and invoke their delegate, so we'll do that once for all `FALayer`s right here in an extension.  We implement it by calling the `forward` function and then invoke the delegate:
+
+//export
+public extension FALayer {
+    @differentiable(vjp: callGrad)
+    func callAsFunction(_ input: Input) -> Output {
+        let activation = forward(input)
+        for d in delegates { d(activation) }
+        return activation
+    }
+       
+    // NOTE: AutoDiff synthesizes a leaking VJP for this, so we define a custom VJP.
+    //    TF-475: https://bugs.swift.org/browse/TF-475
+    // NOTE: If we use `@differentiating`, then there is a linker error. So we use `@differentiable` instead.
+    //    TF-476: https://bugs.swift.org/browse/TF-476
+    func callGrad(_ input: Input) ->
+        (Output, (Self.Output.TangentVector) -> (Self.TangentVector, Self.Input.TangentVector)) {
+        return Swift.valueWithPullback(at: self, input) { (m, i) in m.forward(i) }
+    }
+    
+    //We also add a default init to our `delegates` variable, so that we don't have to define it each time, as
+    //well as a function to easily add a delegate.
+    //var delegates: [(Output) -> ()] { 
+    //    get { return [] }
+    //    set {}
+    //}
+    
+    mutating func addDelegate(_ d: @escaping (Output) -> ()) { delegates.append(d) }
+}
+
+
+# Now we can start implementing some layers, like this dense layer:
+
+# # Dense
+
+# The code is copy-pasted from S4TF for the most general init, and we add a convenience init with kaiming initialization for the weights.
+
+# +
+//export
+
+@frozen
+public struct FADense<Scalar: TensorFlowFloatingPoint>: FALayer {
+    // Note: remove the explicit typealiases after TF-603 is resolved.
+    public typealias Input = Tensor<Scalar>
+    public typealias Output = Tensor<Scalar>
+    public var weight: Tensor<Scalar>
+    public var bias: Tensor<Scalar>
+    public typealias Activation = @differentiable (Tensor<Scalar>) -> Tensor<Scalar>
+    @noDerivative public var delegates: [(Output) -> ()] = []
+    @noDerivative public let activation: Activation
+
+    public init(
+        weight: Tensor<Scalar>,
+        bias: Tensor<Scalar>,
+        activation: @escaping Activation
+    ) {
+        self.weight = weight
+        self.bias = bias
+        self.activation = activation
+    }
+
+    @differentiable
+    public func forward(_ input: Tensor<Scalar>) -> Tensor<Scalar> {
+        return activation(input • weight + bias)
+    }
+}
+
+public extension FADense {
+    init(_ nIn: Int, _ nOut: Int, activation: @escaping Activation = identity) {
+        self.init(weight: Tensor(kaimingNormal: [nIn, nOut], negativeSlope: 1.0),
+                  bias: Tensor(zeros: [nOut]),
+                  activation: activation)
+    }
+}
+# -
+
+# # Conv2D
+
+# +
+//export
+
+@frozen
+public struct FANoBiasConv2D<Scalar: TensorFlowFloatingPoint>: FALayer {
+    // TF-603 workaround.
+    public typealias Input = Tensor<Scalar>
+    public typealias Output = Tensor<Scalar>
+    
+    public var filter: Tensor<Scalar>
+    public typealias Activation = @differentiable (Tensor<Scalar>) -> Tensor<Scalar>
+    @noDerivative public let activation: Activation
+    @noDerivative public let strides: (Int, Int)
+    @noDerivative public let padding: Padding
+    @noDerivative public var delegates: [(Output) -> ()] = []
+
+    public init(
+        filter: Tensor<Scalar>,
+        activation: @escaping Activation,
+        strides: (Int, Int),
+        padding: Padding
+    ) {
+        self.filter = filter
+        self.activation = activation
+        self.strides = strides
+        self.padding = padding
+    }
+
+    @differentiable
+    public func forward(_ input: Tensor<Scalar>) -> Tensor<Scalar> {
+        return activation(conv2D(input, filter: filter,
+                                        strides: (1, strides.0, strides.1, 1),
+                                        padding: padding))
+    }
+}
+
+public extension FANoBiasConv2D {
+    init(
+        filterShape: (Int, Int, Int, Int),
+        strides: (Int, Int) = (1, 1),
+        padding: Padding = .same,
+        activation: @escaping Activation = identity
+    ) {
+        let filterTensorShape = TensorShape([
+            filterShape.0, filterShape.1,
+            filterShape.2, filterShape.3])
+        self.init(
+            filter: Tensor(kaimingNormal: filterTensorShape, negativeSlope: 1.0),
+            activation: activation,
+            strides: strides,
+            padding: padding)
+    }
+}
+
+public extension FANoBiasConv2D {
+    init(_ cIn: Int, _ cOut: Int, ks: Int, stride: Int = 1, padding: Padding = .same,
+         activation: @escaping Activation = identity){
+        self.init(filterShape: (ks, ks, cIn, cOut),
+                  strides: (stride, stride),
+                  padding: padding,
+                  activation: activation)
+    }
+}
+
+# +
+//export
+
+@frozen
+public struct FAConv2D<Scalar: TensorFlowFloatingPoint>: FALayer {
+    // Note: remove the explicit typealiases after TF-603 is resolved.
+    public typealias Input = Tensor<Scalar>
+    public typealias Output = Tensor<Scalar>
+    
+    public var filter: Tensor<Scalar>
+    public var bias: Tensor<Scalar>
+    public typealias Activation = @differentiable (Tensor<Scalar>) -> Tensor<Scalar>
+    @noDerivative public let activation: Activation
+    @noDerivative public let strides: (Int, Int)
+    @noDerivative public let padding: Padding
+    @noDerivative public var delegates: [(Output) -> ()] = []
+
+    public init(
+        filter: Tensor<Scalar>,
+        bias: Tensor<Scalar>,
+        activation: @escaping Activation,
+        strides: (Int, Int),
+        padding: Padding
+    ) {
+        self.filter = filter
+        self.bias = bias
+        self.activation = activation
+        self.strides = strides
+        self.padding = padding
+    }
+
+    @differentiable
+    public func forward(_ input: Tensor<Scalar>) -> Tensor<Scalar> {
+        return activation(conv2D(input, filter: filter,
+                                        strides: (1, strides.0, strides.1, 1),
+                                        padding: padding) + bias)
+    }
+}
+
+public extension FAConv2D {
+    init(
+        filterShape: (Int, Int, Int, Int),
+        strides: (Int, Int) = (1, 1),
+        padding: Padding = .same,
+        activation: @escaping Activation = identity
+    ) {
+        let filterTensorShape = TensorShape([
+            filterShape.0, filterShape.1,
+            filterShape.2, filterShape.3])
+        self.init(
+            filter: Tensor(kaimingNormal: filterTensorShape, negativeSlope: 1.0),
+            bias: Tensor(zeros: TensorShape([filterShape.3])),
+            activation: activation,
+            strides: strides,
+            padding: padding)
+    }
+}
+
+public extension FAConv2D {
+    init(_ cIn: Int, _ cOut: Int, ks: Int, stride: Int = 1, padding: Padding = .same,
+         activation: @escaping Activation = identity){
+        self.init(filterShape: (ks, ks, cIn, cOut),
+                  strides: (stride, stride),
+                  padding: padding,
+                  activation: activation)
+    }
+}
+# -
+
+# # AvgPool2D
+
+# +
+//export
+
+@frozen
+public struct FAAvgPool2D<Scalar: TensorFlowFloatingPoint>: FALayer {
+    // TF-603 workaround.
+    public typealias Input = Tensor<Scalar>
+    public typealias Output = Tensor<Scalar>
+    
+    @noDerivative let poolSize: (Int, Int, Int, Int)
+    @noDerivative let strides: (Int, Int, Int, Int)
+    @noDerivative let padding: Padding
+    @noDerivative public var delegates: [(Output) -> ()] = []
+
+    public init(
+        poolSize: (Int, Int, Int, Int),
+        strides: (Int, Int, Int, Int),
+        padding: Padding
+    ) {
+        self.poolSize = poolSize
+        self.strides = strides
+        self.padding = padding
+    }
+
+    public init(poolSize: (Int, Int), strides: (Int, Int), padding: Padding = .valid) {
+        self.poolSize = (1, poolSize.0, poolSize.1, 1)
+        self.strides = (1, strides.0, strides.1, 1)
+        self.padding = padding
+    }
+    
+    public init(_ sz: Int, padding: Padding = .valid) {
+        poolSize = (1, sz, sz, 1)
+        strides = (1, sz, sz, 1)
+        self.padding = padding
+    }
+
+    @differentiable
+    public func forward(_ input: Tensor<Scalar>) -> Tensor<Scalar> {
+        return avgPool2D(input, filterSize: poolSize, strides: strides, padding: padding)
+    }
+}
+
+# +
+//export
+
+@frozen
+public struct FAGlobalAvgPool2D<Scalar: TensorFlowFloatingPoint>: FALayer {
+    // TF-603 workaround.
+    public typealias Input = Tensor<Scalar>
+    public typealias Output = Tensor<Scalar>
+    @noDerivative public var delegates: [(Output) -> ()] = []
+    
+    public init() {}
+
+    @differentiable
+    public func forward(_ input: Tensor<Scalar>) -> Tensor<Scalar> {
+        return input.mean(squeezingAxes: [1,2])
+    }
+}
+# -
+
+# ### Make Array conform to Differentiable
+
+//export
+extension Array: Layer where Element: Layer, Element.Input == Element.Output {
+    // Note: remove the explicit typealiases after TF-603 is resolved.
+    public typealias Input = Element.Input
+    public typealias Output = Element.Output
+    
+    @differentiable(vjp: _vjpApplied)
+    public func callAsFunction(_ input: Input) -> Output {
+        var activation = input
+        for layer in self {
+            activation = layer(activation)
+        }
+        return activation
+    }
+    
+    public func _vjpApplied(_ input: Input)
+        -> (Output, (Output.TangentVector) -> (Array.TangentVector, Input.TangentVector))
+    {
+        var activation = input
+        var pullbacks: [(Input.TangentVector) -> (Element.TangentVector, Input.TangentVector)] = []
+        for layer in self {
+            let (newActivation, newPullback) = layer.valueWithPullback(at: activation) { $0($1) }
+            activation = newActivation
+            pullbacks.append(newPullback)
+        }
+        func pullback(_ v: Input.TangentVector) -> (Array.TangentVector, Input.TangentVector) {
+            var activationGradient = v
+            var layerGradients: [Element.TangentVector] = []
+            for pullback in pullbacks.reversed() {
+                let (newLayerGradient, newActivationGradient) = pullback(activationGradient)
+                activationGradient = newActivationGradient
+                layerGradients.append(newLayerGradient)
+            }
+            return (Array.TangentVector(layerGradients.reversed()), activationGradient)
+        }
+        return (activation, pullback)
+    }
+}
+
+# ### Simplify some names
+
+# +
+//export 
+extension KeyPathIterable {
+    public var keyPaths: [WritableKeyPath<Self, Tensor<Float>>] {
+        return recursivelyAllWritableKeyPaths(to: Tensor<Float>.self)
+    }
+}
+
+extension Layer {
+    public var variables: AllDifferentiableVariables {
+        get { return allDifferentiableVariables }
+        set { allDifferentiableVariables = newValue }
+    }
+}
+# -
+
+# ## power operator
+
+# +
+// export
+public func ** (lhs: Int, rhs: Int) -> Int {
+    return Int(pow(Double(lhs), Double(rhs)))
+}
+
+public func ** (lhs: Double, rhs: Double) -> Double {
+    return pow(lhs, rhs)
+}
+
+public func **<T : BinaryFloatingPoint>(_ x: T, _ y: T) -> T {
+    return T(pow(Double(x), Double(y)))
+}
+
+public func **<T>(_ x: Tensor<T>, _ y: Tensor<T>) -> Tensor<T>
+  where T : TensorFlowFloatingPoint { return pow(x, y)}
+
+public func **<T>(_ x: T, _ y: Tensor<T>) -> Tensor<T>
+  where T : TensorFlowFloatingPoint { return pow(x, y)}
+
+public func **<T>(_ x: Tensor<T>, _ y: T) -> Tensor<T>
+  where T : TensorFlowFloatingPoint { return pow(x, y)}
+# -
+
+# ### Compose
+
+//export
+public extension Differentiable {
+    @differentiable
+    func compose<L1: Layer, L2: Layer>(_ l1: L1, _ l2: L2) -> L2.Output
+        where L1.Input == Self, L1.Output == L2.Input {
+        return sequenced(through: l1, l2)
+    }
+    
+    @differentiable
+    func compose<L1: Layer, L2: Layer, L3: Layer>(_ l1: L1, _ l2: L2, _ l3: L3) -> L3.Output
+        where L1.Input == Self, L1.Output == L2.Input, L2.Output == L3.Input {
+        return sequenced(through: l1, l2, l3)
+    }
+    
+    @differentiable
+    func compose<L1: Layer, L2: Layer, L3: Layer, L4: Layer>(
+        _ l1: L1, _ l2: L2, _ l3: L3, _ l4: L4
+    ) -> L4.Output
+        where L1.Input == Self, L1.Output == L2.Input, L2.Output == L3.Input,
+              L3.Output == L4.Input {
+        return sequenced(through: l1, l2, l3, l4)
+    }
+    
+    @differentiable
+    func compose<L1: Layer, L2: Layer, L3: Layer, L4: Layer, L5: Layer>(
+        _ l1: L1, _ l2: L2, _ l3: L3, _ l4: L4, _ l5: L5
+    ) -> L5.Output
+        where L1.Input == Self, L1.Output == L2.Input, L2.Output == L3.Input, L3.Output == L4.Input,
+              L4.Output == L5.Input {
+        return sequenced(through: l1, l2, l3, l4, l5)
+    }
+    
+    @differentiable
+    func compose<L1: Layer, L2: Layer, L3: Layer, L4: Layer, L5: Layer, L6: Layer>(
+        _ l1: L1, _ l2: L2, _ l3: L3, _ l4: L4, _ l5: L5, _ l6: L6
+    ) -> L6.Output
+        where L1.Input == Self, L1.Output == L2.Input, L2.Output == L3.Input, L3.Output == L4.Input,
+              L4.Output == L5.Input, L5.Output == L6.Input {
+        return sequenced(through: l1, l2, l3, l4, l5, l6)
+    }
+}
+
+# # Export
+
+import NotebookExport
+let exporter = NotebookExport(Path.cwd/"01a_fastai_layers.ipynb")
+print(exporter.export(usingPrefix: "FastaiNotebook_"))
+
+
diff --git a/nbs/swift/02_fully_connected.ipynb b/nbs/swift/02_fully_connected.ipynb
index fd4ca60..ce0e4bf 100644
--- ./nbs/swift/02_fully_connected.ipynb
+++ ./nbs/swift/02_fully_connected.ipynb
@@ -1570,6 +1570,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/02_fully_connected.py b/nbs/swift/02_fully_connected.py
new file mode 100644
index 0000000..ba60338
--- /dev/null
+++ ./nbs/swift/02_fully_connected.py
@@ -0,0 +1,679 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+# # Fully connected model
+
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/FastaiNotebook_01a_fastai_layers")' FastaiNotebook_01a_fastai_layers
+
+//export
+import Path
+import TensorFlow
+
+import FastaiNotebook_01a_fastai_layers
+
+# ## The forward and backward passes
+
+# Typing `Tensor<Float>` all the time is tedious. The S4TF team expects to make `Float` be the default so we can just say `Tensor`.  Until that happens though, we can define our own alias.
+
+// export
+public typealias TF=Tensor<Float>
+
+# We will need to normalize our data.
+
+// export
+public func normalize(_ x:TF, mean:TF, std:TF) -> TF {
+    return (x-mean)/std
+}
+
+var (xTrain, yTrain, xValid, yValid) = loadMNIST(path: mnistPath, flat: true)
+
+# Normalize the training and validation sets with the training set statistics.
+
+let trainMean = xTrain.mean()
+let trainStd  = xTrain.std()
+print(trainMean, trainStd)
+
+xTrain = normalize(xTrain, mean: trainMean, std: trainStd)
+xValid = normalize(xValid, mean: trainMean, std: trainStd)
+
+# To test everything is going well:
+
+# +
+//export
+public func testNearZero(_ a: TF, tolerance: Float = 1e-3) {
+    assert(abs(a) < tolerance, "Near zero: \(a)")
+}
+
+public func testSame(_ a: TF, _ b: TF) {
+    // Check shapes match so broadcasting doesn't hide shape errors.
+    assert(a.shape == b.shape)
+    testNearZero(a-b)
+}
+# -
+
+testNearZero(xTrain.mean())
+testNearZero(xTrain.std() - 1.0)
+
+let (n,m) = (xTrain.shape[0],xTrain.shape[1])
+let c = yTrain.max()+1
+print(n, m, c)
+
+# ## Foundations version
+
+# ### Basic architecture
+
+//num hidden
+let nh = 50
+
+// simplified kaiming init / he init
+let w1 = TF(randomNormal: [m, nh]) / sqrt(Float(m))
+let b1 = TF(zeros: [nh])
+let w2 = TF(randomNormal: [nh,1]) / sqrt(Float(nh))
+let b2 = TF(zeros: [1])
+
+testNearZero(w1.mean())
+testNearZero(w1.std()-1/sqrt(Float(m)))
+
+// This should be ~ (0,1) (mean,std)...
+print(xValid.mean(), xValid.std())
+
+# In Swift `@` is spelled `•`, which is <kbd>option</kbd>-<kbd>8</kbd> on Mac or <kbd>compose</kbd>-<kbd>.</kbd>-<kbd>=</kbd> elsewhere. Or just use the `matmul()` function we've seen already.
+
+func lin(_ x: TF, _ w: TF, _ b: TF) -> TF { return x•w+b }
+
+let t = lin(xValid, w1, b1)
+
+//...so should this, because we used kaiming init, which is designed to do this
+print(t.mean(), t.std())
+
+func myRelu(_ x:TF) -> TF { return max(x, 0) }
+
+let t = myRelu(lin(xValid, w1, b1))
+
+//...actually it really should be this!
+print(t.mean(),t.std())
+
+// kaiming init / he init for relu
+let w1 = TF(randomNormal: [m,nh]) * sqrt(2.0/Float(m))
+
+print(w1.mean(), w1.std())
+
+let t = myRelu(lin(xValid, w1, b1))
+print(t.mean(), t.std())
+
+# Here is a simple basic model:
+
+func model(_ xb: TF) -> TF {
+    let l1 = lin(xb, w1, b1)
+    let l2 = myRelu(l1)
+    let l3 = lin(l2, w2, b2)
+    return l3
+}
+
+time(repeating: 10) { _ = model(xValid) }
+
+# ### Loss function
+
+# We begin with the mean squared error to have easier gradient computations.
+
+let preds = model(xTrain)
+
+// export
+public func mse(_ out: TF, _ targ: TF) -> TF {
+    return (out.squeezingShape(at: -1) - targ).squared().mean()
+}
+
+# One more step compared to Python, we have to make sure our labels are properly converted to floats.
+
+// Convert these to Float dtype.
+var yTrainF = TF(yTrain)
+var yValidF = TF(yValid)
+
+mse(preds, yTrainF)
+
+# ## Gradients and backward pass
+
+# Here we should how to calculate gradients for a simple model the hard way, manually.
+#
+# To store the gradients a bit like in PyTorch we introduce a `TFGrad` class that has two attributes: the original tensor and the gradient. We choose a class to easily replicate the Python notebook: classes are reference types (which means they are mutable) while structures are value types.
+#
+# In fact, since this is the first time we're discovering Swift classes, let's jump into a [sidebar discussion about Value Semantics vs Reference Semantics](https://docs.google.com/presentation/d/1dc6o2o-uYGnJeCeyvgsgyk05dBMneArxdICW5vF75oU/edit#slide=id.g5669969ead_0_145) since it is a pretty fundamental part of the programming model and a huge step forward that Swift takes.
+#
+# When we get back, we'll keep charging on, even though this is very non-idiomatic Swift code!
+
+/// WARNING: This is designed to be similar to the PyTorch 02_fully_connected lesson,
+/// this isn't idiomatic Swift code.
+class TFGrad {
+    var inner, grad:  TF
+    
+    init(_ x: TF) {
+        inner = x
+        grad = TF(zeros: x.shape)
+    } 
+}
+
+// Redefine our functions on TFGrad.
+func lin(_ x: TFGrad, _ w: TFGrad, _ b: TFGrad) -> TFGrad {
+    return TFGrad(x.inner • w.inner + b.inner)
+}
+func myRelu(_ x: TFGrad) -> TFGrad {
+    return TFGrad(max(x.inner, 0))
+}
+func mse(_ inp: TFGrad, _ targ: TF) -> TF {
+    //grad of loss with respect to output of previous layer
+    return (inp.inner.squeezingShape(at: -1) - targ).squared().mean()
+}
+
+# +
+// Define our gradient functions.
+func mseGrad(_ inp: TFGrad, _ targ: TF) {
+    //grad of loss with respect to output of previous layer
+    inp.grad = 2.0 * (inp.inner.squeezingShape(at: -1) - targ).expandingShape(at: -1) / Float(inp.inner.shape[0])
+}
+
+func reluGrad(_ inp: TFGrad, _ out: TFGrad) {
+    //grad of relu with respect to input activations
+    inp.grad = out.grad.replacing(with: TF(zeros: inp.inner.shape), where: (inp.inner .< 0))
+}
+# -
+
+# This is our python version (we've renamed the python `g` to `grad` for consistency):
+#
+# ```python
+# def lin_grad(inp, out, w, b):
+#     inp.grad = out.grad @ w.t()
+#     w.grad = (inp.unsqueeze(-1) * out.grad.unsqueeze(1)).sum(0)
+#     b.grad = out.grad.sum(0)
+# ```
+
+func linGrad(_ inp:TFGrad, _ out:TFGrad, _ w:TFGrad, _ b:TFGrad){
+    // grad of linear layer with respect to input activations, weights and bias
+    inp.grad = out.grad • w.inner.transposed()
+    w.grad = inp.inner.transposed() • out.grad
+    b.grad = out.grad.sum(squeezingAxes: 0)
+}
+
+let w1a = TFGrad(w1)
+let b1a = TFGrad(b1)
+let w2a = TFGrad(w2)
+let b2a = TFGrad(b2)
+
+func forwardAndBackward(_ inp:TFGrad, _ targ:TF){
+    // forward pass:
+    let l1 = lin(inp, w1a, b1a)
+    let l2 = myRelu(l1)
+    let out = lin(l2, w2a, b2a)
+    //we don't actually need the loss in backward!
+    let loss = mse(out, targ)
+    
+    // backward pass:
+    mseGrad(out, targ)
+    linGrad(l2, out, w2a, b2a)
+    reluGrad(l1, l2)
+    linGrad(inp, l1, w1a, b1a)
+}
+
+let inp = TFGrad(xTrain)
+
+forwardAndBackward(inp, yTrainF)
+
+# # Automatic Differentiation in Swift
+#
+# There are a few challenges with the code above:
+#
+#  * It doesn't follow the principle of value semantics, because TensorGrad is a class.  Mutating a tensor would produce the incorrect results.
+#  * It doesn't compose very well - we need to keep track of values passed in the forward pass and also pass them in the backward pass.
+#  * It is fully dynamic, keeping track of gradients at runtime. This interferes with the compiler's ability to perform fusion and other advanced optimizations.
+#  
+# We want something that is simple, consistent and easy to use, like this:
+#  
+
+# +
+let gradF = gradient { (x : Double) in x*x }
+
+for x in stride(from: 0.0, to: 1, by: 0.1) {
+  print(gradF(x))    
+}
+# -
+
+# Note how we're working with simple doubles here, not having to use tensors.  Other than that, you can use it basically the way PyTorch autodiff works.
+#
+# You can get the gradients of functions, and do everything else you'd expect:
+
+# +
+func doThing(_ x: Float) -> Float {
+    return sin(x*x) + cos(x*x)
+}
+
+print(gradient(at: 3.14, in: doThing))
+# -
+
+# ## Autodiff the Functional Way
+#
+# Swift for TensorFlow's autodiff is built on value semantics and functional programming ideas.
+#
+# Each differentiable function gets an associated "chainer" (described below) that defines its gradient.  When you write a function that, like `model`, calls a bunch of these in sequence, the compiler calls the function and it collects its pullback, then stitches together the pullbacks using the chain rule from Calculus.
+#
+# Let's remember the chain rule - it is written:
+#
+# $$\frac{d}{dx}\left[f\left(g(x)\right)\right] = f'\left(g(x)\right)g'(x)$$
+#
+# Notice how the chain rule requires mixing together expressions from both the forward pass (`g()`) and the backward pass (`f'()` and `g'()`) of a computation to get the derivative.  While it is possible to calculate all the forward versions of a computation, then recompute everything needed again on the backward pass, this would be incredibly inefficient - it makes more sense to save intermediate values from the forward pass and reuse them on the backward pass.
+#
+# The Swift language provides the atoms we need to express this: we can represent math with function calls, and the pullback can be represented with a closure.  This works out well because closures provide a natural way to capture interesting values from the forward pass.
+
+# ## Basic expressions in MSE
+
+# To explore this, let's look at a really simple example of this, the inner computation of MSE.  The full body of MSE looks like this:
+#
+# ```swift
+# func mse(_ inp: TF, _ targ: TF) -> TF {
+#     //grad of loss with respect to output of previous layer
+#     return (inp.squeezingShape(at: -1) - targ).squared().mean()
+# }
+# ```
+#
+# For the purposes of our example, we're going to keep it super super simple and just focus on the `x.squared().mean()` part of the computation, which we'll write as `mseInner(x) = mean(square(x))` to align better with function composition notation.  We want a way to visualize what functions get called, so let's define a little helper that prints the name of its caller whenever it is called. To do this we use a [litteral expression](https://docs.swift.org/swift-book/ReferenceManual/Expressions.html#ID390) `#function` that contains the name of the function we are in.
+
+# +
+// This function prints out the calling function's name.  This 
+// is useful to see what is going on in your program..
+func trace(function: String = #function) {
+  print(function)
+}
+
+// Try out the trace helper function.
+func foo(a: Int, b: Int) -> Int {
+  trace()
+  return a+b
+}
+func bar(x: Int) -> Int {
+  trace()
+  return x*42+17
+}
+
+foo(a: 1, b: 2)
+bar(x: 17)
+
+# -
+
+#
+# Ok, given that, we start by writing the implementation and gradients of these functions, and we put print statements in them so we can tell when they are called.  This looks like:
+#
+
+# +
+func square(_ x: TF) -> TF {
+    trace() 
+    return x * x
+}
+func 𝛁square(_ x: TF) -> TF {
+    trace()
+    return 2 * x
+}
+
+func mean(_ x: TF) -> TF {
+    trace()
+    return x.mean()  // this is a reduction.  (can someone write this out longhand?)
+}
+func 𝛁mean(_ x: TF) -> TF {
+    trace()
+    return TF(ones: x.shape) / Float(x.shape[0])
+}
+# -
+
+# Given these definitions we can now compute the forward and derivative of the `mseInner` function that composes `square` and `mean`, using the chain rule:
+#
+# $$\frac{d}{dx}\left[f\left(g(x)\right)\right] = f'\left(g(x)\right)g'(x)$$
+#
+# where `f` is `mean` and `g` is `square`.  This gives us:
+#
+
+# +
+func mseInner(_ x: TF) -> TF {
+    return mean(square(x))
+}
+
+func 𝛁mseInner(_ x: TF) -> TF {
+    return 𝛁mean(square(x)) * 𝛁square(x)
+}
+# -
+
+# This is all simple, but we have a small problem if (in the common case for deep nets) we want to calculate both the forward and the gradient computation at the same time: we end up redundantly computing `square(x)` in both the forward and backward paths!
+
+# +
+func mseInnerAndGrad(_ x: TF) -> (TF, TF) {
+  return (mseInner(x), 𝛁mseInner(x))    
+}
+
+let exampleData = TF([1, 2, 3, 4])
+
+let (mseInnerResult1, mseInnerGrad1) = mseInnerAndGrad(exampleData)
+print()
+
+print("result:", mseInnerResult1)
+print("gradient:", mseInnerGrad1)
+
+// Check that our gradient matches builtin S4TF's autodiff.
+let builtinGrad = gradient(at: exampleData) { x in (x*x).mean() }
+testSame(mseInnerGrad1, builtinGrad)
+# -
+
+# Note above how `square` got called two times: once in the forward function and once in the gradient.  In more complicated cases, this can be an incredible amount of redundant computation, which would make performance unacceptably slow.
+#
+# **Exercise:** take a look what happens when you use the same techniques to implement more complex functions.
+#
+
+# ## Reducing recomputation with Chainers and the ValueAndChainer pattern
+#
+# We can fix this by refactoring our code.  We want to preserve the linear structure of `mseInner` that calls `square` and then `mean`, but we want to make it so the ultimate *user* of the computation can choose whether they want the gradient computation (or not) and if so, we want to minimize computation.  To do this, we have to slightly generalize our derivative functions.  While it is true that the derivative of $square(x)$ is `2*x`, this is only true for a given point `x`.
+#
+# If we generalize the derivative of `square` to work with an arbitrary **function**, instead of point, then we need to remember that $\frac{d}{dx}x^2 = 2x\frac{d}{dx}$, and therefore the derivative for `square` needs to get $\frac{d}{dx}$ passed in from its nested function.  
+#
+# This form of gradient is known by the academic term "Vector Jacobian Product" (vjp) or the technical term "pullback", but we will refer to it as a 𝛁Chain because it implements the gradient chain rule for the operation.  We can write it like this:
+
+# +
+// The chainer for the gradient of square(x).
+func square𝛁Chain(x: TF, ddx: TF) -> TF {
+  trace()
+  return ddx * 2*x
+}
+
+// The chainer for the gradient of mean(x).
+func mean𝛁Chain(x: TF, ddx: TF) -> TF {
+  trace()
+  return ddx * TF(ones: x.shape) / Float(x.shape[0])
+}
+# -
+
+# Given this very general way of describing gradients, we now want to pull them together in a single bundle that we can keep track of: we do this by changing each atom of computation to return both a normal value with the 𝛁Chain closure that produces a piece of the gradient given the chained input.
+#
+# We refer to this as a "Value With 𝛁Chain" function (since that is what it is) and abreviate this mouthful to "VWC".  This is also an excuse to use labels in tuples, which are a Swift feature that is very useful for return values like this.
+#
+# They look like this:
+
+# +
+// Returns x*x and the chain for the gradient of x*x.
+func squareVWC(_ x: TF) -> (value: TF, 
+                            chain: (TF) -> TF) {
+  trace()
+  return (value: x*x,
+          chain: { ddx in square𝛁Chain(x: x, ddx: ddx) })    
+}
+
+// Returns the mean of x and the chain for the mean.
+func meanVWC(_ x: TF) -> (value: TF,
+                          chain: (TF) -> TF) {
+  trace()
+  return (value: x.mean(),
+          chain: { ddx in mean𝛁Chain(x: x, ddx: ddx) })
+}
+# -
+
+# Given this, we can now implement `mseInner` in the same way.  Notice how our use of named tuple results make the code nice and tidy:
+
+// We implement mean(square(x)) by calling each of the VWCs in turn.
+func mseInnerVWC(_ x: TF) -> (value: TF, 
+                              chain: (TF) -> TF) {
+
+  // square and mean are tuples that carry the value/chain for each step.
+  let square = squareVWC(x)
+  let mean   = meanVWC(square.value)
+
+  // The result is the combination of the results and the pullbacks.
+  return (mean.value,
+          // The mseInner pullback calls the functions in reverse order.
+          { v in square.chain(mean.chain(v)) })
+}
+
+# Now we can choose to evaluate just the forward computation, or we can choose to run both:
+
+# +
+print("Calling the forward function:")
+let mseInner2 = mseInnerVWC(exampleData)
+print()
+
+testSame(mseInner2.value, mseInnerResult1)
+
+
+print("Calling the backward function:")
+let mseInnerGrad2 = mseInner2.chain(TF(1))
+print()
+
+print(mseInnerGrad2)
+// Check that we get the same result.
+testSame(mseInnerGrad2, builtinGrad)
+
+# -
+
+# Ok, great - we only ran each piece of the computation once, and we gained a single conceptual abstraction that bundles everything we need together.
+#
+# Now we have all of the infrastructure and scaffolding necessary to define and compose computations and figure out their backwards versions from the chain rule.  Let's jump up a level to define Jeremy's example using the VWC form of the computation.
+
+# # Implementing Relu, MSE, and Lin with the Value With 𝛁Chain pattern
+#
+# Lets come back to our earlier examples and define pullbacks for our primary functions in the simple model function example.
+
+func reluVWC(_ x: TF) -> (value: TF, chain: (TF) -> TF) {
+    return (value: max(x, 0),
+            // Pullback for max(x, 0)
+            chain: { 𝛁out -> TF in
+              𝛁out.replacing(with: TF(zeros: x.shape), where: x .< 0)
+            })
+}
+
+# ```swift
+# func lin(_ x: TFGrad, _ w: TFGrad, _ b: TFGrad) -> TFGrad {
+#     return TFGrad(x.inner • w.inner + b.inner)
+# }
+# func linGrad(_ inp:TFGrad, _ out:TFGrad, _ w:TFGrad, _ b:TFGrad){
+#     inp.grad = out.grad • w.inner.transposed()
+#     w.grad = inp.inner.transposed() • out.grad
+#     b.grad = out.grad.sum(squeezingAxes: 0)
+# }
+# ```
+
+func linVWC(_ inp: TF, _ w: TF, _ b: TF) -> (value: TF,
+                                             chain: (TF) -> (TF, TF, TF)) {
+    return (value: inp • w + b,
+            // Pullback for inp • w + b.  Three results because 'lin' has three args.
+            chain: { 𝛁out in
+              (𝛁out • w.transposed(), 
+               inp.transposed() • 𝛁out,
+               𝛁out.unbroadcasted(to: b.shape))
+    })
+}
+
+func mseVWC(_ inp: TF, _ targ: TF) -> (value: TF,
+                                       chain: (TF) -> (TF)) {
+    let tmp = inp.squeezingShape(at: -1) - targ
+    
+    // We already wrote a VWC for x.square().mean(), so we can reuse it.
+    let mseInner = mseInnerVWC(tmp)
+    
+    // Return the result, and a pullback that expands back out to
+    // the input shape.
+    return (mseInner.value, 
+            { v in mseInner.chain(v).expandingShape(at: -1) })
+}
+
+# And then our forward and backward can be refactored in:
+
+func forwardAndBackward(_ inp: TF, _ targ: TF) -> (TF, TF, TF, TF, TF) {
+    // Forward pass:
+    let l1   = linVWC(inp, w1, b1)
+    let l2   = reluVWC(l1.value)
+    let out  = linVWC(l2.value, w2, b2)
+    //we don't actually need the loss in backward, but we need the pullback.
+    let loss = mseVWC(out.value, targ)
+    
+    // Backward pass:
+    let 𝛁loss = TF(1) // We don't really need it but the gradient of the loss with respect to itself is 1
+    let 𝛁out = loss.chain(𝛁loss)
+    let (𝛁l2, 𝛁w2, 𝛁b2) = out.chain(𝛁out)
+    let 𝛁l1 = l2.chain(𝛁l2)
+    let (𝛁inp, 𝛁w1, 𝛁b1) = l1.chain(𝛁l1)
+    return (𝛁inp, 𝛁w1, 𝛁b1, 𝛁w2, 𝛁b2)
+}
+
+let (𝛁xTrain, 𝛁w1, 𝛁b1, 𝛁w2, 𝛁b2) = forwardAndBackward(xTrain, yTrainF)
+// Check this is still all correct
+testSame(inp.grad, 𝛁xTrain)
+testSame(w1a.grad, 𝛁w1)
+testSame(b1a.grad, 𝛁b1)
+testSame(w2a.grad, 𝛁w2)
+testSame(b2a.grad, 𝛁b2)
+
+
+# Ok, this is pretty nice - we get composition, we get value semantics, and everything just stacks up nicely.  We have a problem them, which is that this is a real pain to write and it is very easy to make simple mistakes.  This is also very mechanical - and thus boring.
+#
+# This is where Swift's autodiff system comes to the rescue!
+#
+# # Automatically generating 𝛁Chains and VWCs
+#
+# When you define a function with `@differentiable` you're saying that it must be differentiable by the compiler by composing the VWCs of other functions just like we did above manually.  as it turns out, all of the methods on `Tensor` are marked up with `@differentiable` attributes until you get down to the atoms of the raw ops.  For example, this is how the `Tensor.squared` method is [defined in Ops.swift in the TensorFlow module](https://github.com/apple/swift/blob/tensorflow/stdlib/public/TensorFlow/Ops.swift#L960):
+#
+# ```swift
+# // slightly simplified for clarity
+# public extension Tensor {
+#   @differentiable(vjp: _vjpSquared())   // VWCs are called "VJPs" by S4TF 
+#   func squared() -> Tensor {
+#     return Raw.square(self)
+#   }
+# }
+# ```
+#    
+# The Value with 𝛁Chain function is [defined in Gradients.swift](https://github.com/apple/swift/blob/tensorflow/stdlib/public/TensorFlow/Gradients.swift#L470):
+#
+# ```swift
+# public extension Tensor {
+#   func _vjpSquared() -> (Tensor, (Tensor) -> Tensor) {
+#     return (squared(), { 2 * self * $0 })
+#   }
+# }
+# ```
+#
+# This tells the compiler that `squared()` has a manually written VJP that is implemented as we already saw.  Now, anything that calls `squared()` can have its own VJP synthesized out of it.  For example we can write our `mseInner` function the trivial way, and we can get low level access to the 𝛁Chain (which S4TF calls a "pullback") if we want:
+#
+
+# +
+@differentiable
+func mseInnerForAD(_ x: TF) -> TF {
+    return x.squared().mean()
+}
+
+let mseInner𝛁Chain = pullback(at: exampleData, in: mseInnerForAD)
+print(type(of: mseInner𝛁Chain))
+# -
+
+# because the compiler knows the VWCs for the `squared` and `mean` function, it can synthesize them as we need them. Most often though, you don't use the 𝛁Chain function directly.  You can instead ask for both the value and the gradient of a function at a specific point, which is the most typical thing you'd use:
+
+# +
+let (value, grad) = valueWithGradient(at: exampleData, in: mseInnerForAD)
+
+print("value: \(value), grad:  \(grad)")
+# -
+
+# We can also ask for just the gradient.  Of course, we can also use trailing closures, which work very nicely with these functions.
+
+gradient(at: exampleData) { ($0*$0).mean() }
+
+# The @differentiable attribute is normally optional in a S4TF standalone environment, but is currently required in Jupyter notebooks. The S4TF team is planning to relax this limitation when time permits.
+
+# # Bundling up a model into an aggregate value
+#
+# When we work with models and individual layers, we often want to bundle up a bunch of differentiable variables into one value, so we don't have to pass a ton of arguments around.  When we get to building our whole model, it is mathematically just a struct that contains a bunch of differentiable values embedded into it.  It is more convenient to think of a model as a function that takes one value and returns one value rather than something that can take an unbounded number of inputs: our simple model has 4 parameters, and two normal inputs!
+#
+
+@differentiable
+func forward(_ inp: TF, _ targ: TF, w1: TF, b1: TF, w2: TF, b2: TF) -> TF {
+    // FIXME: use lin
+    let l1 = matmul(inp, w1) + b1
+    let l2 = relu(l1)
+    let l3 = matmul(l2, w2) + b2
+    return (l3.squeezingShape(at: -1) - targ).squared().mean()
+}
+
+# Let's try refactoring our single linear model to use a `struct` to simplify this.  We start by defining a structure to contain all the fields we need.  We mark the structure as `: Differentiable` so the compiler knows we want it to be differentiable (not discrete):
+
+# +
+struct MyModel: Differentiable {
+    public var w1, b1, w2, b2: TF
+}
+
+// Create an instance of our model with all the individual parameters we initialized.
+let model = MyModel(w1: w1, b1: b1, w2: w2, b2: b2)
+# -
+
+# We can now define our forward function as a method on this model:
+
+extension MyModel {
+    @differentiable
+    func forward(_ input: TF, _ target: TF) -> TF {
+        // FIXME: use lin
+        let l1 = matmul(input, w1) + b1
+        let l2 = relu(l1)
+        let l3 = matmul(l2, w2) + b2
+        // use mse
+        return (l3.squeezingShape(at: -1) - target).squared().mean()
+    }
+}
+
+# Given this, we can now get the gradient of our entire loss w.r.t to the input and the expected labels:
+
+# +
+// Grads is a struct with one gradient per parameter.
+let grads = gradient(at: model) { model in model.forward(xTrain, yTrainF) }
+
+// Check that this still calculates the same thing.
+testSame(𝛁w1, grads.w1)
+testSame(𝛁b1, grads.b1)
+testSame(𝛁w2, grads.w2)
+testSame(𝛁b2, grads.b2)
+# -
+
+# In terms of timing our implementation gives:
+
+time(repeating: 10) { _ = forwardAndBackward(xTrain, yTrainF) }
+
+time(repeating: 10) {
+    _ = valueWithGradient(at: model) { 
+        model in model.forward(xTrain, yTrainF)
+    }
+}
+
+# # More about AutoDiff
+#
+# There are lots of cool things you can do with Swift autodiff.  One of the great things about understanding how the system fits together is that you do a lot of interesting things by customizing gradients with S4TF.  This can be useful for lots of reasons, for example:
+#
+#  * you want a faster approximation of an expensive gradient
+#  * you want to improve the numerical instability of a gradient
+#  * you want to pursue exotic techniques like learned gradients
+#  * you want to work around a limitation of the current implementation
+#  
+# In fact, we've had to do that in `11_imagenette` where we've built a `SwitchableLayer` with a custom gradient.  Let's go take a look.
+#
+# To find out more, check out this nice tutorial in Colab on custom autodiff](https://colab.research.google.com/github/tensorflow/swift/blob/master/docs/site/tutorials/custom_differentiation.ipynb)
+#
+#
+
+# ### Export
+
+import NotebookExport
+let exporter = NotebookExport(Path.cwd/"02_fully_connected.ipynb")
+print(exporter.export(usingPrefix: "FastaiNotebook_"))
+
+
diff --git a/nbs/swift/02a_why_sqrt5.ipynb b/nbs/swift/02a_why_sqrt5.ipynb
index 810b543..c187b66 100644
--- ./nbs/swift/02a_why_sqrt5.ipynb
+++ ./nbs/swift/02a_why_sqrt5.ipynb
@@ -744,6 +744,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/02a_why_sqrt5.py b/nbs/swift/02a_why_sqrt5.py
new file mode 100644
index 0000000..ab3a3c2
--- /dev/null
+++ ./nbs/swift/02a_why_sqrt5.py
@@ -0,0 +1,199 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/FastaiNotebook_02_fully_connected")' FastaiNotebook_02_fully_connected
+
+//export
+import Foundation
+import TensorFlow
+import Path
+
+import FastaiNotebook_02_fully_connected
+
+# ## Does nn.Conv2d init work well?
+
+var (xTrain, yTrain, xValid, yValid) = loadMNIST(path: Path.home/".fastai"/"data"/"mnist_tst")
+let (trainMean, trainStd) = (xTrain.mean(), xTrain.standardDeviation())
+xTrain = normalize(xTrain, mean: trainMean, std: trainStd)
+xValid = normalize(xValid, mean: trainMean, std: trainStd)
+
+xTrain = xTrain.reshaped(to: [xTrain.shape[0], 28, 28, 1])
+xValid = xValid.reshaped(to: [xValid.shape[0], 28, 28, 1])
+print(xTrain.shape, xValid.shape)
+
+let images = xTrain.shape[0]
+let classes = xValid.max() + 1
+let channels = 32
+
+var layer1 = FAConv2D<Float>(filterShape: (5, 5, 1, channels)) //Conv2D(1, nh, 5)
+
+let x = xValid[0..<100]
+
+x.shape
+
+extension Tensor where Scalar: TensorFlowFloatingPoint {
+    func stats() -> (mean: Tensor, std: Tensor) {
+        return (mean: mean(), std: standardDeviation())
+    }
+}
+
+(filter: layer1.filter.stats(), bias: layer1.bias.stats())
+
+withDevice(.cpu){
+    let result = layer1(x)
+}
+
+let result = layer1(x)
+
+result.stats()
+
+# This is in 1a now so this code is disabled from here:
+#
+# ```swift
+# var rng = PhiloxRandomNumberGenerator.global
+#
+# extension Tensor where Scalar: TensorFlowFloatingPoint {
+#     init(kaimingNormal shape: TensorShape, negativeSlope: Double = 1.0) {
+#         // Assumes Leaky ReLU nonlinearity
+#         let gain = Scalar(sqrt(2.0 / (1.0 + pow(negativeSlope, 2))))
+#         let spatialDimCount = shape.count - 2
+#         let receptiveField = shape[0..<spatialDimCount].contiguousSize
+#         let fanIn = shape[shape.count - 2] * receptiveField
+#         self.init(randomNormal: shape,
+#                   stddev: gain / sqrt(Scalar(fanIn)),
+#                   generator: &rng
+#         )
+#     }
+# }
+# ```
+
+layer1.filter = Tensor(kaimingNormal: layer1.filter.shape, negativeSlope: 1.0)
+layer1(x).stats()
+
+// export
+func leakyRelu<T: TensorFlowFloatingPoint>(
+    _ x: Tensor<T>,
+    negativeSlope: Double = 0.0
+) -> Tensor<T> {
+    return max(0, x) + T(negativeSlope) * min(0, x)
+}
+
+layer1.filter = Tensor(kaimingNormal: layer1.filter.shape, negativeSlope: 0.0)
+leakyRelu(layer1(x)).stats()
+
+var layer1 = FAConv2D<Float>(filterShape: (5, 5, 1, channels)) //Conv2D(1, nh, 5)
+leakyRelu(layer1(x)).stats()
+
+layer1.filter.shape
+
+let spatialDimCount = layer1.filter.rank - 2
+let receptiveField = layer1.filter.shape[0..<spatialDimCount].contiguousSize
+receptiveField
+
+let filtersIn = layer1.filter.shape[2]
+let filtersOut = layer1.filter.shape[3]
+print(filtersIn, filtersOut)
+
+let fanIn = filtersIn * receptiveField
+let fanOut = filtersOut * receptiveField
+print(fanIn, fanOut)
+
+func gain(_ negativeSlope: Double) -> Double {
+    return sqrt(2.0 / (1.0 + pow(negativeSlope, 2.0)))
+}
+
+(gain(1.0), gain(0.0), gain(0.01), gain(0.1), gain(sqrt(5.0)))
+
+(2 * Tensor<Float>(randomUniform: [10000]) - 1).standardDeviation()
+
+1.0 / sqrt(3.0)
+
+//export
+extension Tensor where Scalar: TensorFlowFloatingPoint {
+    init(kaimingUniform shape: TensorShape, negativeSlope: Double = 1.0) {
+        // Assumes Leaky ReLU nonlinearity
+        let gain = Scalar.init(TensorFlow.sqrt(2.0 / (1.0 + TensorFlow.pow(negativeSlope, 2))))
+        let spatialDimCount = shape.count - 2
+        let receptiveField = shape[0..<spatialDimCount].contiguousSize
+        let fanIn = shape[shape.count - 2] * receptiveField
+        let bound = TensorFlow.sqrt(Scalar(3.0)) * gain / TensorFlow.sqrt(Scalar(fanIn))
+        self = bound * (2 * Tensor(randomUniform: shape, generator: &PhiloxRandomNumberGenerator.global) - 1)
+    }
+}
+
+layer1.filter = Tensor(kaimingUniform: layer1.filter.shape, negativeSlope: 0.0)
+leakyRelu(layer1(x)).stats()
+
+layer1.filter = Tensor(kaimingUniform: layer1.filter.shape, negativeSlope: sqrt(5.0))
+leakyRelu(layer1(x)).stats()
+
+public struct Model: Layer {
+    public var conv1 = FAConv2D<Float>(
+        filterShape: (5, 5, 1, 8),   strides: (2, 2), padding: .same, activation: relu
+    )
+    public var conv2 = FAConv2D<Float>(
+        filterShape: (3, 3, 8, 16),  strides: (2, 2), padding: .same, activation: relu
+    )
+    public var conv3 = FAConv2D<Float>(
+        filterShape: (3, 3, 16, 32), strides: (2, 2), padding: .same, activation: relu
+    )
+    public var conv4 = FAConv2D<Float>(
+        filterShape: (3, 3, 32, 1),  strides: (2, 2), padding: .valid
+    )
+    public var flatten = Flatten<Float>()
+
+    @differentiable
+    public func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {
+        return input.sequenced(through: conv1, conv2, conv3, conv4, flatten)
+    }
+}
+
+let y = Tensor<Float>(yValid[0..<100])
+var model = Model()
+
+let prediction = model(x)
+prediction.stats()
+
+# +
+let gradients = gradient(at: model) { model in
+    meanSquaredError(predicted: model(x), expected: y)
+}
+
+gradients.conv1.filter.stats()
+# -
+
+for keyPath in [\Model.conv1, \Model.conv2, \Model.conv3, \Model.conv4] {
+    model[keyPath: keyPath].filter = Tensor(kaimingUniform: model[keyPath: keyPath].filter.shape)
+}
+
+let prediction = model(x)
+prediction.stats()
+
+# +
+let gradients = gradient(at: model) { model in
+    meanSquaredError(predicted: model(x), expected: y)
+}
+
+gradients.conv1.filter.stats()
+# -
+
+# ## Export
+
+import NotebookExport
+let exporter = NotebookExport(Path.cwd/"02a_why_sqrt5.ipynb")
+print(exporter.export(usingPrefix: "FastaiNotebook_"))
+
+
diff --git a/nbs/swift/02b_initializing.ipynb b/nbs/swift/02b_initializing.ipynb
index 6b8ff3f..a5a4afa 100644
--- ./nbs/swift/02b_initializing.ipynb
+++ ./nbs/swift/02b_initializing.ipynb
@@ -568,6 +568,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/02b_initializing.py b/nbs/swift/02b_initializing.py
new file mode 100644
index 0000000..4d72591
--- /dev/null
+++ ./nbs/swift/02b_initializing.py
@@ -0,0 +1,185 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/FastaiNotebook_02a_why_sqrt5")' FastaiNotebook_02a_why_sqrt5
+
+//export
+import Path
+import TensorFlow
+
+import FastaiNotebook_02a_why_sqrt5
+
+# ### Why you need a good init
+
+# To understand why initialization is important in a neural net, we'll focus on the basic operation you have there: matrix multiplications. So let's just take a vector `x`, and a matrix `a` initialized randomly, then multiply them 100 times (as if we had 100 layers). 
+
+var x = TF(randomNormal: [512, 1])
+let a = TF(randomNormal: [512,512])
+
+for i in 0..<100 { x = a • x }
+
+(x.mean(),x.std())
+
+# The problem you'll get with that is activation explosion: very soon, your activations will go to nan. We can even ask the loop to break when that first happens:
+
+var x = TF(randomNormal: [512, 1])
+let a = TF(randomNormal: [512,512])
+
+for i in 0..<100 {
+    x = a • x
+    if x.std().scalarized().isNaN {
+        print(i)
+        break
+    }
+}
+
+# It only takes around 30 multiplications! On the other hand, if you initialize your activations with a scale that is too low, then you'll get another problem:
+
+var x = TF(randomNormal: [512, 1])
+let a = TF(randomNormal: [512,512]) * 0.01
+
+for i in 0..<100 { x = a • x }
+
+(x.mean(),x.std())
+
+# Here, every activation vanished to 0. So to avoid that problem, people have come with several strategies to initialize their weight matrices, such as:
+# - use a standard deviation that will make sure x and Ax have exactly the same scale
+# - use an orthogonal matrix to initialize the weight (orthogonal matrices have the special property that they preserve the L2 norm, so x and Ax would have the same sum of squares in that case)
+# - use [spectral normalization](https://arxiv.org/pdf/1802.05957.pdf) on the matrix A  (the spectral norm of A is the least possible number M such that `matmul(A,x).norm() <= M*x.norm()` so dividing A by this M insures you don't overflow. You can still vanish with this)
+
+# ### The magic number for scaling
+
+# Here we will focus on the first one, which is the Xavier initialization. It tells us that we should use a scale equal to `1/sqrt(n_in)` where `n_in` is the number of inputs of our matrix.
+
+var x = TF(randomNormal: [512, 1])
+let a = TF(randomNormal: [512,512]) / sqrt(512)
+
+for i in 0..<100 { x = a • x }
+
+(mean: x.mean(), std: x.std())
+
+# And indeed it works. Note that this magic number isn't very far from the 0.01 we had earlier.
+
+1 / sqrt(512)
+
+# But where does it come from? It's not that mysterious if you remember the definition of the matrix multiplication. When we do `y = matmul(a, x)`, the coefficients of `y` are defined by
+#
+# $$y_{i} = a_{i,0} x_{0} + a_{i,1} x_{1} + \cdots + a_{i,n-1} x_{n-1} = \sum_{k=0}^{n-1} a_{i,k} x_{k}$$
+#
+# or in code:
+# ```
+# for i in 0..<a.shape[0] {
+#     for k in 0..<b.shape[1] {
+#         y[i][0] += a[i][k] * x[k][0]
+#     }
+# }
+# ```
+#
+# Now at the very beginning, our `x` vector has a mean of roughly 0. and a standard deviation of roughly 1. (since we picked it that way).
+
+var x = TF(randomNormal: [512, 1])
+(mean: x.mean(), std: x.std())
+
+# NB: This is why it's extremely important to normalize your inputs in Deep Learning, the initialization rules have been designed with inputs that have a mean 0. and a standard deviation of 1.
+#
+# If you need a refresher from your statistics course, the mean is the sum of all the elements divided by the number of elements (a basic average). The standard deviation shows whether the data points stay close to the mean or are far away from it. It's computed by the following formula:
+#
+# $$\sigma = \sqrt{\frac{1}{n}\left[(x_{0}-m)^{2} + (x_{1}-m)^{2} + \cdots + (x_{n-1}-m)^{2}\right]}$$
+#
+# where m is the mean and $\sigma$ (the greek letter sigma) is the standard deviation. To avoid that square root, we also often consider a quantity called the variance, which is $\sigma$ squared. 
+#
+# Here we have a mean of 0, so the variance is just the mean of x squared, and the standard deviation is its square root.
+#
+# If we go back to `y = a @ x` and assume that we chose weights for `a` that also have a mean of 0, we can compute the variance of `y` quite easily. Since it's random, and we may fall on bad numbers, we repeat the operation 100 times.
+
+var mean = Float()
+var sqr = Float()
+for i in 0..<100 {
+    let x = TF(randomNormal: [512, 1])
+    let a = TF(randomNormal: [512, 512])
+    let y = a • x
+    mean += y.mean().scalarized()
+    sqr  += pow(y, 2).mean().scalarized()
+}
+(mean/100, sqr/100)
+
+# Now that looks very close to the dimension of our matrix 512. And that's no coincidence! When you compute y, you sum 512 product of one element of a by one element of x. So what's the mean and the standard deviation of such a product of one element of `a` by one element of `x`? We can show mathematically that as long as the elements in `a` and the elements in `x` are independent, the mean is 0 and the std is 1.
+#
+# This can also be seen experimentally:
+
+var mean = Float()
+var sqr = Float()
+for i in 0..<10000 {
+    let x = TF(randomNormal: [])
+    let a = TF(randomNormal: [])
+    let y = a * x
+    mean += y.scalarized()
+    sqr  += pow(y, 2).scalarized()
+}
+(mean/10000,sqrt(sqr/10000))
+
+# Then we sum 512 of those things that have a mean of zero, and a variance of 1, so we get something that has a mean of 0, and variance of 512. To go to the standard deviation, we have to add a square root, hence `sqrt(512)` being our magic number.
+#
+# If we scale the weights of the matrix `a` and divide them by this `sqrt(512)`, it will give us a `y` of scale 1, and repeating the product as many times as we want and it won't overflow or vanish.
+
+# ### Adding ReLU in the mix
+
+# We can reproduce the previous experiment with a ReLU, to see that this time, the mean shifts and the variance becomes 0.5. This time the magic number will be `math.sqrt(2/512)` to properly scale the weights of the matrix.
+
+var mean = Float()
+var sqr = Float()
+for i in 0..<10000 {
+    let x = TF(randomNormal: [])
+    let a = TF(randomNormal: [])
+    var y = (a*x).scalarized()
+    y = y < 0 ? 0 : y
+    mean += y
+    sqr  += pow(y, 2)
+}
+(mean: mean/10000, sqrt: sqr/10000)
+
+# We can double check by running the experiment on the whole matrix product. The variance becomes 512/2 this time:
+
+var mean = Float()
+var sqr = Float()
+for i in 0..<100 {
+    let x = TF(randomNormal: [512, 1])
+    let a = TF(randomNormal: [512, 512])
+    var y = a • x
+    y = max(y, TF(zeros: y.shape))
+    mean += y.mean().scalarized()
+    sqr  += pow(y, 2).mean().scalarized()
+}
+(mean: mean/100, sqrt: sqr/100)
+
+# Or that scaling the coefficient with the magic number gives us a scale of 1.
+
+var mean = Float()
+var sqr = Float()
+for i in 0..<100 {
+    let x = TF(randomNormal: [512, 1])
+    let a = TF(randomNormal: [512, 512]) * sqrt(2/512)
+    var y = a • x
+    y = max(y, TF(zeros: y.shape))
+    mean += y.mean().scalarized()
+    sqr  += pow(y, 2).mean().scalarized()
+}
+(mean: mean/100, sqrt: sqr/100)
+
+# The math behind is a tiny bit more complex, and you can find everything in the [Kaiming](https://arxiv.org/abs/1502.01852) and the [Xavier](http://proceedings.mlr.press/v9/glorot10a.html) paper but this gives the intuition behind those results.
+
+
diff --git a/nbs/swift/02c_autodiff.ipynb b/nbs/swift/02c_autodiff.ipynb
index da79fb2..f5df908 100644
--- ./nbs/swift/02c_autodiff.ipynb
+++ ./nbs/swift/02c_autodiff.ipynb
@@ -471,6 +471,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/02c_autodiff.py b/nbs/swift/02c_autodiff.py
new file mode 100644
index 0000000..dc95621
--- /dev/null
+++ ./nbs/swift/02c_autodiff.py
@@ -0,0 +1,254 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+# # Automatic Differentiation In Swift
+#
+# This notebook builds up the concepts of automatic differentiation in Swift from the constituent pieces.
+
+# ## Step 1: A Trivial Example
+
+# As a warm up, we will start with a trivial example $x^2$. The derivative $\frac{d}{dx} x^2$ is $2x$. We can represent this as follows in code.
+
+# +
+func square(_ x: Float) -> Float {
+    return x * x
+}
+
+func square_derivative(_ x: Float) -> Float {
+    return 2 * x
+}
+# -
+
+# ## Aside: Recall the Chain Rule
+
+# As we discussed before, the chain rule tells us how to differentiate composite functions, and is written: $$\frac{d}{dx}\left[f\left(g(x)\right)\right] = f'\left(g(x)\right)g'(x)$$
+
+# ## Step 2: A slightly more complicated example.
+
+# Simple polynomials are the easy case. Let's take the derivative of a more complicated function: $\sin(x^2)$.
+#
+# The derivative of this expression $\frac{d}{dx}\sin(x^2)$ (recall the chain rule) is: $\cos(x^2) \cdot 2x$.
+#
+# In code, this is as follows:
+
+# +
+import Glibc
+
+func exampleFunction(_ x: Float) -> Float {
+    return sin(x * x)
+}
+
+func exampleFunctionDerivative(_ x: Float) -> Float {
+    return 2 * x * cos(x * x)
+}
+# -
+
+# ## Step 3: A more efficient implementation
+
+# Looking at the chain rule and our derivative implementation above, we notice that there's redundant computation going on. Concretely, in both `exampleFunction` and `exampleFunctionDerivative` we compute `x * x`. (In the chain rule definition, this is $g(x)$.) As a result, we often want to do that computation only once (because in general it can be any expensive computation, and even a multiply can be expensive with large tensors).
+#
+# We can thus rewrite our function and its derivative as follows:
+
+func exampleFunctionDerivativeEfficient(_ x: Float) -> (value: Float, backward: () -> Float) {
+    let xSquared = x * x
+    let value = sin(xSquared)
+    let backward = {2 * x * cos(xSquared)}  // A closure that captures xSquared
+    return (value: value, backward: backward)
+}
+
+# You'll see that we're defining a somewhat more complex *closure* than we've seen before here.
+
+# ## Aside: Fully general derivatives
+
+# We've actually been a little sloppy with our mathematics. To be fully correct, $\frac{d}{dx}x^2 = 2x\frac{d}{dx}$. This is because in mathematics, $x$ doesn't have to be a specific number, it could be itself another expression, which we'd need to use the chain rule to calculate. In order to represent this correctly in code, we need to change the type signature slightly to multiply by the "$\frac{d}{dx}$", resulting in the following (we're changing the name `backward` to `deriv` here to signify that it's a little different):
+
+func exampleFunctionValueWithDeriv(_ x: Float) -> (value: Float, deriv: (Float) -> Float) {
+    let xSquared = x * x
+    let value = sin(xSquared)
+    let deriv = { (v: Float) -> Float in
+        let gradXSquared = v * cos(xSquared)
+        let gradX = gradXSquared * 2 * x
+        return gradX
+    }
+    return (value: value, deriv: deriv)
+}
+
+# ## Step 4: Rewrite using `deriv`s
+
+# We've chosen to represent the drivatives with a `deriv` closure because this allows us to rewrite the forward pass into a very regular form. Below, we rewrite the handwritten derivative above into a regular form.
+#
+# > Note: be sure to carefully read through the code and convince yourself that this new spelling of the `deriv` results in the exact same computation.
+
+# +
+func sinValueWithDeriv(_ x: Float) -> (value: Float, deriv: (Float) -> Float) {
+    return (value: sin(x), deriv: {v in cos(x) * v})
+}
+
+func squareValueWithDeriv(_ x: Float) -> (value: Float, deriv: (Float) -> Float) {
+    return (value: x * x, deriv: {v in 2 * x * v})
+}
+
+func exampleFunctionWithDeriv(_ x: Float) -> (value: Float, deriv: (Float) -> Float) {
+    let (xSquared, deriv1) = squareValueWithDeriv(x)
+    let (value, deriv2) = sinValueWithDeriv(xSquared)
+    return (value: value, deriv: { v in
+        let gradXSquared = deriv2(v)
+        let gradX = deriv1(gradXSquared)
+        return gradX
+    })
+}
+# -
+
+# ## Aside: Generalizing to arbitrary expressions.
+
+# Up until this point, we've been handwriting the derivatives for specific functions. But we now have a formulation that is regular and composible. (In fact, it is so regular, we can make the computer write the backwards pass for us! aka automatic differentiation.) The rules are:
+#
+#  1. Rewrite every expression in the forward pass into a form that computes the value like normal, and also produces an additional deriv function.
+#  2. Construct a backwards pass that threads the derivs together in the reverse order.
+
+# In an abstract form, we transform a function that looks like:
+#
+# ```swift
+# func myFunction(_ arg: Float) -> Float {
+#     let tmp1 = expression1(arg)
+#     let tmp2 = expression2(tmp1)
+#     let tmp3 = expression3(tmp2)
+#     return tmp3
+# }
+# ```
+
+# into a function that looks like this:
+#
+# ```swift
+# func myFunctionValueWithDeriv(_ arg: Float) -> (value: Float, deriv: (Float) -> Float) {
+#     let (tmp1, deriv1) = expression1ValueWithDeriv(arg)
+#     let (tmp2, deriv2) = expression2ValueWithDeriv(tmp1)
+#     let (tmp3, deriv3) = expression3ValueWithDeriv(tmp2)
+#     return (value: tmp3,
+#             deriv: { v in
+#                 let grad2 = deriv3(v)
+#                 let grad1 = deriv2(grad2)
+#                 let gradArg = deriv1(grad1)
+#                 return gradArg
+#     })
+# }
+# ```
+
+# ## Step 5: Generalize beyond unary functions
+
+# Up until now, we have been using functions that don't "reuse" values in the forward pass. Our running example of $\frac{d}{dx}\sin(x^2)$ is too simple. Let's make it a bit more complicated, and use $\frac{d}{dx}\sin(x^2)+x^2$ as our motivating expression going forward. From mathematics, we know that the derivative should be: $$\frac{d}{dx}\sin\left(x^2\right) + x^2 = \left(2x\cos\left(x^2\right)+2x\right)\frac{d}{dx}$$
+#
+# Let's see how we write the deriv (pay attention to the signature of the deriv for the `+` function)!
+
+# +
+func myComplexFunction(_ x: Float) -> Float {
+    let tmp1 = square(x)
+    let tmp2 = sin(tmp1)
+    let tmp3 = tmp2 + tmp1
+    return tmp3
+}
+
+func plusWithDeriv(_ x: Float, _ y: Float) -> (value: Float, deriv: (Float) -> (Float, Float)) {
+    return (value: x + y, deriv: {v in (v, v)})  // Value semantics are great! :-)
+}
+# -
+
+func myComplexFunctionValueWithDeriv(_ x: Float) -> (value: Float, deriv: (Float) -> Float) {
+    let (tmp1, pb1) = squareValueWithDeriv(x)
+    let (tmp2, pb2) = sinValueWithDeriv(tmp1)
+    let (tmp3, pb3) = plusWithDeriv(tmp2, tmp1)
+    return (value: tmp3,
+            deriv: { v in
+        // Initialize the gradients for all values at zero.
+        var gradX = Float(0.0)
+        var grad1 = Float(0.0)
+        var grad2 = Float(0.0)
+        var grad3 = Float(0.0)
+        // Add the temporaries to the gradients as we run the backwards pass.
+        grad3 += v
+        let (tmp2, tmp1b) = pb3(grad3)
+        grad2 += tmp2
+        grad1 += tmp1b
+        let tmp1a = pb2(grad2)
+        grad1 += tmp1a
+        let tmpX = pb1(grad1)
+        gradX += tmpX
+        // Return the computed gradients.
+        return gradX
+    })
+}
+
+// Helper method
+func square(_ x: Float) -> Float {
+    return x * x
+}
+
+# Non-unary functions (e.g. `+`) have a deriv that returns a tuple that corresponds to their arguments. This allows gradients to flow upwards in a pure functional manner.
+#
+# In order to handle the re-use of intermediary values (in this case, the expression $x^2$), we need to introduce 2 additional concepts:
+#
+#  1. **Sum**: We need to sum the derivatives produced by $\frac{d}{dx}x^2$ to the values produced from $\frac{d}{dx}\sin\left(x^2\right)$ in order to correctly compute the derivative value of $\frac{d}{dx}\left(\sin\left(x^2\right) + x^2\right)$.
+#  2. **Zero**: As a result, we need to initialize the derivatives for each variable to a value: zero!
+#
+# We now have all the pieces required for automatic differentiation in Swift. Let's see how they come together.
+
+# ## Step 6: Automatic Differentiation in Swift
+#
+# When you annotate a function `@differentiable`, the compiler will take your function and generate a second function that corresponds to the `...ValueWithDeriv` functions we wrote out by hand above using the simple transformation rules.
+#
+# You can access these auto-generated function by calling `valueWithPullback`:
+
+@differentiable
+func myFunction(_ x: Float) -> Float {
+    return x * x
+}
+
+let (value, deriv) = valueWithPullback(at: 3, in: myFunction)
+print(value)
+print(type(of: deriv))
+
+# ## Step 7: gradient
+#
+# Now that we have a deriv, how to we "kick off" the deriv computation to actually compute the derivative? We use the constant value `1.0`!
+
+deriv(1)
+
+# We have no re-implemented the `gradient` function.
+
+# ## Step 8: Generalized Differentiability & Protocols
+
+# So far, we've been looking at functions operating on scalar (`Float`) values, but you can take derivatives of functions that operate on vectors (aka higher dimensions) too. In order to support this, you need your type to conform to the `Differentiable` protocol, which often involves ensuring your type conforms to the [`AdditiveArithmetic` protocol](https://github.com/apple/swift/blob/0c452616820bfbc4f3197dd418c74adadc830b5c/stdlib/public/core/Integers.swift#L31). The salient bits of that protocol are:
+
+# ```swift
+# public protocol AdditiveArithmetic : Equatable {
+#   /// The zero value.
+#   ///
+#   /// - Note: Zero is the identity element for addition; for any value,
+#   ///   `x + .zero == x` and `.zero + x == x`.
+#   static var zero: Self { get }
+#   /// Adds two values and produces their sum.
+#   ///
+#   /// - Parameters:
+#   ///   - lhs: The first value to add.
+#   ///   - rhs: The second value to add.
+#   static func +(lhs: Self, rhs: Self) -> Self
+#   
+#   //...
+# }
+# ```
+#
+# > Note: The [`Differentiable`](https://github.com/apple/swift/blob/0c452616820bfbc4f3197dd418c74adadc830b5c/stdlib/public/core/AutoDiff.swift#L102) protocol is slightly more complicated in order to support non-differentiable member variables, such as  activation functions and other non-differentiable member variables.
+
+# ### Next up: The `Layer` protocol!
diff --git a/nbs/swift/03_minibatch_training.ipynb b/nbs/swift/03_minibatch_training.ipynb
index 11ce983..e50be21 100644
--- ./nbs/swift/03_minibatch_training.ipynb
+++ ./nbs/swift/03_minibatch_training.ipynb
@@ -1007,6 +1007,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/03_minibatch_training.py b/nbs/swift/03_minibatch_training.py
new file mode 100644
index 0000000..1a60063
--- /dev/null
+++ ./nbs/swift/03_minibatch_training.py
@@ -0,0 +1,320 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+# # Minibatch training
+
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/FastaiNotebook_02a_why_sqrt5")' FastaiNotebook_02a_why_sqrt5
+
+//export
+import Path
+import TensorFlow
+
+import FastaiNotebook_02a_why_sqrt5
+
+# Our labels will be integeres from now on, so to go with our `TF` abbreviation, we introduce `TI`.
+
+// export
+public typealias TI = Tensor<Int32>
+
+# ### Data
+
+# We gather the MNIST data like in the previous notebooks.
+
+var (xTrain,yTrain,xValid,yValid) = loadMNIST(path: Path.home/".fastai"/"data"/"mnist_tst", flat: true)
+
+let trainMean = xTrain.mean()
+let trainStd  = xTrain.std()
+
+xTrain = normalize(xTrain, mean: trainMean, std: trainStd)
+xValid = normalize(xValid, mean: trainMean, std: trainStd)
+
+let (n,m) = (xTrain.shape[0],xTrain.shape[1])
+let c = yTrain.max().scalarized()+1
+print(n,m,c)
+
+# We also define a simple model using our `FADense` layers.
+
+let nHid = 50
+
+public struct MyModel: Layer {
+    public var layer1: FADense<Float>
+    public var layer2: FADense<Float>
+    
+    public init(nIn: Int, nHid: Int, nOut: Int){
+        layer1 = FADense(nIn, nHid, activation: relu)
+        layer2 = FADense(nHid, nOut)
+    }
+    
+    @differentiable
+    public func callAsFunction(_ input: TF) -> TF {
+        return input.sequenced(through: layer1, layer2)
+    }
+}
+
+var model = MyModel(nIn: m, nHid: nHid, nOut: Int(c))
+
+let pred = model(xTrain)
+
+# ### Cross entropy loss
+
+# Before we can train our model, we need to have a loss function. We saw how to write `logSoftMax` from scratch in PyTorch, but let's do it once in swift too.
+
+func logSoftmax<Scalar>(_ activations: Tensor<Scalar>) -> Tensor<Scalar> where Scalar:TensorFlowFloatingPoint{
+    let exped = exp(activations) 
+    return log(exped / exped.sum(alongAxes: -1))
+}
+
+let smPred = logSoftmax(pred)
+
+yTrain[0..<3]
+
+(smPred[0][5],smPred[1][0],smPred[2][4])
+
+# There is no fancy indexing yet so we have to use gather to get the indices we want out of our softmaxed predictions.
+
+func nll<Scalar>(_ input: Tensor<Scalar>, _ target :TI) -> Tensor<Scalar> 
+    where Scalar:TensorFlowFloatingPoint{
+        let idx: TI = Raw.range(start: Tensor(0), limit: Tensor(numericCast(target.shape[0])), delta: Tensor(1))
+        let indices = Raw.concat(concatDim: Tensor(1), [idx.expandingShape(at: 1), target.expandingShape(at: 1)])
+        let losses = Raw.gatherNd(params: input, indices: indices)
+        return -losses.mean()
+    }
+
+nll(smPred, yTrain)
+
+time(repeating: 100){ let _ = nll(smPred, yTrain) }
+
+# Simplify `logSoftmax` with log formulas.
+
+func logSoftmax<Scalar>(_ activations: Tensor<Scalar>) -> Tensor<Scalar> where Scalar:TensorFlowFloatingPoint{
+    return activations - log(exp(activations).sum(alongAxes: -1))
+}
+
+let smPred = logSoftmax(pred)
+
+nll(smPred, yTrain)
+
+# We know use the LogSumExp trick
+
+smPred.max(alongAxes: -1).shape
+
+func logSumExp<Scalar>(_ x: Tensor<Scalar>) -> Tensor<Scalar> where Scalar:TensorFlowFloatingPoint{
+    let m = x.max(alongAxes: -1)
+    return m + log(exp(x-m).sum(alongAxes: -1))
+}
+
+func logSoftmax<Scalar>(_ activations: Tensor<Scalar>) -> Tensor<Scalar> where Scalar:TensorFlowFloatingPoint{
+    return activations - logSumExp(activations)
+}
+
+let smPred = logSoftmax(pred)
+
+nll(smPred, yTrain)
+
+# In S4TF nll loss is combined with softmax in:
+
+let loss = softmaxCrossEntropy(logits: pred, labels: yTrain)
+loss
+
+time(repeating: 100){ _ = nll(logSoftmax(pred), yTrain)}
+
+time(repeating: 100){ _ = softmaxCrossEntropy(logits: pred, labels: yTrain)}
+
+# ## Basic training loop
+
+# Basically the training loop repeats over the following steps:
+# - get the output of the model on a batch of inputs
+# - compare the output to the labels we have and compute a loss
+# - calculate the gradients of the loss with respect to every parameter of the model
+# - update said parameters with those gradients to make them a little bit better
+
+// export
+public func accuracy(_ output: TF, _ target: TI) -> TF{
+    let corrects = TF(output.argmax(squeezingAxis: 1) .== target)
+    return corrects.mean()
+}
+
+# We have a raw model for now, so it should be as good as random: 10% accuracy.
+
+print(accuracy(pred, yTrain))
+
+# So let's begin with a minibatch.
+
+let bs=64                     // batch size
+let xb = xTrain[0..<bs]       // a mini-batch from x
+let preds = model(xb)         // predictions
+print(preds[0], preds.shape)
+
+# Then we can compute a loss
+
+let yb = yTrain[0..<bs]
+let loss = softmaxCrossEntropy(logits: preds, labels: yb)
+
+print(accuracy(preds, yb))
+
+let lr:Float = 0.5   // learning rate
+let epochs = 1       // how many epochs to train for
+
+# Then we can get our loss and gradients.
+
+# Sometimes you'll see closures written this way (required if there is >1 statement in it).
+
+let (loss, grads) = model.valueWithGradient { model -> TF in
+    let preds = model(xb)
+    return softmaxCrossEntropy(logits: preds, labels: yb)
+}
+
+# The full loop by hand would look like this:
+
+for epoch in 1 ... epochs {
+    for i in 0 ..< (n-1)/bs {
+        let startIdx = i * bs
+        let endIdx = startIdx + bs
+        let xb = xTrain[startIdx..<endIdx]
+        let yb = yTrain[startIdx..<endIdx]
+        let (loss, grads) = model.valueWithGradient {
+            softmaxCrossEntropy(logits: $0(xb), labels: yb)
+        }
+        model.layer1.weight -= lr * grads.layer1.weight
+        model.layer1.bias   -= lr * grads.layer1.bias
+        model.layer2.weight -= lr * grads.layer2.weight
+        model.layer2.bias   -= lr * grads.layer2.bias
+    }
+}
+
+let preds = model(xValid)
+accuracy(preds, yValid)
+
+# `>80%` in one epoch, not too bad!
+
+# We use a shorcut: `model.variables` stands for `model.allDifferentiableVariables` in S4TF. It extracts from our model a new struct with only the trainable parameters. For instance if `model` is a BatchNorm layer, it has four tensor of floats: running mean, runing std, weights and bias. The corresponding `model.variables` only has the weights and bias tensors.
+#
+# When we get the gradients of our model, we have another structure of the same type, and it's possible to perform basic arithmetic on those structures to make the update step super simple:
+
+for epoch in 1 ... epochs {
+    for i in 0 ..< (n-1)/bs {
+        let startIdx = i * bs
+        let endIdx = startIdx + bs
+        let xb = xTrain[startIdx..<endIdx]
+        let yb = yTrain[startIdx..<endIdx]
+        let (loss, grads) = model.valueWithGradient {
+            softmaxCrossEntropy(logits: $0(xb), labels: yb)
+        }
+        model.variables -= grads.scaled(by: lr)
+    }
+}
+
+# Then we can use a S4TF optimizer to do the step for us (which doesn't win much just yet - but will be nice when we can use momentum, adam, etc). An optimizer takes a `Model.AllDifferentiableVariables` object and some gradients, and will perform the update.
+
+let optimizer = SGD(for: model, learningRate: lr)
+
+# Here's a handy function (thanks for Alexis Gallagher) to grab a batch of indices at a time.
+
+//export
+public func batchedRanges(start:Int, end:Int, bs:Int) -> UnfoldSequence<Range<Int>,Int>
+{
+  return sequence(state: start) { (batchStart) -> Range<Int>? in
+    let remaining = end - batchStart
+    guard remaining > 0 else { return nil}
+    let currentBs = min(bs,remaining)
+    let batchEnd = batchStart.advanced(by: currentBs)
+    defer {  batchStart = batchEnd  }
+    return batchStart ..< batchEnd
+  }
+}
+
+for epoch in 1 ... epochs{
+    for b in batchedRanges(start: 0, end: n, bs: bs) {
+        let (xb,yb) = (xTrain[b],yTrain[b])
+        let (loss, grads) = model.valueWithGradient {
+            softmaxCrossEntropy(logits: $0(xb), labels: yb)
+        }
+        optimizer.update(&model.variables, along: grads)
+    }
+}
+
+# ## Dataset
+
+# We can create a swift `Dataset` from our arrays. It will automatically batch things for us:
+
+// export
+public struct DataBatch<Inputs: Differentiable & TensorGroup, Labels: TensorGroup>: TensorGroup {
+    public var xb: Inputs
+    public var yb: Labels
+    
+    public init(xb: Inputs, yb: Labels){ (self.xb,self.yb) = (xb,yb) }
+}
+
+let trainDs = Dataset(elements:DataBatch(xb:xTrain, yb:yTrain)).batched(bs)
+
+for epoch in 1...epochs{
+    for batch in trainDs {
+        let (loss, grads) = model.valueWithGradient {
+            softmaxCrossEntropy(logits: $0(xb), labels: yb)
+        }
+        optimizer.update(&model.variables, along: grads)
+    }
+}
+
+# This `Dataset` can also do the shuffle for us:
+
+for epoch in 1...epochs{
+    for batch in trainDs.shuffled(sampleCount: yTrain.shape[0], randomSeed: 42){
+        let (loss, grads) = model.valueWithGradient {
+            softmaxCrossEntropy(logits: $0(xb), labels: yb)
+        }
+        optimizer.update(&model.variables, along: grads)
+    }
+}
+
+# ### Training loop
+
+# With everything before, we can now write a generic training loop. It needs two generic types: the optimizer (`Opt`) and the labels (`Label`):
+
+public func train<Opt: Optimizer, Label:TensorGroup>(
+    _ model: inout Opt.Model,
+    on ds: Dataset<DataBatch<Opt.Model.Input, Label>>,
+    using opt: inout Opt,
+    lossFunc: @escaping @differentiable (Opt.Model.Output, @nondiff Label) -> Tensor<Opt.Scalar>
+) where Opt.Model: Layer,
+        Opt.Model.Input: TensorGroup,
+        Opt.Model.TangentVector == Opt.Model.AllDifferentiableVariables,
+        Opt.Scalar: TensorFlowFloatingPoint
+{
+    for batch in ds {
+        let (loss, 𝛁model) = model.valueWithGradient {
+            lossFunc($0(batch.xb), batch.yb)
+        }
+        opt.update(&model.variables, along: 𝛁model)
+    }
+}
+
+var model = MyModel(nIn: m, nHid: nHid, nOut: Int(c))
+var optimizer = SGD(for: model, learningRate: lr)
+
+train(&model, on: trainDs, using: &optimizer, lossFunc: softmaxCrossEntropy)
+
+let preds = model(xValid)
+accuracy(preds, yValid)
+
+# ### Export
+
+import NotebookExport
+let exporter = NotebookExport(Path.cwd/"03_minibatch_training.ipynb")
+print(exporter.export(usingPrefix: "FastaiNotebook_"))
+
+
diff --git a/nbs/swift/04_callbacks.ipynb b/nbs/swift/04_callbacks.ipynb
index 77139d4..448df6d 100644
--- ./nbs/swift/04_callbacks.ipynb
+++ ./nbs/swift/04_callbacks.ipynb
@@ -973,6 +973,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/04_callbacks.py b/nbs/swift/04_callbacks.py
new file mode 100644
index 0000000..f9b1110
--- /dev/null
+++ ./nbs/swift/04_callbacks.py
@@ -0,0 +1,446 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+# # Callbacks
+
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/FastaiNotebook_03_minibatch_training")' FastaiNotebook_03_minibatch_training
+
+//export
+import Path
+import TensorFlow
+
+import FastaiNotebook_03_minibatch_training
+
+# ## Load data
+
+# We load our data and define a basic model like in the previous notebook.
+
+var (xTrain,yTrain,xValid,yValid) = loadMNIST(path: mnistPath, flat: true)
+
+let (n,m) = (xTrain.shape[0],xTrain.shape[1])
+let c = yTrain.max().scalarized()+1
+print(n,m,c)
+let nHid = 50
+
+// export
+public struct BasicModel: Layer {
+    public var layer1, layer2: FADense<Float>
+    
+    public init(nIn: Int, nHid: Int, nOut: Int){
+        layer1 = FADense(nIn, nHid, activation: relu)
+        layer2 = FADense(nHid, nOut)
+    }
+    
+    @differentiable
+    public func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {
+        return layer2(layer1(input))
+    }
+}
+
+# We can also directly define our model as an array of `FADense` layers:
+
+var model: [FADense<Float>] = [
+    FADense(m, nHid, activation: relu),
+    FADense(nHid, Int(c))] // BasicModel(nIn: m, nHid: nHid, nOut: Int(c))
+
+# ### Dataset/DataBunch
+
+# We add our own wrapper above the S4TF `Dataset` for several reasons:
+# - in S4TF, `Dataset` has no length and we need a `count` property to be able to do efficient hyper-parameters scheduling.
+# - you can only apply `batched` once to a `Dataset` but we sometimes want to change the batch size. We save the original non-batched datasetin `innerDs`.
+# - the shuffle needs to be called each time we want to reshuffle, so we make this happen in the compute property `ds`.
+
+//export 
+public struct FADataset<Element> where Element: TensorGroup {
+    public var innerDs: Dataset<Element>
+    public var shuffle = false
+    public var bs = 64 
+    public var dsCount: Int
+    
+    public var count: Int {
+        return dsCount%bs == 0 ? dsCount/bs : dsCount/bs+1
+    }
+    
+    public var ds: Dataset<Element> { 
+        if !shuffle { return innerDs.batched(bs)}
+        let seed = Int64.random(in: Int64.min..<Int64.max)
+        return innerDs.shuffled(sampleCount: dsCount, randomSeed: seed).batched(bs)
+    }
+    
+    public init(_ ds: Dataset<Element>, len: Int, shuffle: Bool = false, bs: Int = 64) {
+        (self.innerDs,self.dsCount,self.shuffle,self.bs) = (ds, len, shuffle, bs)
+    }
+}
+
+# Then we can define a `DataBunch` to group our training and validation datasets.
+
+// export
+public struct DataBunch<Element> where Element: TensorGroup{
+    public var train, valid: FADataset<Element>
+    
+    public init(train: Dataset<Element>, valid: Dataset<Element>, trainLen: Int, validLen: Int, bs: Int = 64) {
+        self.train = FADataset(train, len: trainLen, shuffle: true,  bs: bs)
+        self.valid = FADataset(valid, len: validLen, shuffle: false, bs: 2*bs)
+    }
+}
+
+# And add a convenience function to get MNIST in a `DataBunch` directly.
+
+//export
+public func mnistDataBunch(path: Path = mnistPath, flat: Bool = false, bs: Int = 64)
+   -> DataBunch<DataBatch<TF, TI>> {
+    let (xTrain,yTrain,xValid,yValid) = loadMNIST(path: path, flat: flat)
+    return DataBunch(train: Dataset(elements: DataBatch(xb:xTrain, yb: yTrain)), 
+                     valid: Dataset(elements: DataBatch(xb:xValid, yb: yValid)),
+                     trainLen: xTrain.shape[0],
+                     validLen: xValid.shape[0],
+                     bs: bs)
+}
+
+let data = mnistDataBunch(flat: true)
+
+data.train.count
+
+# ## Shuffle test
+
+# Timing
+
+//export
+public extension Sequence {
+  func first() -> Element? {
+    return first(where: {_ in true})
+  }
+}
+
+time(repeating: 10) {
+  let tst = data.train.ds
+
+  tst.first()!.yb
+}
+
+# Check we get different batches:
+
+var tst = data.train.ds
+tst.first()!.yb
+
+tst = data.train.ds
+tst.first()!.yb
+
+# # `Learner`,  `LearnerAction`: enums and error handling in Swift, oh my!
+
+# Just like in Python, we'll use "exception handling" to let custom actions indicate that they want to stop, skip over a batch or do other custom processing - e.g. for early stopping.
+#
+# We'll start by defining a custom type to represent the stop reason, and we'll use a Swift enum to describe it:
+
+// export
+public enum LearnerAction: Error {
+    case skipEpoch(reason: String)
+    case skipBatch(reason: String)
+    case stop(reason: String)
+}
+
+# Now this a bit of an unusual thing - we have met protocols before, and `: Error` is a protocol that `LearnerAction` conforms to, but what is going on with those cases?
+#
+# Let's jump briefly into slides to talk about Swift enums:
+#
+# **Slides:** [Supercharged Enums in Swift](https://docs.google.com/presentation/d/1dc6o2o-uYGnJeCeyvgsgyk05dBMneArxdICW5vF75oU/edit#slide=id.g512a2e238a_144_147)
+#
+#
+
+# ### Basic `Learner` class
+
+// export
+/// Initializes and trains a model on a given dataset.
+public final class Learner<Label: TensorGroup,
+                           Opt: TensorFlow.Optimizer & AnyObject>
+    where Opt.Scalar: Differentiable,
+          Opt.Model: Layer,
+          // Constrain model input to Tensor<Float>, to work around
+          // https://forums.fast.ai/t/fix-ad-crash-in-learner/42970.
+          Opt.Model.Input == Tensor<Float>
+{
+    public typealias Model = Opt.Model
+    public typealias Input = Model.Input
+    public typealias Output = Model.Output
+    public typealias Data = DataBunch<DataBatch<Input, Label>>
+    public typealias Loss = TF
+    public typealias Optimizer = Opt
+    public typealias Variables = Model.AllDifferentiableVariables
+    public typealias EventHandler = (Learner) throws -> Void
+    
+    /// A wrapper class to hold the loss function, to work around
+    // https://forums.fast.ai/t/fix-ad-crash-in-learner/42970.
+    public final class LossFunction {
+        public typealias F = @differentiable (Model.Output, @nondiff Label) -> Loss
+        public var f: F
+        init(_ f: @escaping F) { self.f = f }
+    }
+    
+    public var data: Data
+    public var opt: Optimizer
+    public var lossFunc: LossFunction
+    public var model: Model
+    
+    public var currentInput: Input!
+    public var currentTarget: Label!
+    public var currentOutput: Output!
+    
+    public private(set) var epochCount = 0
+    public private(set) var currentEpoch = 0
+    public private(set) var currentGradient = Model.TangentVector.zero
+    public private(set) var currentLoss = Loss.zero
+    public private(set) var inTrain = false
+    public private(set) var pctEpochs = Float.zero
+    public private(set) var currentIter = 0
+    public private(set) var iterCount = 0
+    
+    open class Delegate {
+        open var order: Int { return 0 }
+        public init () {}
+        
+        open func trainingWillStart(learner: Learner) throws {}
+        open func trainingDidFinish(learner: Learner) throws {}
+        open func epochWillStart(learner: Learner) throws {}
+        open func epochDidFinish(learner: Learner) throws {}
+        open func validationWillStart(learner: Learner) throws {}
+        open func batchWillStart(learner: Learner) throws {}
+        open func batchDidFinish(learner: Learner) throws {}
+        open func didProduceNewGradient(learner: Learner) throws {}
+        open func optimizerDidUpdate(learner: Learner) throws {}
+        open func batchSkipped(learner: Learner, reason:String) throws {}
+        open func epochSkipped(learner: Learner, reason:String) throws {}
+        open func trainingStopped(learner: Learner, reason:String) throws {}
+        ///
+        /// TODO: learnerDidProduceNewOutput and learnerDidProduceNewLoss need to
+        /// be differentiable once we can have the loss function inside the Learner
+    }
+    
+    public var delegates: [Delegate] = [] {
+        didSet { delegates.sort { $0.order < $1.order } }
+    }
+    
+    public init(data: Data, lossFunc: @escaping LossFunction.F,
+                optFunc: (Model) -> Optimizer, modelInit: ()->Model) {
+        (self.data,self.lossFunc) = (data,LossFunction(lossFunc))
+        model = modelInit()
+        opt = optFunc(self.model)
+    }
+}
+
+# Then let's write the parts of the training loop:
+
+// export
+extension Learner {
+    private func evaluate(onBatch batch: DataBatch<Input, Label>) throws {
+        currentOutput = model(currentInput)
+        currentLoss = lossFunc.f(currentOutput, currentTarget)
+    }
+    
+    private func train(onBatch batch: DataBatch<Input, Label>) throws {
+        let (xb,yb) = (currentInput!,currentTarget!) //We still have to force-unwrap those for AD...
+        (currentLoss, currentGradient) = model.valueWithGradient { model -> Loss in 
+            let y = model(xb)                                      
+            self.currentOutput = y
+            return self.lossFunc.f(y, yb)
+        }
+        for d in delegates { try d.didProduceNewGradient(learner: self) }
+        opt.update(&model.variables, along: self.currentGradient)
+    }
+    
+    private func train(onDataset ds: FADataset<DataBatch<Input, Label>>) throws {
+        iterCount = ds.count
+        for batch in ds.ds {
+            (currentInput, currentTarget) = (batch.xb, batch.yb)
+            do {
+                for d in delegates { try d.batchWillStart(learner: self) }
+                if inTrain { try train(onBatch: batch) } else { try evaluate(onBatch: batch) }
+            }
+            catch LearnerAction.skipBatch(let reason) {
+                for d in delegates {try d.batchSkipped(learner: self, reason:reason)}
+            }
+            for d in delegates { try d.batchDidFinish(learner: self) }
+        }
+    }
+}
+
+# And the whole fit function.
+
+// export
+extension Learner {
+    /// Starts fitting.
+    /// - Parameter epochCount: The number of epochs that will be run.
+    public func fit(_ epochCount: Int) throws {
+        self.epochCount = epochCount
+        do {
+            for d in delegates { try d.trainingWillStart(learner: self) }
+            for i in 0..<epochCount {
+                self.currentEpoch = i
+                do {
+                    for d in delegates { try d.epochWillStart(learner: self) }
+                    try train(onDataset: data.train)
+                    for d in delegates { try d.validationWillStart(learner: self) }
+                    try train(onDataset: data.valid)
+                } catch LearnerAction.skipEpoch(let reason) {
+                    for d in delegates {try d.epochSkipped(learner: self, reason:reason)}
+                }
+                for d in delegates { try d.epochDidFinish(learner: self) }
+            }
+        } catch LearnerAction.stop(let reason) {
+            for d in delegates {try d.trainingStopped(learner: self, reason:reason)}
+        }
+
+        for d in delegates { try d.trainingDidFinish(learner: self) }
+    }
+}
+
+# ### Test
+
+func optFunc(_ model: BasicModel) ->  SGD<BasicModel> { return SGD(for: model, learningRate: 1e-2)}
+
+func modelInit() -> BasicModel {return BasicModel(nIn: m, nHid: nHid, nOut: Int(c))}
+
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+
+learner.fit(2)
+
+# ## Let's add Callbacks!
+
+# Extension with convenience methods to add delegates:
+
+// export
+public extension Learner {
+    func addDelegate (_ delegate :  Learner.Delegate ) { delegates.append(delegate) }
+    func addDelegates(_ delegates: [Learner.Delegate]) { self.delegates += delegates }
+}
+
+# ### Train/eval
+
+# Callback classes are defined as extensions of the Learner.
+
+// export
+extension Learner {
+    public class TrainEvalDelegate: Delegate {
+        public override func trainingWillStart(learner: Learner) {
+            learner.pctEpochs = 0.0
+        }
+
+        public override func epochWillStart(learner: Learner) {
+            Context.local.learningPhase = .training
+            (learner.pctEpochs,learner.inTrain,learner.currentIter) = (Float(learner.currentEpoch),true,0)
+        }
+        
+        public override func batchDidFinish(learner: Learner) {
+            learner.currentIter += 1
+            if learner.inTrain{ learner.pctEpochs += 1.0 / Float(learner.iterCount) }
+        }
+        
+        public override func validationWillStart(learner: Learner) {
+            Context.local.learningPhase = .inference
+            learner.inTrain = false
+            learner.currentIter = 0
+        }
+    }
+    
+    public func makeTrainEvalDelegate() -> TrainEvalDelegate { return TrainEvalDelegate() }
+}
+
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+
+learner.delegates = [learner.makeTrainEvalDelegate()]
+
+learner.fit(2)
+
+# ### AverageMetric
+
+// export
+extension Learner {
+    public class AvgMetric: Delegate {
+        public let metrics: [(Output, Label) -> TF]
+        var total: Int = 0
+        var partials = [TF]()
+        
+        public init(metrics: [(Output, Label) -> TF]) { self.metrics = metrics}
+        
+        public override func epochWillStart(learner: Learner) {
+            total = 0
+            partials = Array(repeating: Tensor(0), count: metrics.count + 1)
+        }
+        
+        public override func batchDidFinish(learner: Learner) {
+            if !learner.inTrain{
+                let bs = learner.currentInput!.shape[0] //Possible because Input is TF for now
+                total += bs
+                partials[0] += Float(bs) * learner.currentLoss
+                for i in 1...metrics.count{
+                    partials[i] += Float(bs) * metrics[i-1](learner.currentOutput!, learner.currentTarget!)
+                }
+            }
+        }
+        
+        public override func epochDidFinish(learner: Learner) {
+            for i in 0...metrics.count {partials[i] = partials[i] / Float(total)}
+            print("Epoch \(learner.currentEpoch): \(partials)")
+        }
+    }
+    
+    public func makeAvgMetric(metrics: [(Output, Label) -> TF]) -> AvgMetric{
+        return AvgMetric(metrics: metrics)
+    }
+}
+
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+
+learner.delegates = [learner.makeTrainEvalDelegate(), learner.makeAvgMetric(metrics: [accuracy])]
+
+learner.fit(2)
+
+# ### Normalization
+
+// export
+extension Learner {
+    public class Normalize: Delegate {
+        public let mean, std: TF
+        public init(mean: TF, std: TF) { (self.mean,self.std) = (mean,std) }
+        
+        public override func batchWillStart(learner: Learner) {
+            learner.currentInput = (learner.currentInput! - mean) / std
+        }
+    }
+    
+    public func makeNormalize(mean: TF, std: TF) -> Normalize{
+        return Normalize(mean: mean, std: std)
+    }
+}
+
+(mean: xTrain.mean(), std: xTrain.standardDeviation())
+
+// export
+public let mnistStats = (mean: TF(0.13066047), std: TF(0.3081079))
+
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+
+learner.delegates = [learner.makeTrainEvalDelegate(), learner.makeAvgMetric(metrics: [accuracy]),
+                     learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std)]
+
+learner.fit(2)
+
+# ## Export
+
+import NotebookExport
+let exporter = NotebookExport(Path.cwd/"04_callbacks.ipynb")
+print(exporter.export(usingPrefix: "FastaiNotebook_"))
+
+
diff --git a/nbs/swift/05_anneal.ipynb b/nbs/swift/05_anneal.ipynb
index 37dca3e..c113982 100644
--- ./nbs/swift/05_anneal.ipynb
+++ ./nbs/swift/05_anneal.ipynb
@@ -878,6 +878,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/05_anneal.py b/nbs/swift/05_anneal.py
new file mode 100644
index 0000000..c5837aa
--- /dev/null
+++ ./nbs/swift/05_anneal.py
@@ -0,0 +1,347 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+# # Annealing
+
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/FastaiNotebook_04_callbacks")' FastaiNotebook_04_callbacks
+
+//export
+import Path
+import TensorFlow
+
+import FastaiNotebook_04_callbacks
+
+%include "EnableIPythonDisplay.swift"
+IPythonDisplay.shell.enable_matplotlib("inline")
+
+# ## Load data
+
+let data = mnistDataBunch(flat: true)
+
+let (n,m) = (60000,784)
+let c = 10
+let nHid = 50
+
+func optFunc(_ model: BasicModel) -> SGD<BasicModel> {return SGD(for: model, learningRate: 1e-2)}
+
+func modelInit() -> BasicModel {return BasicModel(nIn: m, nHid: nHid, nOut: c)}
+
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+
+learner.delegates = [learner.makeTrainEvalDelegate(), learner.makeAvgMetric(metrics: [accuracy]),
+                     learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std)]
+
+//Crashes! See: SR-10436
+//learner.delegates = [type(of: learner).TrainEvalDelegate(), type(of: learner).AvgMetric(metrics: [accuracy])]
+
+learner.fit(2)
+
+# ## Recoder
+
+# The Recorder's role is to keep track of the loss and our scheduled learning rate. 
+
+// export
+import Python
+public let np = Python.import("numpy")
+public let plt = Python.import("matplotlib.pyplot")
+
+// export
+public func plot<S1, S2>(_ arr1: [S1], _ arr2: [S2], logScale:Bool = false, xLabel: String="", yLabel: String = "") 
+    where S1:PythonConvertible, S2:PythonConvertible{
+    plt.figure(figsize: [6,4])
+    let (npArr1, npArr2) = (np.array(arr1), np.array(arr2))
+    if logScale {plt.xscale("log")} 
+    if !xLabel.isEmpty {plt.xlabel(xLabel)}
+    if !yLabel.isEmpty {plt.ylabel(yLabel)}    
+    let fig = plt.plot(npArr1, npArr2)
+    plt.show(fig)
+}
+
+// export
+extension Learner where Opt.Scalar: PythonConvertible{
+    public class Recorder: Delegate {
+        public var losses: [Loss] = []
+        public var lrs: [Opt.Scalar] = []
+        
+        public override func batchDidFinish(learner: Learner) {
+            if learner.inTrain {
+                losses.append(learner.currentLoss)
+                lrs.append(learner.opt.learningRate)
+            }
+        }
+        
+        public func plotLosses(){
+            plot(Array(0..<losses.count), losses.map{$0.scalar}, xLabel:"iteration", yLabel:"loss")
+        }
+        
+        public func plotLRs(){
+            plot(Array(0..<lrs.count), lrs, xLabel:"iteration", yLabel:"lr")
+        }
+        
+        public func plotLRFinder(){
+            plot(lrs, losses.map{$0.scalar}, logScale: true, xLabel:"lr", yLabel:"loss")
+        }
+        
+    }
+    
+    public func makeRecorder() -> Recorder {
+        return Recorder()
+    }
+}
+
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+
+# Utility optional property to get backour `Recorder` if it was created by a utility function. This doesn't always work properly for unkwnon reasons
+
+//TODO: Fix
+extension Learner where Opt.Scalar: PythonConvertible{
+    public var recorder: Learner.Recorder? {
+        for callback in learner.delegates {
+            if let recorder = callback as? Learner.Recorder { return recorder }
+        }
+        return nil
+    }
+}
+
+learner.delegates = [learner.makeTrainEvalDelegate(), learner.makeAvgMetric(metrics: [accuracy]), 
+                     learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std), learner.makeRecorder()]
+
+learner.fit(2)
+
+learner.recorder!.plotLosses()
+
+# ### Progress bar
+
+# It's nice to keep track of where we're at in the training with a progress bar.
+
+// export
+import Foundation
+
+// export
+func formatTime(_ t: Float) -> String {
+    let t = Int(t)
+    let (h,m,s) = (t/3600, (t/60)%60, t%60)
+    return h != 0 ? String(format: "%02d:%02d:%02d", h, m, s) : String(format: "%02d:%02d", m, s)
+}
+
+formatTime(78.23)
+
+// export
+public struct ProgressBar{
+    let total: Int
+    let length: Int = 50
+    let showEvery: Float = 0.2
+    let fillChar: Character = "X"
+    public var comment: String = ""
+    private var waitFor: Int = 0
+    private var startTime: UInt64 = 0
+    private var lastPrint: UInt64 = 0
+    private var lastShow: UInt64 = 0
+    private var estimatedTotal: Float = 0.0
+    private var bar: String = ""
+    
+    public init(_ c: Int) { total = c }
+    
+    public mutating func update(_ val: Int){
+        lastShow = DispatchTime.now().uptimeNanoseconds
+        if val == 0 { startTime = lastShow } 
+        else {
+            let averageTime = Float(lastShow - startTime) / (1e9 * Float(val))
+            estimatedTotal = Float(total) * averageTime
+        }
+        if val == 0 || lastShow - lastPrint >= Int(1e9 * showEvery) { update_bar(val) }
+    }
+    
+    public mutating func update_bar(_ val: Int){
+        lastPrint = lastShow
+        let prevLength = bar.count
+        bar = String(repeating: fillChar, count: (val * length) / total)
+        bar += String(repeating: "-", count: length - (val * length) / total)
+        let pct = String(format: "%.2f", 100.0 * Float(val)/Float(total))
+        let elapsedTime = Float(lastShow - startTime) / 1e9
+        let remaingTime = estimatedTotal - elapsedTime
+        bar += " \(pct)% [\(val)/\(total) \(formatTime(elapsedTime))<\(formatTime(remaingTime))"
+        bar += comment.isEmpty ? "]" : " \(comment)]"
+        if bar.count < prevLength { bar += String(repeating: " ", count: prevLength-bar.count) }
+        print(bar, terminator:"\r")
+        fflush(stdout)
+    }
+    
+    public func remove(){
+        print(String(repeating: " ", count: bar.count), terminator:"\r")
+        fflush(stdout)
+    }
+}
+
+var tst = ProgressBar(100)
+for i in 0...100{
+    tst.update(i)
+    usleep(50000)
+}
+tst.remove()
+
+// export
+extension Learner {
+    public class ShowProgress: Delegate {
+        var pbar: ProgressBar? = nil
+        var iter: Int = 0
+        
+        public override func epochWillStart(learner: Learner) {
+            pbar = ProgressBar(learner.data.train.count)
+        }
+        
+        public override func validationWillStart(learner: Learner) {
+            if pbar != nil { pbar!.remove() }
+            pbar = ProgressBar(learner.data.valid.count)
+        }
+        
+        public override func epochDidFinish(learner: Learner) {
+            if pbar != nil { pbar!.remove() }
+        }
+        
+        public override func batchWillStart(learner: Learner) {
+            if learner.currentIter == 0 {pbar!.update(0)}
+        }
+        
+        public override func batchDidFinish(learner: Learner) {
+            pbar!.update(learner.currentIter)
+        }
+        
+        public override func trainingDidFinish(learner: Learner) {
+            if pbar != nil { pbar!.remove() }
+        }
+    }
+    
+    public func makeShowProgress() -> ShowProgress { return ShowProgress() }
+}
+
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+
+learner.delegates = [learner.makeTrainEvalDelegate(), learner.makeShowProgress(), 
+                     learner.makeAvgMetric(metrics: [accuracy]), learner.makeRecorder(),
+                     learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std)]
+
+learner.fit(2)
+
+# ### Annealing
+
+// export
+/// A non-generalized learning rate scheduler
+extension Learner where Opt.Scalar: BinaryFloatingPoint {
+    public class LRScheduler: Delegate {
+        public override var order: Int { return 1 }
+        public typealias ScheduleFunc = (Float) -> Float
+
+        // A learning rate schedule from step to float.
+        public var scheduler: ScheduleFunc
+        
+        public init(scheduler: @escaping (Float) -> Float) {
+            self.scheduler = scheduler
+        }
+        
+        override public func batchWillStart(learner: Learner) {
+            learner.opt.learningRate = Opt.Scalar(scheduler(learner.pctEpochs/Float(learner.epochCount)))
+        }
+    }
+    
+    public func makeLRScheduler(scheduler: @escaping (Float) -> Float) -> LRScheduler {
+        return LRScheduler(scheduler: scheduler)
+    }
+}
+
+# +
+// export
+public func linearSchedule(start: Float, end: Float, pct: Float) -> Float {
+    return start + pct * (end - start)
+}
+
+public func makeAnnealer(start: Float, end: Float, schedule: @escaping (Float, Float, Float) -> Float) -> (Float) -> Float { 
+    return { pct in return schedule(start, end, pct) }
+}
+# -
+
+let annealer = makeAnnealer(start: 1e-2, end: 0.1, schedule: linearSchedule)
+annealer(0.3)
+
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+let recorder = learner.makeRecorder()
+
+learner.delegates = [learner.makeTrainEvalDelegate(), learner.makeShowProgress(), 
+                     learner.makeAvgMetric(metrics: [accuracy]), recorder,
+                     learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std),
+                     learner.makeLRScheduler(scheduler: annealer)]
+
+learner.fit(2)
+
+recorder.plotLRs()
+
+# More annealing functions
+
+# +
+// export
+public func constantSchedule(start: Float, end: Float, pct: Float) -> Float {
+    return start
+}
+
+public func cosineSchedule(start: Float, end: Float, pct: Float) -> Float {
+    return start + (1 + cos(Float.pi*(1-pct))) * (end-start) / 2
+}
+
+public func expSchedule(start: Float, end: Float, pct: Float) -> Float {
+    return start * pow(end / start, pct)
+}
+# -
+
+// export
+public func combineSchedules(pcts: [Float], schedules: [(Float) -> Float]) -> ((Float) -> Float){
+    var cumPcts: [Float] = [0]
+    for pct in pcts {cumPcts.append(cumPcts.last! + pct)}
+    func inner(pct: Float) -> Float{
+        if (pct == 0.0) { return schedules[0](0.0) }
+        if (pct > 1.0)  { return schedules.last!(1.0) }
+        let i = cumPcts.firstIndex(where: {$0 >= pct})! - 1
+        let actualPos = (pct-cumPcts[i]) / (cumPcts[i+1]-cumPcts[i])
+        return schedules[i](actualPos)
+    }
+    return inner
+}
+
+let mySchedule = combineSchedules(pcts: [0.3, 0.7], 
+                                  schedules: [makeAnnealer(start: 0.3, end: 0.6, schedule: cosineSchedule),
+                                              makeAnnealer(start: 0.6, end: 0.2, schedule: cosineSchedule)])
+
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+let recorder = learner.makeRecorder()
+
+learner.delegates = [learner.makeTrainEvalDelegate(), learner.makeShowProgress(), 
+                     learner.makeAvgMetric(metrics: [accuracy]), recorder,
+                     learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std),
+                     learner.makeLRScheduler(scheduler: mySchedule)]
+
+learner.fit(2)
+
+recorder.plotLRs()
+
+//Needs fixing 
+//learner.recorder!.plotLRs()
+
+# ## Export
+
+import NotebookExport
+let exporter = NotebookExport(Path.cwd/"05_anneal.ipynb")
+print(exporter.export(usingPrefix: "FastaiNotebook_"))
+
+
diff --git a/nbs/swift/05b_early_stopping.ipynb b/nbs/swift/05b_early_stopping.ipynb
index 952ee0e..69b20e3 100644
--- ./nbs/swift/05b_early_stopping.ipynb
+++ ./nbs/swift/05b_early_stopping.ipynb
@@ -567,6 +567,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/05b_early_stopping.py b/nbs/swift/05b_early_stopping.py
new file mode 100644
index 0000000..e53a43d
--- /dev/null
+++ ./nbs/swift/05b_early_stopping.py
@@ -0,0 +1,184 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+# # Early stopping
+
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/FastaiNotebook_05_anneal")' FastaiNotebook_05_anneal
+
+//export
+import Path
+import TensorFlow
+import Python
+
+import FastaiNotebook_05_anneal
+
+%include "EnableIPythonDisplay.swift"
+IPythonDisplay.shell.enable_matplotlib("inline")
+
+# ## Load data
+
+let data = mnistDataBunch(flat: true)
+
+let (n,m) = (60000,784)
+let c = 10
+let nHid = 50
+
+func optFunc(_ model: BasicModel) -> SGD<BasicModel> {return SGD(for: model, learningRate: 1e-2)}
+
+func modelInit() -> BasicModel {return BasicModel(nIn: m, nHid: nHid, nOut: c)}
+
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+let recorder = learner.makeRecorder()
+
+# Check the previous callbacks load.
+
+learner.delegates = [learner.makeTrainEvalDelegate(), learner.makeShowProgress(),
+                     learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std),
+                     learner.makeAvgMetric(metrics: [accuracy]), recorder]
+
+learner.fit(2)
+
+# Make an extension to quickly load them. 
+
+// export
+//TODO: when recorder can be accessed as a property, remove it from the return
+extension Learner where Opt.Scalar: PythonConvertible {
+    public func makeDefaultDelegates(metrics: [(Output, Label) -> TF] = []) -> Recorder {
+        let recorder = makeRecorder()
+        delegates = [makeTrainEvalDelegate(), makeShowProgress(), recorder]
+        if !metrics.isEmpty { delegates.append(makeAvgMetric(metrics: metrics)) }
+        return recorder
+    }
+}
+
+# ## Control Flow test
+
+extension Learner {
+    public class TestControlFlow: Delegate {
+        public override var order: Int { return 3 }
+        
+        var skipAfter,stopAfter: Int
+        public init(skipAfter:Int, stopAfter: Int){  (self.skipAfter,self.stopAfter) = (skipAfter,stopAfter) }
+        
+        public override func batchWillStart(learner: Learner) throws {
+            print("batchWillStart")
+            if learner.currentIter >= stopAfter {
+                throw LearnerAction.stop(reason: "*** stopped: \(learner.currentIter)")
+            }
+            if learner.currentIter >= skipAfter {
+                throw LearnerAction.skipBatch(reason: "*** skipBatch: \(learner.currentIter)")
+            }
+        }
+        
+        public override func trainingDidFinish(learner: Learner) {
+            print("trainingDidFinish")
+        }
+        
+        public override func batchSkipped(learner: Learner, reason: String) {
+            print(reason)
+        }
+    }
+}
+
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+
+learner.delegates = [type(of: learner).TestControlFlow(skipAfter:5, stopAfter: 8),
+                     learner.makeTrainEvalDelegate()]
+
+learner.fit(5)
+
+# Check if the orders were taken into account:
+
+(learner.delegates[0].order,learner.delegates[1].order)
+
+# ### LR Finder
+
+// export
+extension Learner where Opt.Scalar: BinaryFloatingPoint {
+    public class LRFinder: Delegate {
+        public typealias ScheduleFunc = (Float) -> Float
+
+        // A learning rate schedule from step to float.
+        private var scheduler: ScheduleFunc
+        private var numIter: Int
+        private var minLoss: Float? = nil
+        
+        public init(start: Float = 1e-5, end: Float = 10, numIter: Int = 100) {
+            scheduler = makeAnnealer(start: start, end: end, schedule: expSchedule)
+            self.numIter = numIter
+        }
+        
+        override public func batchWillStart(learner: Learner) {
+            learner.opt.learningRate = Opt.Scalar(scheduler(Float(learner.currentIter)/Float(numIter)))
+        }
+        
+        override public func batchDidFinish(learner: Learner) throws {
+            if minLoss == nil {minLoss = learner.currentLoss.scalar}
+            else { 
+                if learner.currentLoss.scalarized() < minLoss! { minLoss = learner.currentLoss.scalarized()}
+                if learner.currentLoss.scalarized() > 4 * minLoss! { 
+                    throw LearnerAction.stop(reason: "Loss diverged")
+                }
+                if learner.currentIter >= numIter { 
+                    throw LearnerAction.stop(reason: "Finished the range.") 
+                }
+            }
+        }
+        
+        override public func validationWillStart(learner: Learner<Label, Opt>) throws {
+            //Skip validation during the LR range test
+            throw LearnerAction.skipEpoch(reason: "No validation in the LR Finder.")
+        }
+    }
+    
+    public func makeLRFinder(start: Float = 1e-5, end: Float = 10, numIter: Int = 100) -> LRFinder {
+        return LRFinder(start: start, end: end, numIter: numIter)
+    }
+}
+
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+let recorder = learner.makeDefaultDelegates()
+
+learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))
+learner.delegates.append(learner.makeLRFinder())
+
+learner.fit(2)
+
+recorder.plotLRFinder()
+
+// export
+//TODO: when Recorder is a property of Learner don't return it.
+extension Learner where Opt.Scalar: PythonConvertible & BinaryFloatingPoint {
+    public func lrFind(start: Float = 1e-5, end: Float = 10, numIter: Int = 100) -> Recorder {
+        let epochCount = data.train.count/numIter + 1
+        let recorder = makeDefaultDelegates()
+        delegates.append(makeLRFinder(start: start, end: end, numIter: numIter))
+        try! self.fit(epochCount)
+        return recorder
+    }
+}
+
+let recorder = learner.lrFind()
+
+recorder.plotLRFinder()
+
+# ## Export
+
+import NotebookExport
+let exporter = NotebookExport(Path.cwd/"05b_early_stopping.ipynb")
+print(exporter.export(usingPrefix: "FastaiNotebook_"))
+
+
diff --git a/nbs/swift/06_cuda.ipynb b/nbs/swift/06_cuda.ipynb
index 0401127..11cc2e8 100644
--- ./nbs/swift/06_cuda.ipynb
+++ ./nbs/swift/06_cuda.ipynb
@@ -513,6 +513,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/06_cuda.py b/nbs/swift/06_cuda.py
new file mode 100644
index 0000000..4da6348
--- /dev/null
+++ ./nbs/swift/06_cuda.py
@@ -0,0 +1,155 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+# # A CNN Mnist Model
+
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/FastaiNotebook_05b_early_stopping")' FastaiNotebook_05b_early_stopping
+
+//export
+import Path
+import TensorFlow
+import Python
+
+import FastaiNotebook_05b_early_stopping
+
+%include "EnableIPythonDisplay.swift"
+IPythonDisplay.shell.enable_matplotlib("inline")
+
+# ## Load data
+
+let data = mnistDataBunch(flat: false, bs: 512)
+
+# +
+let firstBatch = data.train.ds.first(where: { _ in true })!
+let batchShape = firstBatch.xb.shape
+let batchSize = batchShape.dimensions[0]
+let exampleSideSize = batchShape.dimensions[1]
+assert(exampleSideSize == batchShape.dimensions[2])
+print("Batch size: \(batchSize)")
+print("Example side size: \(exampleSideSize)")
+
+let classCount = firstBatch.yb.shape.dimensions[0]
+print("Class count: \(classCount)")
+# -
+
+firstBatch.xb.shape
+
+// export
+extension Learner {
+    public class AddChannel: Delegate {
+        public override func batchWillStart(learner: Learner) {
+            learner.currentInput = learner.currentInput!.expandingShape(at: -1)
+        }
+    }
+    
+    public func makeAddChannel() -> AddChannel { return AddChannel() }
+}
+
+//export 
+public struct CnnModel: Layer {
+    public var convs: [FAConv2D<Float>]
+    public var pool = FAGlobalAvgPool2D<Float>()
+    public var linear: FADense<Float>
+    
+    public init(channelIn: Int, nOut: Int, filters: [Int]){
+        let allFilters = [channelIn] + filters
+        convs = Array(0..<filters.count).map { i in
+            return FAConv2D(allFilters[i], allFilters[i+1], ks: 3, stride: 2)
+        }
+        linear = FADense<Float>(filters.last!, nOut)
+    }
+    
+    @differentiable
+    public func callAsFunction(_ input: TF) -> TF {
+        return linear(pool(convs(input)))
+    }
+}
+
+let model = CnnModel(channelIn: 1, nOut: 10, filters: [8, 16, 32, 32])
+
+// Test that data goes through the model as expected.
+let predictions = model(firstBatch.xb.expandingShape(at: -1))
+print(predictions.shape)
+print(predictions[0])
+
+# # Compare training on CPU and GPU
+
+func optFunc(_ model: CnnModel) -> SGD<CnnModel> { return SGD(for: model, learningRate: 0.4)}
+func modelInit() -> CnnModel { return CnnModel(channelIn: 1, nOut: 10, filters: [8, 16, 32, 32]) }
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+let recorder = learner.makeDefaultDelegates(metrics: [accuracy])
+learner.addDelegates([learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std),
+                      learner.makeAddChannel()])
+
+// This happens on the GPU (if you have one and it's configured correctly).
+// I tried this on a GCE 8vCPU 30GB + Tesla P100:
+// - time: ~4.3s
+// - nvidia-smi shows ~10% GPU-Util while this is running
+time { try! learner.fit(1) }
+
+// This happens on the CPU.
+// I tried this on a GCE 8vCPU 30GB + Tesla P100:
+// - time: ~6.3s
+// - nvidia-smi shows 0% GPU-Util while this is running
+time {
+    withDevice(.cpu) { try! learner.fit(1) }
+}
+
+
+# # Collect Layer Activation Statistics
+
+class ActivationStatsHook {
+    var means: [Float] = []
+    var stds: [Float] = []    
+    func update(_ act: TF) {
+        means.append(act.mean().scalarized())
+        stds.append (act.std() .scalarized())
+    }
+}
+
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+let recorder = learner.makeDefaultDelegates(metrics: [accuracy])
+learner.addDelegates([learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std),
+                      learner.makeAddChannel()])
+
+var statHooks: [ActivationStatsHook] = (0..<learner.model.convs.count).map { i in 
+    var stat = ActivationStatsHook()
+    learner.model.convs[i].addDelegate(stat.update)
+    return stat
+}
+
+// This LayerDelegate stuff slows it down to ~6s/epoch.
+time { try! learner.fit(2) }
+
+for stat in statHooks {
+    plt.plot(stat.means)
+}
+plt.legend(Array(1...statHooks.count))
+plt.show()
+
+for stat in statHooks {
+    plt.plot(stat.stds)
+}
+plt.legend(Array(1...statHooks.count))
+plt.show()
+
+# ## Export
+
+import NotebookExport
+let exporter = NotebookExport(Path.cwd/"06_cuda.ipynb")
+print(exporter.export(usingPrefix: "FastaiNotebook_"))
+
+
diff --git a/nbs/swift/07_batchnorm.ipynb b/nbs/swift/07_batchnorm.ipynb
index 488d5ab..d656473 100644
--- ./nbs/swift/07_batchnorm.ipynb
+++ ./nbs/swift/07_batchnorm.ipynb
@@ -893,6 +893,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/07_batchnorm.py b/nbs/swift/07_batchnorm.py
new file mode 100644
index 0000000..03b33bc
--- /dev/null
+++ ./nbs/swift/07_batchnorm.py
@@ -0,0 +1,550 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/FastaiNotebook_06_cuda")' FastaiNotebook_06_cuda
+
+//export
+import Path
+import TensorFlow
+import Python
+
+import FastaiNotebook_06_cuda
+
+%include "EnableIPythonDisplay.swift"
+IPythonDisplay.shell.enable_matplotlib("inline")
+
+# ## Load data
+
+let data = mnistDataBunch(flat: false, bs: 512)
+
+func optFunc(_ model: CnnModel) -> SGD<CnnModel> { return SGD(for: model, learningRate: 0.4) }
+func modelInit() -> CnnModel { return CnnModel(channelIn: 1, nOut: 10, filters: [8, 16, 32, 32]) }
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+let recorder = learner.makeDefaultDelegates(metrics: [accuracy])
+learner.addDelegates([learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std),
+                      learner.makeAddChannel()])
+
+time { try! learner.fit(1) }
+
+# ## Batchnorm
+
+# ### Custom
+
+# Let's start by building our own `BatchNorm` layer from scratch. Eventually we intend for this code to do the trick:
+
+struct AlmostBatchNorm<Scalar: TensorFlowFloatingPoint>: Differentiable {
+    // Configuration hyperparameters
+    @noDerivative let momentum, epsilon: Scalar
+    // Running statistics
+    @noDerivative var runningMean, runningVariance: Tensor<Scalar>
+    // Trainable parameters
+    var scale, offset: Tensor<Scalar>
+    
+    init(featureCount: Int, momentum: Scalar = 0.9, epsilon: Scalar = 1e-5) {
+        self.momentum = momentum
+        self.epsilon = epsilon
+        self.scale = Tensor(ones: [featureCount])
+        self.offset = Tensor(zeros: [featureCount])
+        self.runningMean = Tensor(0)
+        self.runningVariance = Tensor(1)
+    }
+
+    mutating func callAsFunction(_ input: Tensor<Scalar>) -> Tensor<Scalar> {
+        let mean: Tensor<Scalar>
+        let variance: Tensor<Scalar>
+        switch Context.local.learningPhase {
+        case .training:
+            mean = input.mean(alongAxes: [0, 1, 2])
+            variance = input.variance(alongAxes: [0, 1, 2])
+            runningMean += (mean - runningMean) * (1 - momentum)
+            runningVariance += (variance - runningVariance) * (1 - momentum)
+        case .inference:
+            mean = runningMean
+            variance = runningVariance
+        }
+        let normalizer = rsqrt(variance + epsilon) * scale
+        return (input - mean) * normalizer + offset
+    }
+}
+
+# But there are some automatic differentiation limitations (control flow support) and `Layer` protocol constraints (mutating `call`) that make this impossible for now (note the lack of `@differentiable` or a `Layer` conformance), so we'll need a few workarounds. A `Reference` will let us update running statistics without declaring the `applied` method `mutating`:
+
+//export
+public class Reference<T> {
+    public var value: T
+    public init(_ value: T) { self.value = value }
+}
+
+# The following snippet will let us differentiate a layer's `call` method if it's composed of training and inference implementations that are each differentiable:
+
+# +
+//export
+public protocol LearningPhaseDependent: FALayer {
+    associatedtype Input
+    associatedtype Output
+    
+    @differentiable func forwardTraining(_ input: Input) -> Output
+    @differentiable func forwardInference(_ input: Input) -> Output
+}
+
+extension LearningPhaseDependent {
+    // This `@differentiable` attribute is necessary, to tell the compiler that this satisfies the FALayer
+    // protocol requirement, even though there is a `@differentiating(forward)` method below.
+    // TODO: It seems nondeterministically necessary. Some subsequent notebooks import this successfully without it,
+    // some require it. Investigate.
+    @differentiable
+    public func forward(_ input: Input) -> Output {
+        switch Context.local.learningPhase {
+        case .training:  return forwardTraining(input)
+        case .inference: return forwardInference(input)
+        }
+    }
+
+    @differentiating(forward)
+    func gradForward(_ input: Input) ->
+        (value: Output, pullback: (Self.Output.TangentVector) ->
+            (Self.TangentVector, Self.Input.TangentVector)) {
+        switch Context.local.learningPhase {
+        case .training:
+            return valueWithPullback(at: input) { $0.forwardTraining ($1) }
+        case .inference:
+            return valueWithPullback(at: input) { $0.forwardInference($1) }
+        }
+    }
+}
+# -
+
+# Now we can implement a BatchNorm that we can use in our models:
+
+# +
+//export
+public protocol Norm: Layer where Input == Tensor<Scalar>, Output == Tensor<Scalar>{
+    associatedtype Scalar
+    init(featureCount: Int, epsilon: Scalar)
+}
+
+public struct FABatchNorm<Scalar: TensorFlowFloatingPoint>: LearningPhaseDependent, Norm {
+    // TF-603 workaround.
+    public typealias Input = Tensor<Scalar>
+    public typealias Output = Tensor<Scalar>
+    @noDerivative public var delegates: [(Self.Output) -> ()] = []
+    
+    // Configuration hyperparameters
+    @noDerivative var momentum, epsilon: Scalar
+    // Running statistics
+    @noDerivative let runningMean, runningVariance: Reference<Tensor<Scalar>>
+    // Trainable parameters
+    public var scale, offset: Tensor<Scalar>
+    
+    public init(featureCount: Int, momentum: Scalar, epsilon: Scalar = 1e-5) {
+        self.momentum = momentum
+        self.epsilon = epsilon
+        self.scale = Tensor(ones: [featureCount])
+        self.offset = Tensor(zeros: [featureCount])
+        self.runningMean = Reference(Tensor(0))
+        self.runningVariance = Reference(Tensor(1))
+    }
+    
+    public init(featureCount: Int, epsilon: Scalar = 1e-5) {
+        self.init(featureCount: featureCount, momentum: 0.9, epsilon: epsilon)
+    }
+
+    @differentiable
+    public func forwardTraining(_ input: Tensor<Scalar>) -> Tensor<Scalar> {
+        let mean = input.mean(alongAxes: [0, 1, 2])
+        let variance = input.variance(alongAxes: [0, 1, 2])
+        runningMean.value += (mean - runningMean.value) * (1 - momentum)
+        runningVariance.value += (variance - runningVariance.value) * (1 - momentum)
+        let normalizer = rsqrt(variance + epsilon) * scale
+        return (input - mean) * normalizer + offset
+    }
+    
+    @differentiable
+    public func forwardInference(_ input: Tensor<Scalar>) -> Tensor<Scalar> {
+        let mean = runningMean.value
+        let variance = runningVariance.value
+        let normalizer = rsqrt(variance + epsilon) * scale
+        return (input - mean) * normalizer + offset
+    }
+}
+# -
+
+# TensorFlow provides a highly optimized batch norm implementation, let us redefine our batch norm to invoke it directly. 
+
+# +
+//export
+struct BatchNormResult<Scalar : TensorFlowFloatingPoint> : Differentiable{
+    var y, batchMean, batchVariance, reserveSpace1, reserveSpace2: Tensor<Scalar>
+}
+
+public struct TFBatchNorm<Scalar: TensorFlowFloatingPoint>: LearningPhaseDependent, Norm {
+    // Configuration hyperparameters
+    @noDerivative var momentum, epsilon: Scalar
+    // Running statistics
+    @noDerivative let runningMean, runningVariance: Reference<Tensor<Scalar>>
+    // Trainable parameters
+    public var scale, offset: Tensor<Scalar>
+    @noDerivative public var delegates: [(Self.Output) -> ()] = []
+    
+    public init(featureCount: Int, momentum: Scalar, epsilon: Scalar = 1e-5) {
+        self.momentum = momentum
+        self.epsilon = epsilon
+        self.scale = Tensor(ones: [featureCount])
+        self.offset = Tensor(zeros: [featureCount])
+        self.runningMean = Reference(Tensor(0))
+        self.runningVariance = Reference(Tensor(1))
+    }
+    
+    public init(featureCount: Int, epsilon: Scalar = 1e-5) {
+        self.init(featureCount: featureCount, momentum: 0.9, epsilon: epsilon)
+    }
+
+    @differentiable
+    public func forwardTraining(_ input: Tensor<Scalar>) -> Tensor<Scalar> {
+        let res = TFBatchNorm<Scalar>.fusedBatchNorm(
+            input, scale: scale, offset: offset, epsilon: epsilon)
+        let (output, mean, variance) = (res.y, res.batchMean, res.batchVariance)
+        runningMean.value += (mean - runningMean.value) * (1 - momentum)
+        runningVariance.value += (variance - runningVariance.value) * (1 - momentum)
+        return output
+     }
+    
+    @differentiable
+    public func forwardInference(_ input: Tensor<Scalar>) -> Tensor<Scalar> {
+        let mean = runningMean.value
+        let variance = runningVariance.value
+        let normalizer = rsqrt(variance + epsilon) * scale
+        return (input - mean) * normalizer + offset
+    }
+    
+    @differentiable(wrt: (x, scale, offset), vjp: _vjpFusedBatchNorm)
+    static func fusedBatchNorm(
+        _ x : Tensor<Scalar>, scale: Tensor<Scalar>, offset: Tensor<Scalar>, epsilon: Scalar
+    ) -> BatchNormResult<Scalar> {
+        let ret = Raw.fusedBatchNormV2(
+            x, scale: scale, offset: offset, 
+            mean: Tensor<Scalar>([] as [Scalar]), variance: Tensor<Scalar>([] as [Scalar]), 
+            epsilon: Double(epsilon))
+        return BatchNormResult(
+            y: ret.y, batchMean: ret.batchMean, batchVariance: ret.batchVariance,
+            reserveSpace1: ret.reserveSpace1, reserveSpace2: ret.reserveSpace2
+        )
+    }
+
+    static func _vjpFusedBatchNorm(
+        _ x : Tensor<Scalar>, scale: Tensor<Scalar>, offset: Tensor<Scalar>, epsilon: Scalar
+    ) -> (BatchNormResult<Scalar>, 
+          (BatchNormResult<Scalar>.TangentVector) -> (Tensor<Scalar>.TangentVector, 
+                                                        Tensor<Scalar>.TangentVector, 
+                                                        Tensor<Scalar>.TangentVector)) {
+      let bnresult = fusedBatchNorm(x, scale: scale, offset: offset, epsilon: epsilon)
+  
+        return (
+            bnresult, 
+            {v in 
+                let res = Raw.fusedBatchNormGradV2(
+                    yBackprop: v.y, x, scale: Tensor<Float>(scale), 
+                    reserveSpace1: bnresult.reserveSpace1, 
+                    reserveSpace2: bnresult.reserveSpace2, 
+                    epsilon: Double(epsilon))
+                return (res.xBackprop, res.scaleBackprop, res.offsetBackprop)
+            })
+    }
+}
+# -
+
+//export
+public struct ConvBN<Scalar: TensorFlowFloatingPoint>: FALayer {
+    // TF-603 workaround.
+    public typealias Input = Tensor<Scalar>
+    public typealias Output = Tensor<Scalar>
+    @noDerivative public var delegates: [(Self.Output) -> ()] = []
+    public var conv: FANoBiasConv2D<Scalar>
+    public var norm: FABatchNorm<Scalar>
+    
+    public init(_ cIn: Int, _ cOut: Int, ks: Int = 3, stride: Int = 1){
+        // TODO (when control flow AD works): use Conv2D without bias
+        self.conv = FANoBiasConv2D(cIn, cOut, ks: ks, stride: stride, activation: relu)
+        self.norm = FABatchNorm(featureCount: cOut, epsilon: 1e-5)
+    }
+
+    @differentiable
+    public func forward(_ input: Tensor<Scalar>) -> Tensor<Scalar> {
+        return norm.forward(conv.forward(input))
+    }
+}
+
+# +
+// Would be great if this generic could work
+// struct ConvNorm<NormType: Norm, Scalar: TensorFlowFloatingPoint>: Layer
+//     where NormType.Scalar == Scalar {
+//     var conv: Conv2D<Scalar>
+//     var norm: NormType
+//     init(
+//         filterShape: (Int, Int, Int, Int),
+//         strides: (Int, Int) = (1, 1),
+//         padding: Padding = .valid,
+//         activation: @escaping Conv2D<Scalar>.Activation = identity
+//     ) {
+//         // TODO (when control flow AD works): use Conv2D without bias
+//         self.conv = Conv2D(
+//             filterShape: filterShape,
+//             strides: strides,
+//             padding: padding,
+//             activation: activation)
+//         self.norm = NormType.init(featureCount: filterShape.3, epsilon: 1e-5)
+//     }
+
+//     @differentiable
+//     func applied(to input: Tensor<Scalar>) -> Tensor<Scalar> {
+//         return norm.applied(to: conv.applied(to: input))
+//     }
+// }
+//typealias ConvBN = ConvNorm<BatchNorm<Float>, Float>
+# -
+
+//export
+public struct CnnModelBN: Layer {
+    public var convs: [ConvBN<Float>]
+    public var pool = FAGlobalAvgPool2D<Float>()
+    public var linear: FADense<Float>
+    @noDerivative public var delegates: [(Self.Output) -> ()] = []
+    
+    public init(channelIn: Int, nOut: Int, filters: [Int]){
+        let allFilters = [channelIn] + filters
+        convs = Array(0..<filters.count).map { i in
+            return ConvBN(allFilters[i], allFilters[i+1], ks: 3, stride: 2)
+        }
+        linear = FADense<Float>(filters.last!, nOut)
+    }
+    
+    @differentiable
+    public func callAsFunction(_ input: TF) -> TF {
+        // TODO: Work around https://bugs.swift.org/browse/TF-606
+        return linear.forward(pool.forward(convs(input)))
+    }
+}
+
+func optFunc(_ model: CnnModelBN) -> SGD<CnnModelBN> { return SGD(for: model, learningRate: 0.4) }
+func modelInit() -> CnnModelBN { return CnnModelBN(channelIn: 1, nOut: 10, filters: [8, 16, 32, 32]) }
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+let recorder = learner.makeDefaultDelegates(metrics: [accuracy])
+learner.addDelegates([learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std),
+                      learner.makeAddChannel()])
+
+time { try! learner.fit(1) }
+
+# ## More norms
+
+# ### Layer norm
+
+# From [the paper](https://arxiv.org/abs/1607.06450): "*batch normalization cannot be applied to online learning tasks or to extremely large distributed models where the minibatches have to be small*".
+
+# General equation for a norm layer with learnable affine:
+#
+# $$y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta$$
+#
+# The difference with BatchNorm is
+# 1. we don't keep a moving average
+# 2. we don't average over the batches dimension but over the hidden dimension, so it's independent of the batch size
+
+# +
+struct LayerNorm2D<Scalar: TensorFlowFloatingPoint>: Norm {
+    @noDerivative public var delegates: [(Self.Output) -> ()] = []
+    // Configuration hyperparameters
+    @noDerivative let epsilon: Scalar
+    // Trainable parameters
+    var scale: Tensor<Scalar>
+    var offset: Tensor<Scalar>
+    
+    init(featureCount: Int, epsilon: Scalar = 1e-5) {
+        self.epsilon = epsilon
+        self.scale = Tensor(ones: [featureCount])
+        self.offset = Tensor(zeros: [featureCount])
+    }
+    
+    @differentiable
+    func callAsFunction(_ input: Tensor<Scalar>) -> Tensor<Scalar> {
+        let mean = input.mean(alongAxes: [1, 2, 3])
+        let variance = input.variance(alongAxes: [1, 2, 3])
+        let normalizer = rsqrt(variance + epsilon) * scale
+        return (input - mean) * normalizer + offset
+    }
+}
+
+struct ConvLN<Scalar: TensorFlowFloatingPoint>: FALayer {
+    @noDerivative public var delegates: [(Self.Output) -> ()] = []
+    var conv: FANoBiasConv2D<Scalar>
+    var norm: LayerNorm2D<Scalar>
+    
+    init(_ cIn: Int, _ cOut: Int, ks: Int = 3, stride: Int = 2){
+        // TODO (when control flow AD works): use Conv2D without bias
+        self.conv = FANoBiasConv2D(cIn, cOut, ks: ks, stride: stride, activation: relu)
+        self.norm = LayerNorm2D(featureCount: cOut, epsilon: 1e-5)
+    }
+
+    @differentiable
+    func forward(_ input: Tensor<Scalar>) -> Tensor<Scalar> {
+        return norm.callAsFunction(conv.forward(input))
+    }
+}
+# -
+
+public struct CnnModelLN: Layer {
+    public var convs: [ConvLN<Float>]
+    public var pool = FAGlobalAvgPool2D<Float>()
+    public var linear: FADense<Float>
+    
+    public init(channelIn: Int, nOut: Int, filters: [Int]){
+        let allFilters = [channelIn] + filters
+        convs = Array(0..<filters.count).map { i in
+            return ConvLN(allFilters[i], allFilters[i+1], ks: 3, stride: 2)
+        }
+        linear = FADense<Float>(filters.last!, nOut)
+    }
+    
+    @differentiable
+    public func callAsFunction(_ input: TF) -> TF {
+        // TODO: Work around https://bugs.swift.org/browse/TF-606
+        return linear.forward(pool.forward(convs(input)))
+    }
+}
+
+
+# +
+struct InstanceNorm<Scalar: TensorFlowFloatingPoint>: Norm {
+    @noDerivative public var delegates: [(Self.Output) -> ()] = []
+    // Configuration hyperparameters
+    @noDerivative let epsilon: Scalar
+    // Trainable parameters
+    var scale: Tensor<Scalar>
+    var offset: Tensor<Scalar>
+    
+    init(featureCount: Int, epsilon: Scalar = 1e-5) {
+        self.epsilon = epsilon
+        self.scale = Tensor(ones: [featureCount])
+        self.offset = Tensor(zeros: [featureCount])
+    }
+    
+    @differentiable
+    func callAsFunction(_ input: Tensor<Scalar>) -> Tensor<Scalar> {
+        let mean = input.mean(alongAxes: [2, 3])
+        let variance = input.variance(alongAxes: [2, 3])
+        let normalizer = rsqrt(variance + epsilon) * scale
+        return (input - mean) * normalizer + offset
+    }
+}
+
+struct ConvIN<Scalar: TensorFlowFloatingPoint>: FALayer {
+    @noDerivative public var delegates: [(Self.Output) -> ()] = []
+    var conv: FANoBiasConv2D<Scalar>
+    var norm: InstanceNorm<Scalar>
+    
+    init(_ cIn: Int, _ cOut: Int, ks: Int = 3, stride: Int = 2){
+        // TODO (when control flow AD works): use Conv2D without bias
+        self.conv = FANoBiasConv2D(cIn, cOut, ks: ks, stride: stride, activation: relu)
+        self.norm = InstanceNorm(featureCount: cOut, epsilon: 1e-5)
+    }
+
+    @differentiable
+    func forward(_ input: Tensor<Scalar>) -> Tensor<Scalar> {
+        return norm.callAsFunction(conv.forward(input))
+    }
+}
+# -
+
+# Lost in all those norms? The authors from the [group norm paper](https://arxiv.org/pdf/1803.08494.pdf) have you covered:
+#
+# ![Various norms](../dev_course/dl2/images/norms.png)
+
+# TODO/skipping GroupNorm
+
+# ### Running Batch Norm
+
+struct RunningBatchNorm<Scalar: TensorFlowFloatingPoint>: LearningPhaseDependent, Norm {
+    @noDerivative public var delegates: [(Self.Output) -> ()] = []
+    // Configuration hyperparameters
+    @noDerivative let momentum: Scalar
+    @noDerivative let epsilon: Scalar
+    // Running statistics
+    @noDerivative let runningSum: Reference<Tensor<Scalar>>
+    @noDerivative let runningSumOfSquares: Reference<Tensor<Scalar>>
+    @noDerivative let runningCount: Reference<Scalar>
+    @noDerivative let samplesSeen: Reference<Int>
+    // Trainable parameters
+    var scale: Tensor<Scalar>
+    var offset: Tensor<Scalar>
+    
+    init(featureCount: Int, momentum: Scalar, epsilon: Scalar = 1e-5) {
+        self.momentum = momentum
+        self.epsilon = epsilon
+        self.scale = Tensor(ones: [featureCount])
+        self.offset = Tensor(zeros: [featureCount])
+        self.runningSum = Reference(Tensor(0))
+        self.runningSumOfSquares = Reference(Tensor(0))
+        self.runningCount = Reference(Scalar(0))
+        self.samplesSeen = Reference(0)
+    }
+    
+    init(featureCount: Int, epsilon: Scalar = 1e-5) {
+        self.init(featureCount: featureCount, momentum: 0.9, epsilon: epsilon)
+    }
+
+    @differentiable
+    func forwardTraining(_ input: Tensor<Scalar>) -> Tensor<Scalar> {
+        let (batch, channels) = (input.shape[0], Scalar(input.shape[3]))
+        let sum = input.sum(alongAxes: [0, 1, 2])
+        let sumOfSquares = (input * input).sum(alongAxes: [0, 1, 2])
+        // TODO: Work around https://bugs.swift.org/browse/TF-607
+        let count = withoutDerivative(at: Scalar(input.scalarCount)) { tmp in tmp } / channels
+        let mom = momentum / sqrt(Scalar(batch) - 1)
+        let runningSum = mom * self.runningSum.value + (1 - mom) * sum
+        let runningSumOfSquares = mom * self.runningSumOfSquares.value + (
+            1 - mom) * sumOfSquares
+        let runningCount = mom * self.runningCount.value + (1 - mom) * count
+        
+        self.runningSum.value = runningSum
+        self.runningSumOfSquares.value = runningSumOfSquares
+        self.runningCount.value = runningCount
+        self.samplesSeen.value += batch
+        
+        let mean = runningSum / runningCount
+        let variance = runningSumOfSquares / runningCount - mean * mean
+        
+        let normalizer = rsqrt(variance + epsilon) * scale
+        return (input - mean) * normalizer + offset
+    }
+    
+    @differentiable
+    func forwardInference(_ input: Tensor<Scalar>) -> Tensor<Scalar> {
+        let mean = runningSum.value / runningCount.value
+        let variance = runningSumOfSquares.value / runningCount.value - mean * mean
+        let normalizer = rsqrt(variance + epsilon) * scale
+        return (input - mean) * normalizer + offset
+    }
+}
+
+# TODO: XLA compilation + test RBN
+
+# ## Export
+
+import NotebookExport
+let exporter = NotebookExport(Path.cwd/"07_batchnorm.ipynb")
+print(exporter.export(usingPrefix: "FastaiNotebook_"))
+
+
+
+
diff --git a/nbs/swift/07b_batchnorm_lesson.ipynb b/nbs/swift/07b_batchnorm_lesson.ipynb
index 8f49d38..bfe61b8 100644
--- ./nbs/swift/07b_batchnorm_lesson.ipynb
+++ ./nbs/swift/07b_batchnorm_lesson.ipynb
@@ -422,6 +422,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/07b_batchnorm_lesson.py b/nbs/swift/07b_batchnorm_lesson.py
new file mode 100644
index 0000000..ea47b97
--- /dev/null
+++ ./nbs/swift/07b_batchnorm_lesson.py
@@ -0,0 +1,298 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+%install '.package(path: "$cwd/FastaiNotebook_06_cuda")' FastaiNotebook_06_cuda
+
+import FastaiNotebook_06_cuda
+%include "EnableIPythonDisplay.swift"
+IPythonDisplay.shell.enable_matplotlib("inline")
+
+//export
+import Path
+import TensorFlow
+import Python
+
+
+# Let's start by building our own batchnorm layer from scratch. Eventually we want something like this to work:
+
+class AlmostBatchNorm<Scalar: TensorFlowFloatingPoint> { // : Layer
+    // Configuration hyperparameters
+    let momentum, epsilon: Scalar
+    // Running statistics
+    var runningMean, runningVariance: Tensor<Scalar>
+    // Trainable parameters
+    var scale, offset: Tensor<Scalar>
+    
+    init(featureCount: Int, momentum: Scalar = 0.9, epsilon: Scalar = 1e-5) {
+        (self.momentum, self.epsilon) = (momentum, epsilon)
+        (scale, offset) = (Tensor(ones: [featureCount]), Tensor(zeros: [featureCount]))
+        (runningMean, runningVariance) = (Tensor(0), Tensor(1))
+    }
+
+    func call(_ input: Tensor<Scalar>) -> Tensor<Scalar> {
+        let mean, variance: Tensor<Scalar>
+        switch Context.local.learningPhase {
+        case .training:
+            mean = input.mean(alongAxes: [0, 1, 2])
+            variance = input.variance(alongAxes: [0, 1, 2])
+            runningMean += (mean - runningMean) * (1 - momentum)
+            runningVariance += (variance - runningVariance) * (1 - momentum)
+        case .inference:
+            (mean, variance) = (runningMean, runningVariance)
+        }
+        let normalizer = rsqrt(variance + epsilon) * scale
+        return (input - mean) * normalizer + offset
+    }
+}
+
+# But there are some automatic differentiation limitations (lack of support for classes and control flow) that make this impossible for now, so we'll need a few workarounds. A `Reference` will let us update running statistics without making the layer a class or declaring the `applied` method `mutating`:
+
+//export
+public class Reference<T> {
+    public var value: T
+    public init(_ value: T) { self.value = value }
+}
+
+# The following snippet will let us differentiate a layer's `forward` method (which is the one called in `call` for `FALayer`) if it's composed of training and inference implementations that are each differentiable:
+
+# +
+//export
+public protocol LearningPhaseDependent: FALayer {
+    associatedtype Input
+    associatedtype Output
+    @differentiable func forwardTraining (_ input: Input) -> Output
+    @differentiable func forwardInference(_ input: Input) -> Output
+}
+
+extension LearningPhaseDependent {
+    public func forward(_ input: Input) -> Output {
+        switch Context.local.learningPhase {
+        case .training:  return forwardTraining(input)
+        case .inference: return forwardInference(input)
+        }
+    }
+
+    @differentiating(forward)
+    func gradForward(_ input: Input) ->
+        (value: Output, pullback: (Self.Output.TangentVector) ->
+            (Self.TangentVector, Self.Input.TangentVector)) {
+        switch Context.local.learningPhase {
+        case .training:  return valueWithPullback(at: input) { $0.forwardTraining($1)  }
+        case .inference: return valueWithPullback(at: input) { $0.forwardInference($1) }
+        }
+    }
+}
+# -
+
+# Now we can implement a BatchNorm that we can use in our models:
+
+# +
+//export
+public protocol Norm: FALayer where Input == TF, Output == TF {
+    init(_ featureCount: Int, epsilon: Float)
+}
+
+public struct FABatchNorm: LearningPhaseDependent, Norm {
+    // Configuration hyperparameters
+    @noDerivative var momentum, epsilon: Float
+    // Running statistics
+    @noDerivative let runningMean, runningVariance: Reference<TF>
+    // Trainable parameters
+    public var scale, offset: TF
+    
+    public init(_ featureCount: Int, momentum: Float, epsilon: Float = 1e-5) {
+        self.momentum = momentum
+        self.epsilon = epsilon
+        self.scale = Tensor(ones: [featureCount])
+        self.offset = Tensor(zeros: [featureCount])
+        self.runningMean = Reference(Tensor(0))
+        self.runningVariance = Reference(Tensor(1))
+    }
+    
+    public init(_ featureCount: Int, epsilon: Float = 1e-5) {
+        self.init(featureCount, momentum: 0.9, epsilon: epsilon)
+    }
+
+    @differentiable
+    public func forwardTraining(_ input: TF) -> TF {
+        let mean = input.mean(alongAxes: [0, 1, 2])
+        let variance = input.variance(alongAxes: [0, 1, 2])
+        runningMean.value += (mean - runningMean.value) * (1 - momentum)
+        runningVariance.value += (variance - runningVariance.value) * (1 - momentum)
+        let normalizer = rsqrt(variance + epsilon) * scale
+        return (input - mean) * normalizer + offset
+    }
+    
+    @differentiable
+    public func forwardInference(_ input: TF) -> TF {
+        let (mean, variance) = (runningMean.value, runningVariance.value)
+        let normalizer = rsqrt(variance + epsilon) * scale
+        return (input - mean) * normalizer + offset
+    }
+}
+# -
+
+# Here is a generic `ConvNorm` layer, that combines a conv2d and a norm (like batchnorm, running batchnorm etc...) layer.
+
+//export
+public struct ConvNorm<NormType: Norm & FALayer>: FALayer
+    where NormType.AllDifferentiableVariables == NormType.TangentVector {
+    public var conv: FANoBiasConv2D<Float>
+    public var norm: NormType
+    
+    public init(_ cIn: Int, _ cOut: Int, ks: Int = 3, stride: Int = 2){
+        self.conv = FANoBiasConv2D(cIn, cOut, ks: ks, stride: stride, activation: relu) 
+        self.norm = NormType(cOut, epsilon: 1e-5)
+    }
+
+    @differentiable
+    public func forward(_ input: Tensor<Float>) -> Tensor<Float> {
+        return norm(conv(input))
+    }
+}
+
+//export
+public struct CnnModelNormed<NormType: Norm & FALayer>: FALayer
+    where NormType.AllDifferentiableVariables == NormType.TangentVector {
+    public var convs: [ConvNorm<NormType>]
+    public var pool = FAGlobalAvgPool2D<Float>()
+    public var linear: FADense<Float>
+    
+    public init(channelIn: Int, nOut: Int, filters: [Int]){
+        let allFilters = [channelIn] + filters
+        convs = Array(0..<filters.count).map { i in
+            return ConvNorm<NormType>(allFilters[i], allFilters[i+1], ks: 3, stride: 2)
+        }
+        linear = FADense<Float>(filters.last!, nOut)
+    }
+    
+    @differentiable
+    public func forward(_ input: TF) -> TF {
+        // TODO: Work around https://bugs.swift.org/browse/TF-606
+        return linear.forward(pool.forward(convs(input)))
+    }
+}
+
+# Let's benchmark this batchnorm implementation!
+
+func benchmark(forward: () -> (), backward: () -> ()) {
+    print("forward:")
+    time(repeating: 10, forward)
+    print("backward:")
+    time(repeating: 10, backward)
+}
+
+let input = TF(randomUniform: [64, 28, 28, 32])
+let norm = FABatchNorm(32)
+let pb = pullback(at: input) { x in norm(x) }
+benchmark(forward: { norm(input) }, backward: { pb(input) })
+
+# Yikes, that's pretty bad. Luckily, TensorFlow has a built-in fused batchnorm layer. Let's see how the performance looks for that:
+
+let input = TF(randomUniform: [64, 28, 28, 32])
+let norm = FABatchNorm(32)
+let bnresult = Raw.fusedBatchNormV2(
+    input, scale: norm.scale, offset: norm.offset, 
+    mean: TF([] as [Float]), variance: TF([] as [Float]), 
+    epsilon: Double(norm.epsilon))
+benchmark(
+    forward: {
+        Raw.fusedBatchNormV2(
+            input, scale: norm.scale, offset: norm.offset, 
+            mean: TF([] as [Float]), variance: TF([] as [Float]), 
+            epsilon: Double(norm.epsilon))
+    },
+    backward: {
+        Raw.fusedBatchNormGradV2(
+            yBackprop: input, input, scale: TF(norm.scale), 
+            reserveSpace1: bnresult.reserveSpace1, 
+            reserveSpace2: bnresult.reserveSpace2, 
+            epsilon: Double(norm.epsilon))
+    })
+
+# +
+struct PullbackArgs<T : TensorGroup, U : TensorGroup> : TensorGroup {
+    let input: T
+    let cotangent: U
+}
+
+class CompiledFunction<Input: Differentiable & TensorGroup, Output: Differentiable & TensorGroup> {
+    let f: @differentiable (Input) -> Output
+    init(_ f: @escaping @differentiable (Input) -> Output) {
+        self.f = f
+    }
+}
+
+func xlaCompiled<T : Differentiable & TensorGroup, U : Differentiable & TensorGroup>(
+    _ fn: @escaping @differentiable (T) -> U) -> CompiledFunction<T, U>
+    where T.TangentVector : TensorGroup, U.TangentVector : TensorGroup {
+    let xlaCompiledFn: (T) -> U = _graph(fn, useXLA: true)
+    let xlaCompiledPullback = _graph(
+        { (pbArgs: PullbackArgs<T, U.TangentVector>) in
+            pullback(at: pbArgs.input, in: fn)(pbArgs.cotangent) },
+        useXLA: true
+    )
+    return CompiledFunction(differentiableFunction { x in
+        (value: xlaCompiledFn(x), pullback: { v in
+            xlaCompiledPullback(PullbackArgs(input: x, cotangent: v))})
+    })
+}
+
+# +
+struct TrainingKernelInput: TensorGroup, Differentiable, AdditiveArithmetic {
+    var input, scale, offset, runningMean, runningVariance, momentum, epsilon: TF
+}
+
+struct TrainingKernelOutput: TensorGroup, Differentiable, AdditiveArithmetic {
+    var normalized, newRunningMean, newRunningVariance: TF
+}
+
+@differentiable
+func trainingKernel(_ input: TrainingKernelInput) -> TrainingKernelOutput {
+    let mean = input.input.mean(alongAxes: [0, 1, 2])
+    let variance = input.input.variance(alongAxes: [0, 1, 2])
+    let invMomentum = TF(1) - input.momentum
+    let newRunningMean = input.runningMean * input.momentum + mean * invMomentum
+    let newRunningVariance = input.runningVariance * input.momentum + variance * invMomentum
+    let normalizer = rsqrt(variance + input.epsilon) * input.scale
+    let normalized = (input.input - mean) * normalizer + input.offset
+    return TrainingKernelOutput(
+        normalized: normalized,
+        newRunningMean: newRunningMean,
+        newRunningVariance: newRunningVariance
+    )
+}
+
+# +
+let input = TF(randomUniform: [64, 28, 28, 32])
+let norm = FABatchNorm(32)
+let compiledTrainingKernel = xlaCompiled(trainingKernel)
+let kernelInput = TrainingKernelInput(
+    input: input,
+    scale: norm.scale,
+    offset: norm.offset,
+    runningMean: norm.runningMean.value,
+    runningVariance: norm.runningVariance.value,
+    momentum: Tensor(norm.momentum),
+    epsilon: Tensor(norm.epsilon))
+let pb = pullback(at: kernelInput) { x in compiledTrainingKernel.f(x) }
+let kernelOutput = compiledTrainingKernel.f(kernelInput)
+
+benchmark(
+    forward: { compiledTrainingKernel.f(kernelInput) },
+    backward: { pb(kernelOutput) })
+# -
+
+
diff --git a/nbs/swift/08_data_block.ipynb b/nbs/swift/08_data_block.ipynb
index 6ae540b..3a2a31e 100644
--- ./nbs/swift/08_data_block.ipynb
+++ ./nbs/swift/08_data_block.ipynb
@@ -1180,6 +1180,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/08_data_block.py b/nbs/swift/08_data_block.py
new file mode 100644
index 0000000..6fd231d
--- /dev/null
+++ ./nbs/swift/08_data_block.py
@@ -0,0 +1,461 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+# # Data block foundations
+
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/FastaiNotebook_07_batchnorm")' FastaiNotebook_07_batchnorm
+
+//export
+import Path
+import TensorFlow
+import Python
+
+import FastaiNotebook_07_batchnorm
+
+%include "EnableIPythonDisplay.swift"
+IPythonDisplay.shell.enable_matplotlib("inline")
+
+# ## Image ItemList
+
+# ### Download Imagenette
+
+# First things first, we need to download Imagenette and untar it. What follows is very close to what we did for MNIST.
+
+//export
+public let dataPath = Path.home/".fastai"/"data"
+
+//export
+public func downloadImagenette(path: Path = dataPath, sz:String="-160") -> Path {
+    let url = "https://s3.amazonaws.com/fast-ai-imageclas/imagenette\(sz).tgz"
+    let fname = "imagenette\(sz)"
+    let file = path/fname
+    try! path.mkdir(.p)
+    if !file.exists {
+        downloadFile(url, dest:(path/"\(fname).tgz").string)
+        _ = "/bin/tar".shell("-xzf", (path/"\(fname).tgz").string, "-C", path.string)
+    }
+    return file
+}
+
+let path = downloadImagenette(sz:"-320")
+
+# If we look at `path.ls()`, we see it returns a list of entries, which are structures with a `kind` and a `path` attribute. The `kind` is an enum that can be `file` or `directory`. `path` then points to the corresponding location.
+
+for e in path.ls() { print("\(e.path) (\(e.kind == .directory ? "directory": "file"))")}
+
+for e in (path/"val").ls() { print("\(e.path) (\(e.kind == .directory ? "directory": "file"))")}
+
+# Let's have a look inside a class folder (the first class is tench):
+
+let pathTench = path/"val"/"n01440764"
+
+let imgFn = Path.home/".fastai/data/imagenette-320/val/n01440764/ILSVRC2012_val_00006697.JPEG"
+imgFn.string
+
+# We will use `tf.data` to read and resize our images in parallel. `tf.data` needs to operate on tensors, so we convert our `Path` image filename to that format. We can then apply the extensions that we defined previously in 01.
+
+let decodedImg = StringTensor(readFile: imgFn.string).decodeJpeg(channels: 3)
+
+print(decodedImg.shape)
+
+# By converting this image to numpy, we can use `plt` to plot it:
+
+# +
+//export
+public func show_img<T:NumpyScalarCompatible>(_ img: Tensor<T>, _ w: Int = 7, _ h: Int = 5) {
+    show_img(img.makeNumpyArray(), w, h)
+}
+
+public func show_img(_ img: PythonObject, _ w: Int = 7, _ h: Int = 5) {
+    plt.figure(figsize: [w, h])
+    plt.imshow(img)
+    plt.axis("off")
+    plt.show()
+}
+# -
+
+show_img(decodedImg)
+
+# ### Grab all the images
+
+# Now that we have donloaded the data, we need to be able to recursively grab all the filenames in the imagenette folder. The following function walks recursively through the folder and adds the filenames that have the right extension.
+
+//export
+public func fetchFiles(path: Path, recurse: Bool = false, extensions: [String]? = nil) -> [Path] {
+    var res: [Path] = []
+    for p in try! path.ls(){
+        if p.kind == .directory && recurse { 
+            res += fetchFiles(path: p.path, recurse: recurse, extensions: extensions)
+        } else if extensions == nil || extensions!.contains(p.path.extension.lowercased()) {
+            res.append(p.path)
+        }
+    }
+    return res
+}
+
+# Note that we don't have a generic `open_image` function like in python here, but will be using a specific decode function (here for jpegs, but there is one for gifs or pngs). That's why we limit ourselves to jpeg exensions here.
+
+time { let fNames = fetchFiles(path: path, recurse: true, extensions: ["jpeg", "jpg"]) }
+
+let fNames = fetchFiles(path: path, recurse: true, extensions: ["jpeg", "jpg"])
+
+fNames.count == 13394
+
+# ## Prepare the data
+
+# `Dataset` can handle all the transforms that go on a `Tensor`, including opening an image and resizing it since it takes `StringTensor`. That makes the `tfms` attribute of `ItemList` irrelevant, so `ItemList` is just an array of `Item` with a path (if get method seems useful later, we can add it).
+
+// export
+public struct ItemList<Item>{
+    public var items: [Item]
+    public let path: Path
+    
+    public init(items: [Item], path: Path){
+        (self.items,self.path) = (items,path)
+    }
+}
+
+// export
+public extension ItemList where Item == Path {
+    init(fromFolder path: Path, extensions: [String], recurse: Bool = true) {
+        self.init(items: fetchFiles(path: path, recurse: recurse, extensions: extensions),
+                  path:  path)
+    }
+}
+
+let il = ItemList(fromFolder: path, extensions: ["jpeg", "jpg"])
+
+# ### Split
+
+// export
+public struct SplitData<Item>{
+    public let train: ItemList<Item>
+    public let valid: ItemList<Item>
+    public var path: Path { return train.path }
+    
+    public init(train: ItemList<Item>, valid: ItemList<Item>){
+        (self.train, self.valid) = (train, valid)
+    }
+    
+    public init(_ il: ItemList<Item>, fromFunc: (Item) -> Bool){
+        self.init(train: ItemList(items: il.items.filter { !fromFunc($0) }, path: il.path),
+                  valid: ItemList(items: il.items.filter {  fromFunc($0) }, path: il.path))
+    }
+}
+
+// export
+public func grandParentSplitter(fName: Path, valid: String = "valid") -> Bool{
+    return fName.parent.parent.basename() == valid
+}
+
+let sd = SplitData(il) { grandParentSplitter(fName: $0, valid: "val") }
+
+# ### Processor
+
+// export
+public protocol Processor {
+    associatedtype Input
+    associatedtype Output
+    
+    mutating func initState(items: [Input])
+    func process1(item: Input) -> Output
+    func deprocess1(item: Output) -> Input
+}
+
+// export
+public extension Processor {
+    func process(items: [Input]) -> [Output] {
+        return items.map { process1(item: $0) }
+    }
+    
+    func deprocess(items: [Output]) -> [Input] {
+        return items.map { deprocess1(item: $0) }
+    }
+}
+
+// export
+public struct NoopProcessor<Item>: Processor {
+    public init() {}
+   
+    public mutating func initState(items: [Item]) {}
+    
+    public func process1  (item: Item) -> Item { return item }
+    public func deprocess1(item: Item) -> Item { return item }
+}
+
+// export
+public struct CategoryProcessor: Processor {
+    public init() {}
+    public var vocab: [String]? = nil
+    public var reverseMap: [String: Int32]? = nil
+    
+    public mutating func initState(items: [String]) {
+        vocab = Array(Set(items)).sorted()
+        reverseMap = [:]
+        for (i,x) in vocab!.enumerated() { reverseMap![x] = Int32(i) }
+    }
+    
+    public func process1  (item: String) -> Int32 { return reverseMap![item]! }
+    public func deprocess1(item: Int32)  -> String { return vocab![Int(item)] }
+}
+
+# ### Label
+
+# When we build the datasets, we don't need to return a tupe (item, label) but to have the tensor(s) with the items and the tensor(s) with the labels separately.
+
+//export
+public struct LabeledItemList<PI,PL> where PI: Processor, PL: Processor{
+    public var items: [PI.Output]
+    public var labels: [PL.Output]
+    public let path: Path
+    public var procItem: PI
+    public var procLabel: PL
+    
+    public init(rawItems: [PI.Input], rawLabels: [PL.Input], path: Path, procItem: PI, procLabel: PL){
+        (self.procItem,self.procLabel,self.path) = (procItem,procLabel,path)
+        self.items = procItem.process(items: rawItems)
+        self.labels = procLabel.process(items: rawLabels)
+    }
+    
+    public init(_ il: ItemList<PI.Input>, fromFunc: (PI.Input) -> PL.Input, procItem: PI, procLabel: PL){
+        self.init(rawItems:  il.items,
+                  rawLabels: il.items.map{ fromFunc($0)},
+                  path:      il.path,
+                  procItem:  procItem,
+                  procLabel: procLabel)
+    }
+    
+    public func rawItem (_ idx: Int) -> PI.Input { return procItem.deprocess1 (item: items[idx])  }
+    public func rawLabel(_ idx: Int) -> PL.Input { return procLabel.deprocess1(item: labels[idx]) }
+}
+
+# +
+//export
+public struct SplitLabeledData<PI,PL> where PI: Processor, PL: Processor{
+    public let train: LabeledItemList<PI,PL>
+    public let valid: LabeledItemList<PI,PL>
+    public var path: Path { return train.path }
+    
+    public init(train: LabeledItemList<PI,PL>, valid: LabeledItemList<PI,PL>){
+        (self.train, self.valid) = (train, valid)
+    }
+    
+    public init(_ sd: SplitData<PI.Input>, fromFunc: (PI.Input) -> PL.Input, procItem: inout PI, procLabel: inout PL){
+        procItem.initState(items: sd.train.items)
+        let trainLabels = sd.train.items.map{ fromFunc($0) }
+        procLabel.initState(items: trainLabels)
+        self.init(train: LabeledItemList(rawItems: sd.train.items, rawLabels: trainLabels, path: sd.path, 
+                                         procItem: procItem, procLabel: procLabel),
+                  valid: LabeledItemList(sd.valid, fromFunc: fromFunc, procItem: procItem, procLabel: procLabel))
+    }
+}
+
+/// Make a labeled data without an input processor, by defaulting to a noop processor.
+public func makeLabeledData<T, PL: Processor>(_ sd: SplitData<T>, fromFunc: (T) -> PL.Input, procLabel: inout PL) 
+ -> SplitLabeledData<NoopProcessor<T>, PL> {
+    var pi = NoopProcessor<T>()
+    return SplitLabeledData(sd, fromFunc: fromFunc, procItem: &pi, procLabel: &procLabel)
+}
+
+# -
+
+//export
+public func parentLabeler(_ fName: Path) -> String { return fName.parent.basename() }
+
+var (procItem,procLabel) = (NoopProcessor<Path>(),CategoryProcessor())
+let sld = SplitLabeledData(sd, fromFunc: parentLabeler, procItem: &procItem, procLabel: &procLabel)
+
+print(sld.train.labels[0])
+print(sld.train.rawLabel(0))
+print(sld.train.procLabel.vocab!)
+
+# ### Datasets
+
+# To go in a Dataset, our array of items and array of labels need to be converted to tensors.
+
+// export
+public struct LabeledElement<I: TensorGroup, L: TensorGroup>: TensorGroup {
+    public var xb: I
+    public var yb: L    
+    
+    public init(xb: I, yb: L) {
+        (self.xb, self.yb) = (xb, yb)
+    }
+}
+
+// export
+public extension SplitLabeledData {
+    func toDataBunch<XB, YB> (
+        itemToTensor: ([PI.Output]) -> XB, labelToTensor: ([PL.Output]) -> YB, bs: Int = 64
+    ) -> DataBunch<LabeledElement<XB, YB>> where XB: TensorGroup, YB: TensorGroup {
+        let trainDs = Dataset<LabeledElement<XB, YB>>(
+            elements: LabeledElement(xb: itemToTensor(train.items), yb: labelToTensor(train.labels)))
+        let validDs = Dataset<LabeledElement<XB, YB>>(
+            elements: LabeledElement(xb: itemToTensor(valid.items), yb: labelToTensor(valid.labels)))
+        return DataBunch(train: trainDs, valid: validDs, 
+                         trainLen: train.items.count, validLen: valid.items.count,
+                         bs: bs)
+    }
+}
+
+// export
+public func pathsToTensor(_ paths: [Path]) -> StringTensor { return StringTensor(paths.map{ $0.string })}
+public func intsToTensor(_ items: [Int32]) -> Tensor<Int32> { return Tensor<Int32>(items)}
+
+let dataset = sld.toDataBunch(itemToTensor: pathsToTensor, labelToTensor: intsToTensor)
+
+# ### Transforms
+
+# We directly plug in to the dataset the transforms we want to apply.
+
+// export
+public func transformData<I,TI,L>(
+    _ data: DataBunch<LabeledElement<I,L>>, 
+    nWorkers:Int=4,
+    tfmItem: (I) -> TI
+) -> DataBunch<DataBatch<TI,L>> 
+where I: TensorGroup, TI: TensorGroup & Differentiable, L: TensorGroup{
+    return DataBunch(train: data.train.innerDs.map(parallelCallCount: nWorkers){ DataBatch(xb: tfmItem($0.xb), yb: $0.yb) },
+                     valid: data.valid.innerDs.map(parallelCallCount: nWorkers){ DataBatch(xb: tfmItem($0.xb), yb: $0.yb) },
+                     trainLen: data.train.dsCount, 
+                     validLen: data.valid.dsCount,
+                     bs: data.train.bs)
+}
+
+// export
+public func openAndResize(fname: StringTensor, size: Int) -> TF{
+    let decodedImg = StringTensor(readFile: fname).decodeJpeg(channels: 3)
+    let resizedImg = Tensor<Float>(Raw.resizeBilinear(
+        images: Tensor<UInt8>([decodedImg]), 
+        size: Tensor<Int32>([Int32(size), Int32(size)]))) / 255.0
+    return resizedImg.reshaped(to: TensorShape(size, size, 3))
+}
+
+let tfmData = transformData(dataset) { openAndResize(fname: $0, size: 128) }
+
+// export
+public extension FADataset {
+    func oneBatch() -> Element? {
+        for batch in ds { return batch }
+        return nil
+    }
+} 
+
+let batch = tfmData.train.oneBatch()!
+batch.xb.shape
+
+// export
+public func showImages(_ xb: TF, labels: [String]? = nil) {
+    let (rows,cols) = (3,3)
+    plt.figure(figsize: [9, 9])
+    for i in 0..<(rows * cols) {
+        let img = plt.subplot(rows, cols, i + 1)
+        img.axis("off")
+        let x = xb[i].makeNumpyArray()
+        img.imshow(x)
+        if labels != nil { img.set_title(labels![i]) }
+        if (i + 1) >= (rows * cols) { break }
+    }
+    plt.show()
+}
+
+let labels = batch.yb.scalars.map { sld.train.procLabel.vocab![Int($0)] }
+showImages(batch.xb, labels: labels)
+
+# ### To summarize:
+
+let il = ItemList(fromFolder: path, extensions: ["jpeg", "jpg"])
+let sd = SplitData(il, fromFunc: {grandParentSplitter(fName: $0, valid: "val")})
+var (procItem,procLabel) = (NoopProcessor<Path>(), CategoryProcessor())
+let sld = SplitLabeledData(sd, fromFunc: parentLabeler, procItem: &procItem, procLabel: &procLabel)
+var rawData = sld.toDataBunch(itemToTensor: pathsToTensor, labelToTensor: intsToTensor, bs: 256)
+var data = transformData(rawData) { openAndResize(fname: $0, size: 224) }
+
+// tf.data reads the whole file into memory if we shuffle!
+data.train.shuffle = false
+
+time { let _ = data.train.oneBatch() }
+
+func allBatches() -> (Int,TF) {
+    var m = TF(zeros: [224, 224, 3])
+    var c: Int = 0
+    for batch in data.train.ds { 
+        m += batch.xb.mean(squeezingAxes: 0) 
+        c += 1
+    }
+    return (c,m)
+}
+
+time {let (c,m) = allBatches()}
+
+let il = ItemList(fromFolder: path, extensions: ["jpeg", "jpg"])
+let sd = SplitData(il, fromFunc: {grandParentSplitter(fName: $0, valid: "val")})
+var (procItem,procLabel) = (NoopProcessor<Path>(), CategoryProcessor())
+let sld = SplitLabeledData(sd, fromFunc: parentLabeler, procItem: &procItem, procLabel: &procLabel)
+var rawData = sld.toDataBunch(itemToTensor: pathsToTensor, labelToTensor: intsToTensor)
+let data = transformData(rawData) { openAndResize(fname: $0, size: 128) }
+
+# Let's try to train it:
+
+//export 
+public let imagenetStats = (mean: TF([0.485, 0.456, 0.406]), std: TF([0.229, 0.224, 0.225]))
+
+//export
+public func prevPow2(_ x: Int) -> Int { 
+    var res = 1
+    while res <= x { res *= 2 }
+    return res / 2
+}
+
+//export
+public struct CNNModel: Layer {
+    public var convs: [ConvBN<Float>]
+    public var pool = FAGlobalAvgPool2D<Float>()
+    public var linear: FADense<Float>
+    
+    public init(channelIn: Int, nOut: Int, filters: [Int]){
+        convs = []
+        let (l1,l2) = (channelIn, prevPow2(channelIn * 9))
+        convs = [ConvBN(l1,   l2,   stride: 1),
+                 ConvBN(l2,   l2*2, stride: 2),
+                 ConvBN(l2*2, l2*4, stride: 2)]
+        let allFilters = [l2*4] + filters
+        for i in 0..<filters.count { convs.append(ConvBN(allFilters[i], allFilters[i+1], stride: 2)) }
+        linear = FADense<Float>(filters.last!, nOut)
+    }
+    
+    @differentiable
+    public func callAsFunction(_ input: TF) -> TF {
+        // TODO: Work around https://bugs.swift.org/browse/TF-606
+        return linear.forward(pool.forward(convs(input)))
+    }
+}
+
+func optFunc(_ model: CNNModel) -> SGD<CNNModel> { return SGD(for: model, learningRate: 0.1) }
+func modelInit() -> CNNModel { return CNNModel(channelIn: 3, nOut: 10, filters: [64, 64, 128, 256]) }
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+let recorder = learner.makeDefaultDelegates(metrics: [accuracy])
+learner.addDelegate(learner.makeNormalize(mean: imagenetStats.mean, std: imagenetStats.std))
+
+learner.fit(1)
+
+# ## Export
+
+import NotebookExport
+let exporter = NotebookExport(Path.cwd/"08_data_block.ipynb")
+print(exporter.export(usingPrefix: "FastaiNotebook_"))
+
+
diff --git a/nbs/swift/08a_heterogeneous_dictionary.ipynb b/nbs/swift/08a_heterogeneous_dictionary.ipynb
index 7330766..713ed26 100644
--- ./nbs/swift/08a_heterogeneous_dictionary.ipynb
+++ ./nbs/swift/08a_heterogeneous_dictionary.ipynb
@@ -277,6 +277,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/08a_heterogeneous_dictionary.py b/nbs/swift/08a_heterogeneous_dictionary.py
new file mode 100644
index 0000000..8c86ac4
--- /dev/null
+++ ./nbs/swift/08a_heterogeneous_dictionary.py
@@ -0,0 +1,92 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/FastaiNotebook_08_data_block")' FastaiNotebook_08_data_block
+
+//export
+import Path
+
+import FastaiNotebook_08_data_block
+
+// export
+public protocol HetDictKey {
+    associatedtype ValueType
+    static var defaultValue: ValueType { get }
+}
+
+# +
+// export
+
+public struct HeterogeneousDictionary {
+    private var underlying: [ObjectIdentifier : Any] = [:]
+    
+    public init() {}
+    public init<T: HetDictKey>(_ key: T.Type, _ value: T.ValueType) {
+        self.underlying = [ObjectIdentifier(key): value]
+    }
+    public init<T1: HetDictKey, T2: HetDictKey>(_ key1: T1.Type, _ value1: T1.ValueType, _ key2: T2.Type, _ value2: T2.ValueType) {
+        self.underlying = [ObjectIdentifier(key1): value1, ObjectIdentifier(key2): value2]
+    }
+
+    public subscript<T: HetDictKey>(key: T.Type) -> T.ValueType {
+        get { return underlying[ObjectIdentifier(key), default: T.defaultValue] as! T.ValueType }
+        set { underlying[ObjectIdentifier(key)] = newValue as Any }
+    }
+    
+    public mutating func merge(_ other: HeterogeneousDictionary,
+        uniquingKeysWith combine: (Any, Any) throws -> Any) rethrows {
+        try self.underlying.merge(other.underlying, uniquingKeysWith: combine)
+    }
+}
+
+# -
+
+// export
+// Common keys
+public struct LearningRate: HetDictKey {
+    public static var defaultValue: Float = 0.4
+}
+
+public struct StepCount: HetDictKey {
+    public static var defaultValue = 0
+}
+
+// Sample usage
+var m = HeterogeneousDictionary()
+
+
+# +
+print(m[LearningRate.self])
+m[LearningRate.self] = 3.4
+print(m[LearningRate.self])
+
+print(m[StepCount.self])
+m[StepCount.self] = 3
+print(m[StepCount.self])
+
+# -
+
+print(type(of: m[StepCount.self]))
+print(type(of: m[LearningRate.self]))
+
+
+# ## Export
+
+import NotebookExport
+let exporter = NotebookExport(Path.cwd/"08a_heterogeneous_dictionary.ipynb")
+print(exporter.export(usingPrefix: "FastaiNotebook_"))
+
+
diff --git a/nbs/swift/08b_data_block_opencv.ipynb b/nbs/swift/08b_data_block_opencv.ipynb
index af1da44..b67b151 100644
--- ./nbs/swift/08b_data_block_opencv.ipynb
+++ ./nbs/swift/08b_data_block_opencv.ipynb
@@ -1249,6 +1249,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/08b_data_block_opencv.py b/nbs/swift/08b_data_block_opencv.py
new file mode 100644
index 0000000..86744da
--- /dev/null
+++ ./nbs/swift/08b_data_block_opencv.py
@@ -0,0 +1,456 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+# # Data block foundations
+
+# Uncomment line below when using Colab (this installs OpenCV4)
+# %system SwiftCV/install/install_colab.sh
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/FastaiNotebook_07_batchnorm")' FastaiNotebook_07_batchnorm
+%install '.package(path: "$cwd/SwiftCV")' SwiftCV
+
+//export
+import Path
+import TensorFlow
+import Python
+
+import FastaiNotebook_07_batchnorm
+
+%include "EnableIPythonDisplay.swift"
+IPythonDisplay.shell.enable_matplotlib("inline")
+
+# ## Image ItemList
+
+# ### Download Imagenette
+
+# First things first, we need to download Imagenette and untar it. What follows is very close to what we did for MNIST.
+
+//export
+public let dataPath = Path.home/".fastai"/"data"
+
+//export
+public func downloadImagenette(path: Path = dataPath) -> Path {
+    let url = "https://s3.amazonaws.com/fast-ai-imageclas/imagenette-160.tgz"
+    let fname = "imagenette-160"
+    let file = path/fname
+    try! path.mkdir(.p)
+    if !file.exists {
+        downloadFile(url, dest:(path/"\(fname).tgz").string)
+        _ = "/bin/tar".shell("-xzf", (path/"\(fname).tgz").string, "-C", path.string)
+    }
+    return file
+}
+
+let path = downloadImagenette()
+
+# If we look at `path.ls()`, we see it returns a list of entries, which are structures with a `kind` and a `path` attribute. The `kind` is an enum that can be `file` or `directory`. `path` then points to the corresponding location.
+
+for e in path.ls() { print("\(e.path) (\(e.kind == .directory ? "directory": "file"))")}
+
+for e in (path/"val").ls() { print("\(e.path) (\(e.kind == .directory ? "directory": "file"))")}
+
+# Let's have a look inside a class folder (the first class is tench):
+
+let pathTench = path/"val"/"n01440764"
+
+let imgFn = Path.home/".fastai/data/imagenette-160/val/n01440764/ILSVRC2012_val_00006697.JPEG"
+imgFn.string
+
+# We will use opencv to read and resize our images.
+
+//export
+import SwiftCV
+import Foundation
+
+//load the image in memory
+let imgContent = Data(contentsOf: imgFn.url)
+// make opencv image
+var cvImg = imdecode(imgContent)
+// convert to RGB
+cvImg = cvtColor(cvImg, nil, ColorConversionCode.COLOR_BGR2RGB)
+
+# By converting this image to a tensor then numpy, we can use `plt` to plot it:
+
+let tensImg = Tensor<UInt8>(cvMat: cvImg)!
+let numpyImg = tensImg.makeNumpyArray()
+plt.imshow(numpyImg) 
+plt.axis("off")
+plt.show()
+
+# ### Grab all the images
+
+# Now that we have donloaded the data, we need to be able to recursively grab all the filenames in the imagenette folder. The following function walks recursively through the folder and adds the filenames that have the right extension.
+
+//export
+public func fetchFiles(path: Path, recurse: Bool = false, extensions: [String]? = nil) -> [Path] {
+    var res: [Path] = []
+    for p in try! path.ls(){
+        if p.kind == .directory && recurse { 
+            res += fetchFiles(path: p.path, recurse: recurse, extensions: extensions)
+        } else if extensions == nil || extensions!.contains(p.path.extension.lowercased()) {
+            res.append(p.path)
+        }
+    }
+    return res
+}
+
+# Note that we don't have a generic `open_image` function like in python here, but will be using a specific decode function (here for jpegs, but there is one for gifs or pngs). That's why we limit ourselves to jpeg exensions here.
+
+time { let fNames = fetchFiles(path: path, recurse: true, extensions: ["jpeg", "jpg"]) }
+
+let fNames = fetchFiles(path: path, recurse: true, extensions: ["jpeg", "jpg"])
+
+fNames.count == 13394
+
+print(type(of: cvImg))
+
+# ## Prepare the data
+
+//export
+public protocol ItemBase {
+    func transform( _ tfms: [(inout Self) -> ()]) -> Self
+}
+
+public struct Image: ItemBase{
+    
+    public var path: Path
+    public lazy var img: Mat = {
+        //print("Load image in memory")
+        return imdecode(try! Data(contentsOf: path.url))
+    } ()
+    
+    public init(_ img: Mat, _ path: Path) {
+        (self.path,self.img) = (path,img)
+    }
+    
+    public init(_ path: Path) { self.path = path }
+    
+    public mutating func show(){
+        let tensImg = Tensor<UInt8>(cvMat: img)!
+        let numpyImg = tensImg.makeNumpyArray()
+        plt.imshow(numpyImg) 
+        plt.axis("off")
+        plt.show()
+    }
+    
+    public func transform(_ tfms: [(inout Image) -> ()]) -> Image{
+        var tfmedImg = Image(path)
+        tfms.forEach() { $0(&tfmedImg) }
+        return tfmedImg
+    }
+    
+    public mutating func toTensor() -> TF {
+        return TF(Tensor<UInt8>(cvMat: img)!)
+    }
+}
+
+var img = Image(imgFn)
+
+img.show()
+
+# ### ItemList
+
+// export
+public struct ItemList<T> where T: ItemBase{
+    public var items: [T]
+    public let path: Path
+    public var tfms: [(inout T) -> ()] = [] 
+    
+    public init(items: [T], path: Path, tfms: [(inout T) -> ()] = []){
+        (self.items,self.path,self.tfms) = (items,path,tfms)
+    }
+    
+    public init (_ il: ItemList<T>, newItems: [T]) {
+        self.init(items: newItems, path: il.path, tfms: il.tfms)
+    }
+    
+    public subscript(index: Int) -> T {
+        return items[index].transform(tfms)
+    }
+}
+
+// export
+public protocol InitableFromPath {
+    init(_ path: Path)
+}
+extension Image: InitableFromPath {}
+
+// export
+public extension ItemList where T: InitableFromPath {
+    init(fromFolder path: Path, extensions: [String], recurse: Bool = true, tfms: [(inout T) -> ()] = []) {
+        self.init(items: fetchFiles(path: path, recurse: recurse, extensions: extensions).map { T($0) },
+                  path:  path,
+                  tfms: tfms)
+    }
+}
+
+let il: ItemList<Image> = ItemList(fromFolder: path, extensions: ["jpeg", "jpg"])
+
+var img = il[0]
+img.show()
+
+# +
+func convertRGB(_ img: inout Image) {
+    img.img = cvtColor(img.img, nil, ColorConversionCode.COLOR_BGR2RGB)
+}
+
+func resize(_ img: inout Image, size: Int) {
+    img.img = resize(img.img, nil, Size(size, size), 0, 0, InterpolationFlag.INTER_AREA)
+}
+# -
+
+let il: ItemList<Image> = ItemList(fromFolder: path, extensions: ["jpeg", "jpg"],
+                                  tfms: [convertRGB, { resize(&$0, size:128) }])
+
+var img = il[0]
+img.show()
+
+# ### Split
+
+// export
+public struct SplitData<T> where T: ItemBase{
+    public let train, valid: ItemList<T>
+    public var path: Path { return train.path }
+    
+    public init(train: ItemList<T>, valid: ItemList<T>){
+        (self.train, self.valid) = (train, valid)
+    }
+    
+    public init(_ il: ItemList<T>, fromFunc: (T) -> Bool){
+        self.init(train: ItemList(il, newItems: il.items.filter { !fromFunc($0) }),
+                  valid: ItemList(il, newItems: il.items.filter {  fromFunc($0) }))
+    }
+}
+
+// export
+public func grandParentSplitter(fName: Path, valid: String = "valid") -> Bool{
+    return fName.parent.parent.basename() == valid
+}
+
+let sd = SplitData(il) { grandParentSplitter(fName: $0.path, valid: "val") }
+
+var img = sd.train[0]
+img.show()
+
+# ### Processor
+
+// export
+public protocol Processor {
+    associatedtype Input: ItemBase
+    associatedtype Output: ItemBase
+    
+    mutating func initState(_ items: [Input])
+    func process1(_ item: Input) -> Output
+    func deprocess1(_ item: Output) -> Input
+}
+
+// export
+public extension Processor {
+    func process(_ items: [Input]) -> [Output] {
+        return items.map { process1($0) }
+    }
+    
+    func deprocess(_ items: [Output]) -> [Input] {
+        return items.map { deprocess1($0) }
+    }
+}
+
+// export
+public struct NoopProcessor<Item>: Processor where Item: ItemBase{
+    public init() {}
+   
+    public mutating func initState(_ items: [Item]) {}
+    
+    public func process1  (_ item: Item) -> Item { return item }
+    public func deprocess1(_ item: Item) -> Item { return item }
+}
+
+# +
+//export
+extension String: ItemBase {
+    public func transform(_ tfms: [(inout String) -> ()]) -> (String) { return self }
+}
+
+extension Int: ItemBase {
+    public func transform(_ tfms: [(inout Int) -> ()]) -> (Int) { return self }
+}
+# -
+
+// export
+public struct CategoryProcessor: Processor {
+    public init() {}
+    public var vocab: [String]? = nil
+    public var reverseMap: [String: Int]? = nil
+    
+    public mutating func initState(_ items: [String]) {
+        vocab = Array(Set(items)).sorted()
+        reverseMap = [:]
+        for (i,x) in vocab!.enumerated() { reverseMap![x] = i }
+    }
+    
+    public func process1  (_ item: String) -> Int { return reverseMap![item]! }
+    public func deprocess1(_ item: Int) -> String { return vocab![item] }
+}
+
+# ### Label
+
+# When we build the datasets, we don't need to return a tupe (item, label) but to have the tensor(s) with the items and the tensor(s) with the labels separately.
+
+# +
+public struct LabeledItemList<I, L> where I:ItemBase, L: ItemBase{
+    public var inputs: ItemList<I>
+    public var labels: ItemList<L>
+    public var path: Path { return inputs.path }
+    
+    public init(inputs: ItemList<I>, labels: ItemList<L>) {
+        (self.inputs,self.labels) = (inputs,labels)
+    }
+    
+    public subscript(_ i: Int) -> (I, L) {
+        return (inputs[i], labels[i])
+    }
+}
+
+public extension LabeledItemList {
+    init(_ il: ItemList<I>, labelWithFunc f: @escaping (I) -> L) {
+        self.init(inputs: il, labels: ItemList(items: il.items.map{ f($0) }, path: il.path, tfms: []))
+    }
+}
+# -
+
+public func parentLabeler(_ fName: Path) -> String { return fName.parent.basename() }
+
+let ll = LabeledItemList(il, labelWithFunc: { parentLabeler($0.path) })
+
+var x = ll[0]
+x.0.show()
+print(x.1)
+
+public func process<PI> (_ il: ItemList<PI.Input>, proc: PI) -> ItemList<PI.Output> where PI: Processor {
+    return ItemList(items: proc.process(il.items), path: il.path, tfms: [])
+}
+
+public func process<PI, PL> (_ lil: LabeledItemList<PI.Input, PL.Input>, procInp: PI, procLab: PL) 
+-> LabeledItemList<PI.Output, PL.Output> where PI: Processor, PL: Processor {
+    return LabeledItemList(
+        inputs: process(lil.inputs, proc: procInp),
+        labels: process(lil.labels, proc: procLab)
+    )
+}
+
+public struct SplitLabeledData<PI, PL> where PI: Processor, PL: Processor{
+    public var train, valid: LabeledItemList<PI.Output, PL.Output>
+    public var path: Path { return train.path }
+    public var procInp: PI
+    public var procLab: PL
+    
+    public init(_ rawTrain: LabeledItemList<PI.Input,PL.Input>, 
+                _ rawValid: LabeledItemList<PI.Input,PL.Input>,
+                procInp: inout PI,
+                procLab: inout PL) {
+        procInp.initState(rawTrain.inputs.items)
+        procLab.initState(rawTrain.labels.items)
+        train = process(rawTrain, procInp: procInp, procLab: procLab)
+        valid = process(rawValid, procInp: procInp, procLab: procLab)
+        (self.procInp,self.procLab) = (procInp,procLab)
+    }
+}
+
+public extension SplitLabeledData {
+    init(_ sd: SplitData<PI.Input>, 
+         labelWithFunc f: @escaping (PI.Input) -> PL.Input,
+         procInp: inout PI,
+         procLab: inout PL) {
+        self.init(LabeledItemList(sd.train, labelWithFunc: f),
+                  LabeledItemList(sd.valid, labelWithFunc: f),
+                  procInp: &procInp,
+                  procLab: &procLab)
+    }
+}
+
+var procInp = NoopProcessor<Image>()
+var procLab = CategoryProcessor()
+
+var sld = SplitLabeledData(sd, labelWithFunc: { parentLabeler($0.path) }, procInp: &procInp, procLab: &procLab)
+
+# Labeling loses the transforms.
+
+var x = sld.train[0]
+x.0.show()
+print(sld.procLab.deprocess1(x.1))
+
+# So we add them back
+
+sld.train.inputs.tfms = [convertRGB, { resize(&$0, size:128) }]
+
+var x = sld.train[0]
+x.0.show()
+print(sld.procLab.deprocess1(x.1))
+
+public extension SplitLabeledData{
+    mutating func transform(_ tfms: ([(inout PI.Output) -> ()], [(inout PI.Output) -> ()])){
+        train.inputs.tfms = tfms.0
+        valid.inputs.tfms = tfms.1
+    }
+}
+
+var sld = SplitLabeledData(sd, labelWithFunc: { parentLabeler($0.path) }, procInp: &procInp, procLab: &procLab)
+let tfms = [convertRGB, { resize(&$0, size:512) }]
+sld.transform((tfms, tfms))
+
+var x = sld.train[0]
+x.0.show()
+print(sld.procLab.deprocess1(x.1))
+
+# What's below doesn't work with that's above.
+
+func loadSync(_ n: Int) -> TF {
+    var imgs: [TF] = []
+    for i in 1...n { 
+        var img = sld.train[i].0
+        imgs.append(img.toTensor().expandingShape(at: 0) / 255.0)
+    }
+    return TF(concatenating: imgs, alongAxis: 0)
+}
+
+time { let imgs = loadSync(100) }
+
+func loadQSync(_ n: Int) -> [Image] {
+    var imgs: [Image] = []
+    let queue = DispatchQueue(label: "myqueue")
+    queue.sync {
+        for i in 1...n { imgs.append(sld.train[i].0) }
+    }
+    return imgs
+}
+
+time { let imgs = loadQSync(100) }
+
+func loadAsync(_ n: Int) -> [Image] {
+    var imgs: [Image] = []
+    let group = DispatchGroup()
+    group.enter()
+    let queue = DispatchQueue(label: "myqueue")
+    queue.async {
+        for i in 1...n { imgs.append(sld.train[i].0) }
+        group.leave()
+    }
+    group.wait()
+    return imgs
+}
+
+time { let imgs = loadAsync(100) }
+
+
diff --git a/nbs/swift/08c_data_block-lightlyfunctional.ipynb b/nbs/swift/08c_data_block-lightlyfunctional.ipynb
index 7c89a22..f6396cf 100644
--- ./nbs/swift/08c_data_block-lightlyfunctional.ipynb
+++ ./nbs/swift/08c_data_block-lightlyfunctional.ipynb
@@ -1128,6 +1128,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/08c_data_block-lightlyfunctional.py b/nbs/swift/08c_data_block-lightlyfunctional.py
new file mode 100644
index 0000000..3cc9fb9
--- /dev/null
+++ ./nbs/swift/08c_data_block-lightlyfunctional.py
@@ -0,0 +1,481 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+# # Data block foundations, in Swifty/functional style
+
+# Uncomment line below when using Colab (this installs OpenCV4)
+# %system SwiftCV/install/install_colab.sh
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/FastaiNotebook_07_batchnorm")' FastaiNotebook_07_batchnorm
+%install '.package(path: "$cwd/SwiftCV")' SwiftCV
+
+import Path
+import TensorFlow
+import Python
+
+import FastaiNotebook_07_batchnorm
+
+%include "EnableIPythonDisplay.swift"
+IPythonDisplay.shell.enable_matplotlib("inline")
+
+# ## DataBlock-like manipulation in a lightweight functional, Swifty style
+
+# The DataBlock API in Python is designed to help with the routine data manipulations involved in modelling: downloading data, loading it given an understanding of its layout on the filesystem, processing it, and feeding it into an ML framework like fastai. This is a data pipeline. How do we do this in Swift?
+#
+# One approach is to build a set of types (structs, protocols, etc.) which represent various stages of this pipeline. By making the types generic, we could build a library that handled data for many kinds of models. However, it is sometimes a good rule of thumb, before writing generic types, to start by writing concrete types and then to notice what to abstract into a generic later. And another good rule of thumb, before writing concrete types, is to write no types at all, and to see how far you can get with a more primitive tool for composition: functions.
+#
+# This notebook shows how to perform DataBlock-like operations using a _lightweight functional style_. This means, first, to rely as much as possible on _pure_ functions -- that is, functions which do nothing but return outputs based on their inputs, and which don't mutate values anywhere. Second, in particular, it means to use Swift's support for _higher-order functions_ (functions which take functions, like `map`, `filter`, `reduce`, and `compose`). Finally, this example relies on _tuples_. Like structs, tuples can have named, typed properties. Unlike structs, you don't need to name them. They can be a fast, ad-hoc way to explore the data types that you actually need, without being distracted by considering what's a method, an initializer, etc.,
+#
+# Swift has excellent, understated support for a such a style. 
+
+# ## Getting Started
+#
+# First things first, we need to download Imagenette and untar it. What follows is very close to what we did for MNIST.
+
+public let dataPath = Path.home/".fastai"/"data"
+
+public func downloadImagenette(path: Path = dataPath, sz:String="-320") -> Path {
+    let url = "https://s3.amazonaws.com/fast-ai-imageclas/imagenette\(sz).tgz"
+    let fname = "imagenette\(sz)"
+    let file = path/fname
+    try! path.mkdir(.p)
+    if !file.exists {
+        downloadFile(url, dest:(path/"\(fname).tgz").string)
+        _ = "/bin/tar".shell("-xzf", (path/"\(fname).tgz").string, "-C", path.string)
+    }
+    return file
+}
+
+# ### Defining Imagenette configurations
+#
+# Here is what we know ahead of time about how imagenette data is laid out on disk:
+#
+# ```
+# .
+# └── data                                           # <-- this is the fastai data root path
+#     ├── imagenette-160                             # <-- this is the imagenette dataset path
+#     │   ├── train                                  # <-- the train/ and val/ dirs are our two segments
+#     │   │   ├── n01440764                          # <-- this is an image category _label_
+#     │   │   │   ├── n01440764_10026.JPEG           # <-- this is an image (a _sample_) with that label
+#     │   │   │   ├── n01440764_10027.JPEG
+#     │   │   │   ├── n01440764_10042.JPEG
+#    ...
+#     │   ├── val
+#     │       └── n03888257
+#     │           ├── ILSVRC2012_val_00001440.JPEG
+#     │           ├── ILSVRC2012_val_00002508.JPEG
+#    ...  
+#
+# ```
+#
+#
+# We will define one type, an `enum`, to capture this information.
+#
+# This "empty" `enum` will serve only as a namespace, a grouping, for pure functions representing this information. By putting this information into one type, our code is more modular: it more clearly distinguishes facts about _this dataset_, from _general purpose data manipulators_, from _computations for this analysis_.
+#
+# Here's our Imagenette configuration type:
+
+// export
+enum ImageNette
+{
+  /// Downloads imagenette given the fastai data, and returns its "dataset root"
+  static func download() -> Path { return downloadImagenette() }
+
+  /// Extensions of paths which represent imagenette samples (i.e., items)
+  static var extensions:[String] = ["jpeg", "jpg"]
+
+  // Returns whether an image is in the training set (vs validation set), based on its path
+  static func isTraining(_ p:Path) -> Bool {
+    return p.parent.parent.basename() == "train"
+  }
+
+  /// Returns an image's label given the image's path
+  static func labelOf(_ p:Path) -> String { return p.parent.basename() }
+}
+
+# ### Download Imagenette
+
+let path = ImageNette.download()
+
+# After download, this `path` is the first _value_, and the remaining steps of analysis can all be seen as applying functions which successively compute new values from past values.
+#
+# For instance, this path and the allowed files extensions are the _input_ to the function `collectFilePaths(under:filteringToExtensions)` which _outputs_ an array of all samples in the dataet. (This is like `fetchFiles` but we rename it here to to emphasize the conventional functional operation of "filtering" and to reflect that it does not actually fetch files over the network:
+
+//export
+public func collectFiles(under path: Path, recurse: Bool = false, filtering extensions: [String]? = nil) -> [Path] {
+    var res: [Path] = []
+    for p in try! path.ls(){
+        if p.kind == .directory && recurse { 
+            res += collectFiles(under: p.path, recurse: recurse, filtering: extensions)
+        } else if extensions == nil || extensions!.contains(p.path.extension.lowercased()) {
+            res.append(p.path)
+        }
+    }
+    return res
+}
+
+# Now we compute the next value, the array of all paths.
+
+var allPaths = collectFiles(under: path, recurse: true, filtering: ImageNette.extensions)
+
+# If we look at a random element, compared to the imagenette root, it has the filesystem layout structure we expected
+
+(path.string, allPaths.randomElement()!.string)
+
+# Let us verify that our configurations functions correctly encode the segment (train or val) and the label of an arbitrary item:
+
+func describeSample(_ path:Path) 
+{
+    let isTraining = ImageNette.isTraining(path)
+    let label = ImageNette.labelOf(path)
+    print("""
+          path: \(path.string)
+          training?:  \(isTraining)
+          label: \(label)
+          """)
+}
+
+
+describeSample(allPaths.randomElement()!)
+
+# We can see that our functions for _path->isTraining_ and _path->label_ are working as expected.
+
+# ### Split the data
+
+# Now we want to split our samples into a training and validation sets. Since this is so routine we define a standard function that does so.
+#
+# It is enough to take an array and returns a named tuple of two arrays, one for training and one for validation.
+
+# +
+// export
+
+/// takes a [T] of items, and returns a tuple (train:[T],val:[T]) items
+func partitionIntoTrainVal<T>(_ items:[T],isTrain:((T)->Bool)) -> (train:[T],valid:[T]){
+    return (train: items.filter(isTrain), valid: items.filter { !isTrain($0) })
+}
+# -
+
+# We pass the `ImageNette.isTraining` test function into the partitioner directly
+
+var samples = partitionIntoTrainVal(allPaths, isTrain:ImageNette.isTraining)
+
+# And verify that it works as expected:
+
+describeSample(samples.valid.randomElement()!)
+
+describeSample(samples.train.randomElement()!)
+
+# ### Process the data
+#
+# We process the data by taking all training labels, uniquing them, sorting them, and then defining an integer to represent the label.
+#
+# Those numerical labels let us define two functions, a function for label->number and the inverse function number->label.
+#
+# But notable point is that the process that produces those functions _is also a function_: the input is a list of training labels, and the output is the label<->number bidirectional mappings.
+#
+# That function which creates the bidirectional mapping is just the initializer of the String<->Int mapper we define below:
+
+// export
+/// Defines bidirectional maps from String <-> Int32, initialized from a collection of Strings
+public struct StringIntMapper {
+  private(set) public var labelMap:[String]
+  private(set) public var inverseLabelMap:[String:Int]
+  public init<S:Sequence>(labels ls:S) where S.Element == String {
+    labelMap = Array(Set(ls)).sorted()
+    inverseLabelMap = Dictionary(uniqueKeysWithValues:
+      labelMap.enumerated().map({ ($0.element, $0.offset) }))
+  }
+  public func labelToInt(_ label:String) -> Int { return inverseLabelMap[label]! }
+  public func intToLabel(_ labelIndex:Int) -> String { return labelMap[labelIndex] }
+}
+
+# Let us create a labelNumber mapper from the training data. First we use the function `labelOf` to get all the training labels, then we can initialize a `StringIntMapper`.
+
+var trainLabels = samples.train.map(ImageNette.labelOf)
+var labelMapper = StringIntMapper(labels: trainLabels)
+
+# The labelMapper now supplies the two bidirectional functions. We can verify they have the required inverse relationship:
+
+var randomLabel = labelMapper.labelMap.randomElement()!
+print("label = \(randomLabel)")
+var numericalizedLabel = labelMapper.labelToInt(randomLabel)
+print("number = \(numericalizedLabel)")
+var labelFromNumber = labelMapper.intToLabel(numericalizedLabel)
+print("label = \(labelFromNumber)")
+
+# ### Label the data
+
+# Now we are in a position to give the data numerical labels.
+#
+# Now in order to map from a sample item (a `Path`), to a numerical label (an `Int32)`, we just compose our Path->label function with a label->int function. Curiously, Swift does not define its own compose function, so we defined a `compose` operator `>|` ourselves. We can use it to create our new function as a composition explicitly:
+
+// export
+public func >| <A, B, C>(_ f: @escaping (A) -> B,
+                   _ g: @escaping (B) -> C) -> (A) -> C {
+    return { g(f($0)) }
+}
+
+# The we define a function which map a raw sample (`Path`) to a numericalized label (`Int`)
+
+var pathToNumericalizedLabel = ImageNette.labelOf >| labelMapper.labelToInt
+
+# Now we can, if we wish, compute numericalized labels over all the training and validation items:
+
+var trainNumLabels = samples.train.map(pathToNumericalizedLabel)
+var validNumLabels = samples.valid.map(pathToNumericalizedLabel)
+
+# We've gotten pretty far just using mostly just variables, functions, and function composition. But one downside is that our results are now scattered over a few different variables, `samples`, `trainNumLabels`, `valNumLabels`. We collect these values into one structure for convenience:
+
+struct SplitLabeledData {
+    var mapper: StringIntMapper
+    var train: [(x: Path, y: Int)]
+    var valid: [(x: Path, y: Int)]
+    
+    init(mapper: StringIntMapper, train: [(x: Path, y: Int)], valid: [(x: Path, y: Int)]) {
+        (self.mapper,self.train,self.valid) = (mapper,train,valid)
+    }
+}
+
+# And we can define a convenience init to build this directly from the filenames.
+
+extension SplitLabeledData {
+    init(paths:[Path]){
+        let samples = partitionIntoTrainVal(paths, isTrain:ImageNette.isTraining)
+        let trainLabels = samples.train.map(ImageNette.labelOf)
+        var labelMapper = StringIntMapper(labels:trainLabels)
+        let pathToNumericalizedLabel = ImageNette.labelOf >| labelMapper.labelToInt
+        self.init(mapper: labelMapper,
+                  train:  samples.train.map { ($0, pathToNumericalizedLabel($0)) },
+                  valid:  samples.valid.map { ($0, pathToNumericalizedLabel($0)) })
+    }
+}
+
+let allPaths = collectFiles(under: path, recurse: true, filtering: ImageNette.extensions)
+let sld = SplitLabeledData(paths: allPaths)
+
+# ### opening images
+
+# We can use the same compose approach to convert our images from `Path` filenames to resized images.
+
+import Foundation
+import SwiftCV
+
+# First let's open those images with openCV:
+
+func openImage(_ fn: Path) -> Mat {
+    return imdecode(try! Data(contentsOf: fn.url))
+}
+
+# And add a convenience function to have a look.
+
+func showCVImage(_ img: Mat) {
+    let tensImg = Tensor<UInt8>(cvMat: img)!
+    let numpyImg = tensImg.makeNumpyArray()
+    plt.imshow(numpyImg) 
+    plt.axis("off")
+    plt.show()
+}
+
+showCVImage(openImage(sld.train.randomElement()!.x))
+
+# The channels are in BGR instead of RGB so we first switch them with openCV
+
+func BGRToRGB(_ img: Mat) -> Mat {
+    return cvtColor(img, nil, ColorConversionCode.COLOR_BGR2RGB)
+}
+
+# Then we can resize them
+
+func resize(_ img: Mat, size: Int) -> Mat {
+    return resize(img, nil, Size(size, size), 0, 0, InterpolationFlag.INTER_LINEAR)
+}
+
+# With our compose operator, the succession of transforms can be written in this pretty way:
+
+let transforms = openImage >| BGRToRGB >| { resize($0, size: 224) }
+
+# And we can have a look at one of our elements:
+
+showCVImage(transforms(sld.train.randomElement()!.x))
+
+# ## Conversion to Tensor and batching
+
+# Now we will need tensors to train our model, so we need to convert our images and ints to tensors.
+
+func cvImgToTensorInt(_ img: Mat) -> Tensor<UInt8> {
+    return Tensor<UInt8>(cvMat: img)!
+}
+
+# We compose our transforms with that last function to get tensors.
+
+let pathToTF = transforms >| cvImgToTensorInt
+
+func intTOTI(_ i: Int) -> TI { return TI(Int32(i)) } 
+
+# Now we define a `Batcher` that will be responsible for creating minibatches as an iterator. It has the properties you know from PyTorch (batch size, num workers, shuffle) and will use multiprocessing to gather the images in parallel.
+#
+# To be able to write `for batch in Batcher(...)`, `Batcher` needs to conform to `Sequence`, which means it needs to have a `makeIterator` function. That function has to return another struct that conforms to `IteratorProtocol`. The only thing required there is a `next` property that returns the next batch (or `nil` if we are finished).
+#
+# The code is pretty striaghtforward: we shuffle the dataset at each beginning of iteration if we want, then we apply the transforms in parallel with the use of `concurrentMap`, that works just like map but with `numWorkers` processes.
+
+# +
+struct Batcher: Sequence {
+    let dataset: [(Path, Int)]
+    let xToTensor: (Path) -> Tensor<UInt8>
+    let yToTensor: (Int) ->  TI
+    var bs: Int = 64
+    var numWorkers: Int = 4
+    var shuffle: Bool = false
+    
+    init(_ ds: [(Path, Int)], xToTensor: @escaping (Path) -> Tensor<UInt8>, yToTensor: @escaping (Int) ->  TI,
+         bs: Int = 64, numWorkers: Int = 4, shuffle: Bool = false) {
+        (dataset,self.xToTensor,self.yToTensor,self.bs) = (ds,xToTensor,yToTensor,bs)
+        (self.numWorkers,self.shuffle) = (numWorkers,shuffle)
+    }
+    
+    func makeIterator() -> BatchIterator { 
+        return BatchIterator(self, numWorkers: numWorkers, shuffle: shuffle)
+    }
+}
+
+struct BatchIterator: IteratorProtocol {
+    let b: Batcher
+    var numWorkers: Int = 4
+    private var idx: Int = 0
+    private var ds: [(Path, Int)]
+    
+    init(_ batcher: Batcher, numWorkers: Int = 4, shuffle: Bool = false){ 
+        (b,self.numWorkers,idx) = (batcher,numWorkers,0) 
+        self.ds = shuffle ? b.dataset.shuffled() : b.dataset
+    }
+    
+    mutating func next() -> (xb:TF, yb:TI)? {
+        guard idx < b.dataset.count else { return nil }
+        let end = idx + b.bs < b.dataset.count ? idx + b.bs : b.dataset.count 
+        let samples = Array(ds[idx..<end])
+        idx += b.bs
+        return (xb: TF(Tensor<UInt8>(concatenating: samples.concurrentMap(nthreads: numWorkers) { 
+            self.b.xToTensor($0.0).expandingShape(at: 0) }))/255.0, 
+                yb: TI(concatenating: samples.concurrentMap(nthreads: numWorkers) { 
+            self.b.yToTensor($0.1).expandingShape(at: 0) }))
+    }
+    
+}
+# -
+
+SetNumThreads(0)
+
+let batcher = Batcher(sld.train, xToTensor: pathToTF, yToTensor: intTOTI, bs:256, shuffle:true)
+
+time {var c = 0
+      for batch in batcher { c += 1 }
+     }
+
+let firstBatch = batcher.first(where: {_ in true})!
+
+func showTensorImage(_ img: TF) {
+    let numpyImg = img.makeNumpyArray()
+    plt.imshow(numpyImg) 
+    plt.axis("off")
+    plt.show()
+}
+
+showTensorImage(firstBatch.xb[0])
+
+# ### With fast collate
+
+# +
+public protocol Countable {
+    var count:Int {get}
+}
+extension Mat  :Countable {}
+extension Array:Countable {}
+
+public extension Sequence where Element:Countable {
+    var totalCount:Int { return map{ $0.count }.reduce(0, +) }
+}
+# -
+
+func collateMats(_ imgs:[Mat]) -> TF {
+    let c = imgs.totalCount
+    let ptr = UnsafeMutableRawPointer.allocate(byteCount: c, alignment: 1)
+    defer {ptr.deallocate()}
+    var p = ptr
+    for img in imgs {
+        p.copyMemory(from: img.dataPtr, byteCount: img.count)
+        p += img.count
+    }
+    let r = UnsafeBufferPointer(start: ptr.bindMemory(to: UInt8.self, capacity: c), count: c)
+    let cvImg = imgs[0]
+    let shape = TensorShape([imgs.count, cvImg.rows, cvImg.cols, cvImg.channels])
+    let res = Tensor(shape: shape, scalars: r)
+    return Tensor<Float>(res)/255.0
+}
+
+# +
+struct Batcher1: Sequence {
+    let dataset: [(Path, Int)]
+    let xToTensor: (Path) -> Mat
+    let collateFunc: ([Mat]) -> TF
+    let yToTensor: (Int) ->  TI
+    var bs: Int = 64
+    var numWorkers: Int = 4
+    var shuffle: Bool = false
+    
+    init(_ ds: [(Path, Int)], xToTensor: @escaping (Path) -> Mat, collateFunc: @escaping ([Mat]) -> TF, 
+         yToTensor: @escaping (Int) ->  TI,
+         bs: Int = 64, numWorkers: Int = 4, shuffle: Bool = false) {
+        (dataset,self.xToTensor,self.collateFunc,self.yToTensor) = (ds,xToTensor,collateFunc,yToTensor)
+        (self.bs,self.numWorkers,self.shuffle) = (bs,numWorkers,shuffle)
+    }
+    
+    func makeIterator() -> BatchIterator1 { 
+        return BatchIterator1(self, numWorkers: numWorkers, shuffle: shuffle)
+    }
+}
+
+struct BatchIterator1: IteratorProtocol {
+    let b: Batcher1
+    var numWorkers: Int = 4
+    private var idx: Int = 0
+    private var ds: [(Path, Int)]
+    
+    init(_ batcher: Batcher1, numWorkers: Int = 4, shuffle: Bool = false){ 
+        (b,self.numWorkers,idx) = (batcher,numWorkers,0) 
+        self.ds = shuffle ? b.dataset.shuffled() : b.dataset
+    }
+    
+    mutating func next() -> (xb:TF, yb:TI)? {
+        guard idx < b.dataset.count else { return nil }
+        let end = idx + b.bs < b.dataset.count ? idx + b.bs : b.dataset.count 
+        let samples = Array(ds[idx..<end])
+        idx += b.bs
+        return (xb: b.collateFunc(samples.concurrentMap(nthreads: numWorkers) { 
+            self.b.xToTensor($0.0) }), 
+                yb: TI(concatenating: samples.concurrentMap(nthreads: numWorkers) { 
+            self.b.yToTensor($0.1).expandingShape(at: 0) }))
+    }
+    
+}
+# -
+
+let batcher1 = Batcher1(sld.train, xToTensor: transforms, collateFunc: collateMats, yToTensor: intTOTI, bs:256, shuffle:true)
+
+time {var c = 0
+      for batch in batcher1 { c += 1 }
+     }
+
+
diff --git a/nbs/swift/08c_data_block_generic.ipynb b/nbs/swift/08c_data_block_generic.ipynb
index 57f872b..4aaa5c2 100644
--- ./nbs/swift/08c_data_block_generic.ipynb
+++ ./nbs/swift/08c_data_block_generic.ipynb
@@ -1049,6 +1049,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/08c_data_block_generic.py b/nbs/swift/08c_data_block_generic.py
new file mode 100644
index 0000000..6fb0287
--- /dev/null
+++ ./nbs/swift/08c_data_block_generic.py
@@ -0,0 +1,440 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+# # Data block foundations, in Swifty/functional style
+
+# Uncomment line below when using Colab (this installs OpenCV4)
+# %system SwiftCV/install/install_colab.sh
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/FastaiNotebook_07_batchnorm")' FastaiNotebook_07_batchnorm
+%install '.package(path: "$cwd/SwiftCV")' SwiftCV
+
+//export
+import Path
+import TensorFlow
+import Python
+
+import FastaiNotebook_07_batchnorm
+
+%include "EnableIPythonDisplay.swift"
+IPythonDisplay.shell.enable_matplotlib("inline")
+
+# ## DataBlock-like manipulation in a lightweight functional, Swifty style
+
+# The DataBlock API in Python is designed to help with the routine data manipulations involved in modelling: downloading data, loading it given an understanding of its layout on the filesystem, processing it, and feeding it into an ML framework like fastai. This is a data pipeline. How do we do this in Swift?
+#
+# One approach is to build a set of types (structs, protocols, etc.) which represent various stages of this pipeline. By making the types generic, we could build a library that handled data for many kinds of models. However, it is sometimes a good rule of thumb, before writing generic types, to start by writing concrete types and then to notice what to abstract into a generic later. And another good rule of thumb, before writing concrete types, is to write no types at all, and to see how far you can get with a more primitive tool for composition: functions.
+#
+# This notebook shows how to perform DataBlock-like operations using a _lightweight functional style_. This means, first, to rely as much as possible on _pure_ functions -- that is, functions which do nothing but return outputs based on their inputs, and which don't mutate values anywhere. Second, in particular, it means to use Swift's support for _higher-order functions_ (functions which take functions, like `map`, `filter`, `reduce`, and `compose`). Finally, this example relies on _tuples_. Like structs, tuples can have named, typed properties. Unlike structs, you don't need to name them. They can be a fast, ad-hoc way to explore the data types that you actually need, without being distracted by considering what's a method, an initializer, etc.,
+#
+# Swift has excellent, understated support for a such a style. 
+
+# ## Getting Started
+#
+# First things first, we need to download Imagenette and untar it. What follows is very close to what we did for MNIST.
+
+//export
+public let dataPath = Path.home/".fastai"/"data"
+
+//export
+public func downloadImagenette(path: Path = dataPath, sz:String="-320") -> Path {
+    let url = "https://s3.amazonaws.com/fast-ai-imageclas/imagenette\(sz).tgz"
+    let fname = "imagenette\(sz)"
+    let file = path/fname
+    try! path.mkdir(.p)
+    if !file.exists {
+        downloadFile(url, dest:(path/"\(fname).tgz").string)
+        _ = "/bin/tar".shell("-xzf", (path/"\(fname).tgz").string, "-C", path.string)
+    }
+    return file
+}
+
+# Then we write a function to collect all the files in a directory, recursively.
+
+//export
+public func collectFiles(under path: Path, recurse: Bool = false, filtering extensions: [String]? = nil) -> [Path] {
+    var res: [Path] = []
+    for p in try! path.ls(){
+        if p.kind == .directory && recurse { 
+            res += collectFiles(under: p.path, recurse: recurse, filtering: extensions)
+        } else if extensions == nil || extensions!.contains(p.path.extension.lowercased()) {
+            res.append(p.path)
+        }
+    }
+    return res
+}
+
+# To build our dataset, we need, at the core, only four functions that tells us:
+# - how do we dowload the dataset
+# - how do we get the inputs
+# - how do we split them between train and valid
+# - how do we label them
+#
+# We put those four requirements in a `DatasetConfig` protocol.
+
+//export
+public protocol DatasetConfig {
+    associatedtype Item
+    associatedtype Label
+    
+    static func download() -> Path
+    static func getItems(_ path: Path) -> [Item]
+    static func isTraining(_ item: Item) -> Bool
+    static func labelOf(_ item: Item) -> Label
+}
+
+# Here is what we know ahead of time about how imagenette data is laid out on disk:
+#
+# ```
+# .
+# └── data                                           # <-- this is the fastai data root path
+#     ├── imagenette-160                             # <-- this is the imagenette dataset path
+#     │   ├── train                                  # <-- the train/ and val/ dirs are our two segments
+#     │   │   ├── n01440764                          # <-- this is an image category _label_
+#     │   │   │   ├── n01440764_10026.JPEG           # <-- this is an image (a _sample_) with that label
+#     │   │   │   ├── n01440764_10027.JPEG
+#     │   │   │   ├── n01440764_10042.JPEG
+#    ...
+#     │   ├── val
+#     │       └── n03888257
+#     │           ├── ILSVRC2012_val_00001440.JPEG
+#     │           ├── ILSVRC2012_val_00002508.JPEG
+#    ...  
+#
+# ```
+#
+#
+# We will define one type, an `enum`, to capture this information.
+#
+# This "empty" `enum` will serve only as a namespace, a grouping, for pure functions representing this information. By putting this information into one type, our code is more modular: it more clearly distinguishes facts about _this dataset_, from _general purpose data manipulators_, from _computations for this analysis_.
+#
+# Here's our Imagenette configuration type:
+
+//export
+public enum ImageNette: DatasetConfig {
+    
+    public static func download() -> Path { return downloadImagenette() }
+    
+    public static func getItems(_ path: Path) -> [Path] {
+        return collectFiles(under: path, recurse: true, filtering: ["jpeg", "jpg"])
+    }
+    
+    public static func isTraining(_ p:Path) -> Bool {
+        return p.parent.parent.basename() == "train"
+    }
+    
+    public static func labelOf(_ p:Path) -> String { return p.parent.basename() }
+}
+
+
+# From this configuration, we can get values by calling the `download`  and `getItems` function. This step would be exactly the same for all datasets following the `DatasetConfig` protocol:
+
+let path = ImageNette.download()
+let allFnames = ImageNette.getItems(path)
+
+# This function will use our dataset configuration to describe a given item:
+
+//export
+public func describeSample<C>(_ item: C.Item, config: C.Type) where C: DatasetConfig {
+    let isTraining = C.isTraining(item)
+    let label = C.labelOf(item)
+    print("""
+          item: \(item)
+          training?:  \(isTraining)
+          label: \(label)
+          """)
+}
+
+describeSample(allFnames[0], config: ImageNette.self)
+
+# We can see that our functions for _path->isTraining_ and _path->label_ are working as expected.
+
+# ### Split the data
+
+# Now we want to split our samples into a training and validation sets. Since this is so routine we define a standard function that does so.
+#
+# It is enough to take an array and returns a named tuple of two arrays, one for training and one for validation.
+
+//export
+public func partitionIntoTrainVal<T>(_ items:[T],isTrain:((T)->Bool)) -> (train:[T],valid:[T]){
+    return (train: items.filter(isTrain), valid: items.filter { !isTrain($0) })
+}
+
+var samples = partitionIntoTrainVal(allFnames, isTrain:ImageNette.isTraining)
+
+# And verify that it works as expected:
+
+describeSample(samples.valid.randomElement()!, config: ImageNette.self)
+
+describeSample(samples.train.randomElement()!, config: ImageNette.self)
+
+# ### Process the data
+#
+# We process the data by taking all training labels, uniquing them, sorting them, and then defining an integer to represent the label.
+#
+# Those numerical labels let us define two functions, a function for label->number and the inverse function number->label.
+#
+# But notable point is that the process that produces those functions _is also a function_: the input is a list of training labels, and the output is the label<->number bidirectional mappings.
+#
+# That function which creates the bidirectional mapping is called `initState` below. Those steps are generic and might be applied for other tasks, so we define another protocol for them.
+
+//export
+public protocol Processor {
+    associatedtype Input
+    associatedtype Output
+    
+    mutating func initState(_ items: [Input])
+    func process  (_ item: Input)  -> Output
+    func deprocess(_ item: Output) -> Input
+}
+
+# And the specific `CategoryProcessor` we need in this case.
+
+//export
+public struct CategoryProcessor: Processor {
+    private(set) public var intToLabel: [String] = []
+    private(set) public var labelToInt: [String:Int] = [:]
+    
+    public init() {}
+    
+    public mutating func initState(_ items: [String]) {
+        intToLabel = Array(Set(items)).sorted()
+        labelToInt = Dictionary(uniqueKeysWithValues:
+            intToLabel.enumerated().map{ ($0.element, $0.offset) })
+    }
+    
+    public func process(_ item: String) -> Int { return labelToInt[item]! }
+    public func deprocess(_ item: Int) -> String { return intToLabel[item] }
+}
+
+# Let us create a labelNumber mapper from the training data. First we use the function `labelOf` to get all the training labels, then we can initialize a `CategoryProcessor`.
+
+var trainLabels = samples.train.map(ImageNette.labelOf)
+var labelMapper = CategoryProcessor()
+labelMapper.initState(trainLabels)
+
+# The labelMapper now supplies the two bidirectional functions. We can verify they have the required inverse relationship:
+
+var randomLabel = labelMapper.intToLabel.randomElement()!
+print("label = \(randomLabel)")
+var numericalizedLabel = labelMapper.process(randomLabel)
+print("number = \(numericalizedLabel)")
+var labelFromNumber = labelMapper.deprocess(numericalizedLabel)
+print("label = \(labelFromNumber)")
+
+# ### Label the data
+
+# Now we are in a position to give the data numerical labels.
+#
+# Now in order to map from a sample item (a `Path`), to a numerical label (an `Int`), we just compose our Path->label function with a label->int function. Curiously, Swift does not define its own compose function, so we defined a `compose` operator `>|` ourselves. We can use it to create our new function as a composition explicitly:
+
+// export
+public func >| <A, B, C>(_ f: @escaping (A) -> B,
+                   _ g: @escaping (B) -> C) -> (A) -> C {
+    return { g(f($0)) }
+}
+
+# The we define a function which map a raw sample (`Path`) to a numericalized label (`Int`)
+
+var pathToNumericalizedLabel = ImageNette.labelOf >| labelMapper.process
+
+# Now we can, if we wish, compute numericalized labels over all the training and validation items:
+
+var trainNumLabels = samples.train.map(pathToNumericalizedLabel)
+var validNumLabels = samples.valid.map(pathToNumericalizedLabel)
+
+# We've gotten pretty far just using mostly just variables, functions, and function composition. But one downside is that our results are now scattered over a few different variables, `samples`, `trainNumLabels`, `valNumLabels`. We collect these values into one structure for convenience:
+
+//export
+public struct SplitLabeledData<Item,Label> {
+    public var train: [(x: Item, y: Label)]
+    public var valid: [(x: Item, y: Label)]
+    
+    public init(train: [(x: Item, y: Label)], valid: [(x: Item, y: Label)]) {
+        (self.train,self.valid) = (train,valid)
+    }
+}
+
+# And we can define a convenience function to build it directly from our config and a processor.
+
+//export
+public func makeSLD<C, P>(config: C.Type, procL: inout P) -> SplitLabeledData<C.Item, P.Output> 
+where C: DatasetConfig, P: Processor, P.Input == C.Label{
+    let path = C.download()
+    let items = C.getItems(path)
+    let samples = partitionIntoTrainVal(items, isTrain:C.isTraining)
+    let trainLabels = samples.train.map(C.labelOf)
+    procL.initState(trainLabels)
+    let itemToProcessedLabel = C.labelOf >| procL.process
+    return SplitLabeledData(train: samples.train.map { ($0, itemToProcessedLabel($0)) },
+                            valid: samples.valid.map { ($0, itemToProcessedLabel($0)) })
+}
+
+var procL = CategoryProcessor()
+let sld = makeSLD(config: ImageNette.self, procL: &procL)
+
+# ### Opening images
+
+# We can use the same compose approach to convert our images from `Path` filenames to resized images, or add all the data augmentation we want.
+
+//export
+import Foundation
+import SwiftCV
+
+# First let's open those images with openCV:
+
+//export
+public func openImage(_ fn: Path) -> Mat {
+    return imdecode(try! Data(contentsOf: fn.url))
+}
+
+# And add a convenience function to have a look.
+
+//export
+public func showCVImage(_ img: Mat) {
+    let tensImg = Tensor<UInt8>(cvMat: img)!
+    let numpyImg = tensImg.makeNumpyArray()
+    plt.imshow(numpyImg) 
+    plt.axis("off")
+    plt.show()
+}
+
+showCVImage(openImage(sld.train.randomElement()!.x))
+
+# The channels are in BGR instead of RGB so we first switch them with openCV
+
+//export
+public func BGRToRGB(_ img: Mat) -> Mat {
+    return cvtColor(img, nil, ColorConversionCode.COLOR_BGR2RGB)
+}
+
+# Then we can resize them
+
+//export
+public func resize(_ img: Mat, size: Int) -> Mat {
+    return resize(img, nil, Size(size, size), 0, 0, InterpolationFlag.INTER_LINEAR)
+}
+
+# With our compose operator, the succession of transforms can be written in this pretty way:
+
+let transforms = openImage >| BGRToRGB >| { resize($0, size: 224) }
+
+# And we can have a look at one of our elements:
+
+showCVImage(transforms(sld.train.randomElement()!.x))
+
+# ## Conversion to Tensor and batching
+
+# Now we will need tensors to train our model, so we need to convert our images and ints to tensors. Images are naturally converted to tensor of bytes.
+
+//export
+public func cvImgToTensor(_ img: Mat) -> Tensor<UInt8> {
+    return Tensor<UInt8>(cvMat: img)!
+}
+
+# We compose our transforms with that last function to get tensors.
+
+let pathToTF = transforms >| cvImgToTensor
+
+//export
+public func intTOTI(_ i: Int) -> TI { return TI(Int32(i)) } 
+
+# Now we define a `Batcher` that will be responsible for creating minibatches as an iterator. It has the properties you know from PyTorch (batch size, num workers, shuffle) and will use multiprocessing to gather the images in parallel.
+#
+# To be able to write `for batch in Batcher(...)`, `Batcher` needs to conform to `Sequence`, which means it needs to have a `makeIterator` function. That function has to return another struct that conforms to `IteratorProtocol`. The only thing required there is a `next` property that returns the next batch (or `nil` if we are finished).
+#
+# The code is pretty straightforward: we shuffle the dataset at each beginning of iteration if we want, then we apply the transforms in parallel with the use of `concurrentMap`, that works just like map but with `numWorkers` processes.
+
+# +
+//export
+public struct Batcher<Item,Label,ScalarI: TensorFlowScalar,ScalarL: TensorFlowScalar>: Sequence {
+    public let dataset: [(Item, Label)]
+    public let xToTensor: (Item) -> Tensor<ScalarI>
+    public let yToTensor: (Label) -> Tensor<ScalarL>
+    public let collateFunc: (Tensor<ScalarI>, Tensor<ScalarL>) -> DataBatch<TF, TI>
+    public var bs: Int = 64
+    public var numWorkers: Int = 4
+    public var shuffle: Bool = false
+    
+    public init(_ ds: [(Item, Label)], 
+         xToTensor: @escaping (Item) -> Tensor<ScalarI>, 
+         yToTensor: @escaping (Label) ->  Tensor<ScalarL>,
+         collateFunc: @escaping (Tensor<ScalarI>, Tensor<ScalarL>) -> DataBatch<TF, TI>,
+         bs: Int = 64, numWorkers: Int = 4, shuffle: Bool = false) {
+        (dataset,self.xToTensor,self.yToTensor,self.collateFunc) = (ds,xToTensor,yToTensor,collateFunc)
+        (self.bs,self.numWorkers,self.shuffle) = (bs,numWorkers,shuffle)
+    }
+    
+    public func makeIterator() -> BatchIterator<Item,Label,ScalarI,ScalarL> { 
+        return BatchIterator(self, numWorkers: numWorkers, shuffle: shuffle)
+    }
+}
+
+public struct BatchIterator<Item,Label,ScalarI: TensorFlowScalar,ScalarL: TensorFlowScalar>: IteratorProtocol {
+    public let b: Batcher<Item,Label,ScalarI,ScalarL>
+    public var numWorkers: Int = 4
+    private var idx: Int = 0
+    private var ds: [(Item, Label)]
+    
+    public init(_ batcher: Batcher<Item,Label,ScalarI,ScalarL>, numWorkers: Int = 4, shuffle: Bool = false){ 
+        (b,self.numWorkers,idx) = (batcher,numWorkers,0) 
+        self.ds = shuffle ? b.dataset.shuffled() : b.dataset
+    }
+    
+    public mutating func next() -> DataBatch<TF,TI>? {
+        guard idx < b.dataset.count else { return nil }
+        let end = idx + b.bs < b.dataset.count ? idx + b.bs : b.dataset.count 
+        let samples = Array(ds[idx..<end])
+        idx += b.bs
+        return b.collateFunc(Tensor<ScalarI>(concatenating: samples.concurrentMap(nthreads: numWorkers) { 
+            self.b.xToTensor($0.0).expandingShape(at: 0) }), 
+                Tensor<ScalarL>(concatenating: samples.concurrentMap(nthreads: numWorkers) { 
+            self.b.yToTensor($0.1).expandingShape(at: 0) }))
+    }
+    
+}
+# -
+
+SetNumThreads(0)
+
+//export
+public func collateFunc(_ xb: Tensor<UInt8>, _ yb: TI) -> DataBatch<TF, TI> {
+    return DataBatch(xb: TF(xb)/255.0, yb: yb)
+}
+
+let batcher = Batcher(sld.train, xToTensor: pathToTF, yToTensor: intTOTI, collateFunc: collateFunc, bs:256, shuffle:true)
+
+time {var c = 0
+      for batch in batcher { c += 1 }
+     }
+
+let firstBatch = batcher.first(where: {_ in true})!
+
+//export
+func showTensorImage(_ img: TF) {
+    let numpyImg = img.makeNumpyArray()
+    plt.imshow(numpyImg) 
+    plt.axis("off")
+    plt.show()
+}
+
+showTensorImage(firstBatch.xb[0])
+
+import NotebookExport
+let exporter = NotebookExport(Path.cwd/"08c_data_block_generic.ipynb")
+print(exporter.export(usingPrefix: "FastaiNotebook_"))
+
+
diff --git a/nbs/swift/09_optimizer.ipynb b/nbs/swift/09_optimizer.ipynb
index 669d029..002c18b 100644
--- ./nbs/swift/09_optimizer.ipynb
+++ ./nbs/swift/09_optimizer.ipynb
@@ -1591,6 +1591,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/09_optimizer.py b/nbs/swift/09_optimizer.py
new file mode 100644
index 0000000..1813d42
--- /dev/null
+++ ./nbs/swift/09_optimizer.py
@@ -0,0 +1,615 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/FastaiNotebook_08a_heterogeneous_dictionary")' FastaiNotebook_08a_heterogeneous_dictionary
+
+// export
+import Path
+import TensorFlow
+
+import FastaiNotebook_08a_heterogeneous_dictionary
+
+%include "EnableIPythonDisplay.swift"
+IPythonDisplay.shell.enable_matplotlib("inline")
+
+# ## Load data
+
+let path = downloadImagenette()
+
+let il = ItemList(fromFolder: path, extensions: ["jpeg", "jpg"])
+let sd = SplitData(il, fromFunc: {grandParentSplitter(fName: $0, valid: "val")})
+var procLabel = CategoryProcessor()
+let sld = makeLabeledData(sd, fromFunc: parentLabeler, procLabel: &procLabel)
+let rawData = sld.toDataBunch(itemToTensor: pathsToTensor, labelToTensor: intsToTensor)
+let data = transformData(rawData, tfmItem: { openAndResize(fname: $0, size: 128) })
+
+func modelInit() -> CNNModel { return CNNModel(channelIn: 3, nOut: 10, filters: [64, 64, 128, 256]) }
+
+# ## Stateful optimizer
+
+# Before we begin, we create this structure to contain the names of our hyper-parameters. This will give us some tab completion and typo-proof way of handling them.
+
+//export
+public struct HyperParams {
+    public static let lr = "learningRate"
+}
+
+# Like in the python version, we create `statDelegates` that will be responsible for computing/updating statistics in the state (like the moving average of gradients) and `stepDelegates` that will be responsible for performing a part of the update of the weights. 
+#
+# In PyTorch we created a basic class with functions that needed to be implemented. In swift this is what protocols are for.
+
+# +
+//export
+public protocol StatDelegate {
+    var name: String {get}
+    var defaultHPs: [String:Float] {get}
+    
+    func update(_ state: inout [String:TF], p: TF, 𝛁p: TF, hps: inout [String:Float])
+}
+
+public protocol StepDelegate {
+    var defaultHPs: [String:Float] {get}
+    
+    func update(_ p: inout TF, 𝛁p: inout TF, state: [String:TF], hps: inout [String:Float])
+}
+# -
+
+# Those are helper functions to merge dictionaries that we'll use in the `StatefulOptimizer`.
+
+# +
+//export
+public func mergeDicts(_ dicts: inout [[String:Float]], with newDict: [String:Float]) {
+    for i in dicts.indices { 
+        dicts[i].merge(newDict) { (_, new) in new } 
+    }
+}
+
+public func mergeDicts(_ dicts: inout [[String:Float]], with newDicts: [[String:Float]]) {
+    for i in dicts.indices { 
+        dicts[i].merge(newDicts[i]) { (_, new) in new } 
+    }
+}
+# -
+
+# Those two extensions are there to initialize dicts easily.
+
+# +
+//export
+extension Dictionary where Value == Int{
+    public init(mapFromArrays arrays: [[Key]]){
+        self.init(uniqueKeysWithValues: arrays.enumerated().flatMap { i, arr in arr.map { ($0, i) } })
+    }
+}
+
+extension Dictionary {
+    public init(constant: Value, keys: [Key]){
+        self.init(uniqueKeysWithValues: keys.map { ($0, constant) })
+    }
+}
+# -
+
+# This is the initial state of our StatefulOptimizer. It's a dictionary keyPath (see below) to dictionary that maps names to tensor of floats.
+
+//export
+public func initState<Model: Layer>(for model: Model, names: [String]) 
+-> [WritableKeyPath<Model.AllDifferentiableVariables, TF>: [String:TF]] {
+    return [WritableKeyPath<Model.AllDifferentiableVariables, TF>: [String:TF]](
+        constant: [String: TF](constant: TF(0), keys: names),
+        keys: model.variables.keyPaths)
+}
+
+# And we can define the main `StatefulOptimizer`. It takes a model, hyperparameters for each parameter group, some `steppers` and `stats` and a `splitArray` that defines our different parameter groups.
+#
+# To understand how this work, you need to know a little bit about keyPaths. This is a tool in swift to access any elements in a nested structure like our models: one model typically has a few attributes that are modules which in turn contain other modules and so forth until we reach the primitives layers like `Conv2d` or `Dense`. KeyPaths will allow us to index in that nested structure the objects of a particular type. 
+#
+# For instance, the shortcut `keyPaths` you can apply to any `Layer` will find all the tensors of floats. If we apply it to a `Model.AllDifferentiableVariables` object, we will find all the parameters of the model (since `model.allDifferentiableVariables` only contain the trainable parameters).
+#
+# That's why the inner loop of our `StatefulOptimizer` is over `variables.keyPaths`. The same keyPath index will give us the gradients. Then we create a `state` to be a dictionary of such keyPaths to `[String:TF]` and the `splitArray` we provide is an array of different keyPaths (each giving us a parameter group) from which we build a `splitDict` that maps our keyPaths to the index of the corresponding group.
+
+//export
+public class StatefulOptimizer<Model: Layer>
+    where Model.AllDifferentiableVariables == Model.TangentVector {
+    public typealias ModelKeyPath = WritableKeyPath<Model.AllDifferentiableVariables, TF>
+    public typealias SplitDict = [ModelKeyPath: Int]
+    public var hpGroups: [[String:Float]]
+    public var splitDict: SplitDict
+    public var states: [ModelKeyPath: [String: TF]]
+    public var stats: [StatDelegate]
+    public var steppers: [StepDelegate]
+    public init(        
+        for model: __shared Model,
+        steppers: [StepDelegate],
+        stats: [StatDelegate],
+        hpGroups: [[String:Float]],
+        splitArray: [[ModelKeyPath]]
+    ) {
+        self.hpGroups = Array(repeating: [:], count: hpGroups.count)
+        (self.steppers,self.stats) = (steppers,stats)
+        self.splitDict = SplitDict(mapFromArrays: splitArray)
+        states = [:]
+        steppers.forEach { mergeDicts(&self.hpGroups, with: $0.defaultHPs) }
+        stats.forEach    { mergeDicts(&self.hpGroups, with: $0.defaultHPs) }
+        states = initState(for: model, names: stats.map { $0.name })
+        mergeDicts(&self.hpGroups, with: hpGroups)
+    }
+        
+    public func update(
+        _ variables: inout Model.AllDifferentiableVariables,
+        along direction: Model.TangentVector
+    ) {
+        for kp in variables.keyPaths {
+            var 𝛁p = direction[keyPath: kp]
+            var hps = hpGroups[splitDict[kp]!]
+            stats.forEach() { $0.update(&states[kp]!, p: variables[keyPath: kp], 𝛁p: 𝛁p, hps: &hps) }
+            steppers.forEach() { $0.update(&variables[keyPath: kp], 𝛁p: &𝛁p, state: states[kp]!, hps: &hps) }
+            hpGroups[splitDict[kp]!] = hps
+        }
+    }
+}
+
+# To make `StatefulOptimizer` conform to the `Optimizer` protocol, we need to add a `learningRate` property.
+
+//export
+extension StatefulOptimizer: Optimizer{
+    public var learningRate: Float {
+        get { return hpGroups.last![HyperParams.lr]! } 
+        set { 
+            for i in hpGroups.indices {self.hpGroups[i][HyperParams.lr] = newValue }
+        }
+    }
+    //For discriminative learning rates
+    public var learningRates: [Float] {
+        get { return hpGroups.map { $0[HyperParams.lr]! } }
+        set { 
+            for i in hpGroups.indices {self.hpGroups[i][HyperParams.lr] = newValue[i] } 
+        }
+    }
+}
+
+# When we don't have any parameter groups, we just use one with all the `keyPaths`. This convenience init automatically does that for us.
+
+//export
+extension StatefulOptimizer{
+    public convenience init (for model: __shared Model,
+                             steppers: [StepDelegate],
+                             stats: [StatDelegate],
+                             hps: [String:Float]) {
+        self.init(for: model,
+                  steppers: steppers,
+                  stats: stats,
+                  hpGroups: [hps],
+                  splitArray: [model.variables.keyPaths])
+    }
+}
+
+# We are now ready to define `steppers` and `stats`. Let's begin with basic SGD:
+
+//export
+public struct SGDStep: StepDelegate {
+    public var defaultHPs: [String: Float] { return [HyperParams.lr: 3e-3] }
+    public init() {}
+    public func update(_ p: inout TF, 𝛁p: inout TF, state: [String:TF], hps: inout [String:Float]) {
+        p -= 𝛁p * hps[HyperParams.lr]!
+    }
+}
+
+# We can check all is working and train:
+
+var hps: [String:Float] = [HyperParams.lr: 0.01]
+func optFunc(_ model: CNNModel) -> StatefulOptimizer<CNNModel> {
+    return StatefulOptimizer(for: model, steppers: [SGDStep()], stats: [], hps: hps)
+}
+
+var learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+var recorder = learner.makeDefaultDelegates(metrics: [accuracy])
+learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))
+
+learner.fit(1)
+
+# Then we can add weight decay and L2 regularization.
+
+# +
+//export
+public extension HyperParams {
+    static let wd = "weightDecay"
+}
+
+public struct WeightDecay: StepDelegate {
+    public var defaultHPs: [String: Float] { return [HyperParams.wd: 0] }
+    public init() {}
+    public func update(_ p: inout TF, 𝛁p: inout TF, state: [String:TF], hps: inout [String:Float]) {
+        p *= 1 - hps[HyperParams.lr]! * hps[HyperParams.wd]!
+    }
+}
+# -
+
+//export
+public struct L2Regularization: StepDelegate {
+    public var defaultHPs: [String: Float] { return [HyperParams.wd: 0] }
+    public init() {}
+    public func update(_ p: inout TF, 𝛁p: inout TF, state: [String:TF], hps: inout [String:Float]) {
+        𝛁p += hps[HyperParams.wd]! * p
+    }
+}
+
+# The next step is SGD with momentum. For this we need a statistic that keeps track of the moving average of the gradients.
+
+//export
+//Expandable enum to have tab completes/typo-proof for state variable names.
+public struct StateKeys {
+    public static let avgGrad = "averageGrad"
+}
+
+# +
+//export
+public extension HyperParams {
+    static let mom = "momentum"
+    static let momDamp = "dampening"
+}
+
+public struct AverageGrad: StatDelegate {
+    public var defaultHPs: [String: Float] { return [HyperParams.mom: 0.9] }
+    public let dampened: Bool
+    public init(dampened: Bool = false) { self.dampened = dampened }
+    public var name: String { return StateKeys.avgGrad }
+    public func update(_ state: inout [String: TF], p: TF, 𝛁p: TF, hps: inout [String:Float]) {
+        state[StateKeys.avgGrad]! *= hps[HyperParams.mom]!
+        hps[HyperParams.momDamp] = 1.0 - (dampened ? hps[HyperParams.mom]! : 0.0)
+        state[StateKeys.avgGrad]! += hps[HyperParams.momDamp]! * 𝛁p
+    }
+}
+# -
+
+//export
+public struct MomentumStep: StepDelegate {
+    public var defaultHPs: [String: Float] = [:]
+    public init() {}
+    public func update(_ p: inout TF, 𝛁p: inout TF, state: [String: TF], hps: inout [String:Float]) {
+        p -= state[StateKeys.avgGrad]! * hps[HyperParams.lr]!
+    }
+}
+
+# And we can check it trains properly.
+
+let hps: [String:Float] = [HyperParams.lr: 0.01]
+func optFunc(_ model: CNNModel) -> StatefulOptimizer<CNNModel> {
+    return StatefulOptimizer(for: model, steppers: [MomentumStep()], stats: [AverageGrad()], hps: hps)
+}
+
+var learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+var recorder = learner.makeDefaultDelegates(metrics: [accuracy])
+learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))
+
+learner.fit(1)
+
+# The hyper-parameters have taken the default values provided (except for learning rates).
+
+learner.opt.hpGroups[0]
+
+# The next step is Adam. For that we need to keep track of the averages of the gradients squared.
+
+# +
+//export
+public extension HyperParams {
+    static let ²mom = "momentumSquares"
+    static let ²momDamp = "dampeningSquares"
+}
+
+public extension StateKeys {
+    static let avgSqr = "averageSquaredGrad"
+}
+
+public struct AverageSquaredGrad: StatDelegate {
+    let dampened: Bool
+    public init(dampened: Bool = true) { self.dampened = dampened }
+    public var name: String { return StateKeys.avgSqr }
+    public var defaultHPs: [String: Float] { return [HyperParams.²mom: 0.99] }
+    public func update(_ state: inout [String: TF], p: TF, 𝛁p: TF, hps: inout [String:Float]) {
+        state[StateKeys.avgSqr]! *= hps[HyperParams.²mom]!
+        hps[HyperParams.²momDamp] = 1.0 - (dampened ? hps[HyperParams.²mom]! : 0.0)
+        state[StateKeys.avgSqr]! += hps[HyperParams.²momDamp]! * 𝛁p.squared()
+    }
+}
+# -
+
+# And we also need to keep track of the number of iterations we did.
+
+# +
+//export
+public extension StateKeys {
+    static let step = "stepCount"
+}
+
+public struct StepCount: StatDelegate {
+    public var name: String { return StateKeys.step }
+    public var defaultHPs: [String:Float] = [:]
+    public init() {}
+    public func update(_ state: inout [String: TF], p: TF, 𝛁p: TF, hps: inout [String:Float]) {
+        state[StateKeys.step]! += 1.0
+    }
+}
+# -
+
+//export
+//public struct Epsilon: HetDictKey { public static var defaultValue: Float = 1e-5 }
+public extension HyperParams {
+    static let eps = "epsilon"
+}
+
+//export
+public struct AdamStep: StepDelegate {
+    public var defaultHPs: [String: Float] { return [HyperParams.eps: 1e-5] }
+    public init() {}
+    public func update(_ p: inout TF, 𝛁p: inout TF, state: [String: TF], hps: inout [String:Float]) {
+        let step = state[StateKeys.step]!
+        let (mom,damp) = (hps[HyperParams.mom]!,hps[HyperParams.momDamp]!)
+        let debias1 = damp * (1 - pow(mom, step)) / (1 - mom)
+        let num = debias1 * state[StateKeys.avgGrad]!
+        
+        let (²mom,²damp) = (hps[HyperParams.²mom]!,hps[HyperParams.²momDamp]!)
+        let debias2 = ²damp * (1 - pow(²mom, step)) / (1 - ²mom)
+        let denom = sqrt(state[StateKeys.avgSqr]!/debias2) + hps[HyperParams.eps]!
+        
+        p -= hps[HyperParams.lr]! * num / denom
+    }
+}
+
+# Again let's check it's all training properly.
+
+func optFunc(_ model: CNNModel) -> StatefulOptimizer<CNNModel> {
+    return StatefulOptimizer(
+        for: model,
+        steppers: [AdamStep()], 
+        stats: [AverageGrad(dampened: true), AverageSquaredGrad(), StepCount()], 
+        hps: [HyperParams.lr: 1e-3])
+}
+
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+let recorder = learner.makeDefaultDelegates(metrics: [accuracy])
+learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))
+
+learner.fit(1)
+
+# We can also check the values of the hyper-parameters have been set properly.
+
+learner.opt.hpGroups[0]
+
+# Defining the Lamb optimizer is as easy as before.
+
+public struct LambStep: StepDelegate {
+    public var defaultHPs: [String: Float] { return [HyperParams.eps: 1e-6, HyperParams.wd: 0.0] }
+    public func update(_ p: inout TF, 𝛁p: inout TF, state: [String: TF], hps: inout [String:Float]) {
+        let stepCount = state[StateKeys.step]!
+        let (mom,damp) = (hps[HyperParams.mom]!,hps[HyperParams.momDamp]!)
+        let debias1 = damp * (1 - pow(mom, stepCount)) / (1 - mom)
+        let num = debias1 * state[StateKeys.avgGrad]!
+        
+        let (²mom,²damp) = (hps[HyperParams.²mom]!,hps[HyperParams.²momDamp]!)
+        let debias2 = ²damp * (1 - pow(²mom, stepCount)) / (1 - ²mom)
+        let denom = sqrt(state[StateKeys.avgSqr]!/debias2) + hps[HyperParams.eps]!
+        
+        let step = num / denom + hps[HyperParams.wd]! * p
+        let r1 = sqrt((p * p).mean())
+        let r2 = sqrt((step * step).mean())
+        let factor = min(r1 / r2, Float(10.0))
+        p -= hps[HyperParams.lr]! * factor * step
+    }
+}
+
+# ### Making convenience functions
+
+# To easily create our optimizers, we have two convenience functions.
+
+// export
+public func sgdOpt<Model>(lr: Float, mom: Float = 0.9, wd: Float = 0.0, dampening: Bool = false
+                         ) -> ((Model) -> StatefulOptimizer<Model>) {
+    var steppers: [StepDelegate] = (mom != 0) ? [MomentumStep()] : [SGDStep()]
+    if wd != 0 { steppers.append(WeightDecay()) }
+    let stats = (mom != 0) ? [AverageGrad(dampened: dampening)] : []
+    var hps: [String: Float] = [HyperParams.lr: lr]
+    if mom != 0 { hps[HyperParams.mom] = mom }
+    if wd != 0  { hps[HyperParams.wd ] = wd  }
+    return {model in 
+        return StatefulOptimizer(for: model, steppers: steppers, stats: stats, hps: hps)}
+}
+
+// export
+public func adamOpt<Model>(lr: Float, mom: Float = 0.9, beta: Float=0.99, wd: Float = 0.0, eps: Float = 1e-5
+                         ) -> ((Model) -> StatefulOptimizer<Model>) {
+    var steppers: [StepDelegate] = [AdamStep()]
+    if wd != 0 { steppers.append(WeightDecay()) }
+    let stats: [StatDelegate] = [AverageGrad(dampened: true), AverageSquaredGrad(), StepCount()]
+    var hps: [String: Float] = [HyperParams.lr: lr]
+    hps[HyperParams.mom] = mom
+    hps[HyperParams.²mom] = beta
+    hps[HyperParams.eps] = eps
+    if wd != 0  { hps[HyperParams.wd ] = wd  }
+    return {model in 
+        return StatefulOptimizer(for: model, steppers: steppers, stats: stats, hps: hps)}
+}
+
+# ### Schedule the hyperparams
+
+# The next thing is that we need to schedule our hyper-parameters. The following function allows us to schedule any of them, as long as they are present in the `hpGroups` dictionaries.
+
+// export
+public extension StatefulOptimizer {
+    func setParam(_ hp: String, _ val: Float) {
+        for i in 0..<hpGroups.count { hpGroups[i][hp] = val }
+    }
+}
+
+// export
+extension Learner where Opt.Scalar: BinaryFloatingPoint, 
+    Opt.Model.AllDifferentiableVariables == Opt.Model.TangentVector{
+    public class ParamScheduler: Delegate {
+        public override var order: Int { return 1 }
+        public typealias ScheduleFunc = (Float) -> Float
+
+        // A learning rate schedule from step to float.
+        public var scheduler: ScheduleFunc
+        public let hp: String
+        
+        public init(scheduler: @escaping (Float) -> Float, hp: String) {
+            (self.scheduler,self.hp) = (scheduler,hp)
+        }
+        
+        override public func batchWillStart(learner: Learner) {
+            let val = scheduler(learner.pctEpochs/Float(learner.epochCount))
+            (learner.opt as! StatefulOptimizer<Opt.Model>).setParam(hp, val)
+        }
+    }
+    
+    public func makeParamScheduler(_ scheduler: @escaping (Float) -> Float, hp: String) -> ParamScheduler {
+        return ParamScheduler(scheduler: scheduler, hp: hp)
+    }
+}
+
+# We can then define a helper function to schedule a 1cycle policy.
+
+// export 
+public func oneCycleSchedulers(_ lrMax: Float, pctStart:Float=0.25, divStart: Float = 10, divEnd: Float = 1e5, 
+                               moms: (Float,Float,Float) = (0.95,0.85,0.95)) 
+-> ((Float) -> Float, (Float) -> Float){
+    let lrSched = combineSchedules(
+        pcts: [pctStart, 1-pctStart], 
+        schedules: [makeAnnealer(start: lrMax/divStart, end: lrMax, schedule: cosineSchedule),
+                    makeAnnealer(start: lrMax, end: lrMax/divEnd, schedule: cosineSchedule)])
+    let momSched = combineSchedules(
+        pcts: [pctStart, 1-pctStart], 
+        schedules: [makeAnnealer(start: moms.0, end: moms.1, schedule: cosineSchedule),
+                    makeAnnealer(start: moms.1, end: moms.2, schedule: cosineSchedule)])
+    return (lrSched, momSched)
+}
+
+// export
+extension Learner where Opt.Scalar: BinaryFloatingPoint, 
+    Opt.Model.AllDifferentiableVariables == Opt.Model.TangentVector{
+
+    public func addOneCycleDelegates(_ lrMax: Float, pctStart:Float=0.25, divStart: Float = 10, divEnd: Float = 1e5, 
+                               moms: (Float,Float,Float) = (0.95,0.85,0.95)) {
+        let scheds = oneCycleSchedulers(lrMax, pctStart: pctStart, divStart: divStart, divEnd: divEnd, moms: moms)
+        addDelegates([makeParamScheduler(scheds.0 , hp: HyperParams.lr), 
+                      makeParamScheduler(scheds.1 , hp: HyperParams.mom)])
+    }
+}
+
+# And check it's all training properly.
+
+let optFunc: (CNNModel) -> StatefulOptimizer<CNNModel> = adamOpt(lr: 1e-3, mom: 0.9, beta: 0.99, wd: 1e-2, eps: 1e-6)
+
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+let recorder = learner.makeDefaultDelegates(metrics: [accuracy])
+learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))
+
+learner.addOneCycleDelegates(1e-3)
+learner.fit(1)
+
+recorder.plotLRs()
+
+# ### Differential learning rates
+
+# To train at differential learning rates (or freeze part of the models) we need to pass to our optimizer arrays of KeyPaths (which will define our layer groups). For instance, we can begin with the firt 9 keyPaths (which corresponds to the first three ConvBNs):
+
+func modelInit() -> CNNModel { return CNNModel(channelIn: 3, nOut: 10, filters: [64, 64, 128, 256]) }
+
+var model = modelInit()
+let splitArray = [Array(model.variables.keyPaths[0..<9]), Array(model.variables.keyPaths[9...])]
+
+let hpGroups: [[String: Float]] = [[HyperParams.lr: 0], [HyperParams.lr: 0.1]]
+func optFunc(_ model: CNNModel) -> StatefulOptimizer<CNNModel> {
+    return StatefulOptimizer(for: model, steppers: [SGDStep()], stats: [], hpGroups: hpGroups, splitArray: splitArray)
+}
+
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+let recorder = learner.makeDefaultDelegates(metrics: [accuracy])
+learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))
+
+# First parameter shouldn't change since the corresponding layer group as a LR of 0., second should.
+
+learner.model.convs[0].norm.scale
+
+learner.model.convs[3].norm.scale
+
+learner.fit(1)
+
+learner.model.convs[0].norm.scale
+
+learner.model.convs[3].norm.scale
+
+# Another way to get those keyPaths is to use the keyPaths to certain layers then append the keyPaths of all the parameters inside. This function takes a model, a layer and the keyPath that points from `model.variables` to `layer.variables` and returns the keyPaths of all the parameters of that layer. 
+
+public func parameterKeyPaths<M1, M2>(
+    _ model: M1,
+    _ kp: WritableKeyPath<M1.AllDifferentiableVariables, M2.AllDifferentiableVariables>,
+    _ layer: M2) -> [WritableKeyPath<M1.AllDifferentiableVariables, TF>]
+where M1: Layer, M2: Layer {
+    return model.variables[keyPath: kp].keyPaths.map { kp.appending(path: $0) }
+}
+
+# To access a keyPath directly, we use \ commands. Here is the keyPath to the array of convs, which lets us easily get the keyPaths for all the body of our CNN:
+
+let kp = \(CNNModel.AllDifferentiableVariables).convs
+let conv = model.convs
+let bodyKeyPaths = parameterKeyPaths(model, kp, conv)
+
+# Then we could split body and head:
+
+let splitArray = [bodyKeyPaths, model.variables.keyPaths.filter { return !bodyKeyPaths.contains($0) }]
+splitArray.map { $0.count }
+
+# If we want to refine this a bit and split our body between the first 4 convs and the last 3 we can proceed like this:
+
+let x = [1,2,3]
+let y = [4,5,6]
+zip(x,y).map { print($0, $1) }
+
+# +
+let deepBody = (0..<4).map { parameterKeyPaths(
+    model, 
+    \(CNNModel.AllDifferentiableVariables).convs.base[$0], 
+    model.convs[$0]
+) }.reduce([], +)
+
+let upperBody = (4..<7).map { parameterKeyPaths(
+    model, 
+    \(CNNModel.AllDifferentiableVariables).convs.base[$0], 
+    model.convs[$0]
+) }.reduce([], +)
+# -
+
+let splitArray = [deepBody, upperBody, model.variables.keyPaths.filter { return !bodyKeyPaths.contains($0) }]
+splitArray.map { $0.count }
+
+# Now let's say we want a parameter group will all the batchnorm layers. KeyPaths allow us to get all the batchnorm layers by saying `(to: FABatchNorm<Float>.self)`. The `.keyPaths` method we have been using is jsut a shortcut for `recursivelyAllWritableKeyPaths(to: TF.self)`, which grabs all the keypaths to all the tensors.
+
+let bns = model.recursivelyAllWritableKeyPaths(to: FABatchNorm<Float>.self).map { model[keyPath: $0] }
+
+# Then we need the keypaths from model.variables to those batchnorms.
+
+let bnKeyPaths = model.variables.recursivelyAllWritableKeyPaths(to: FABatchNorm<Float>.AllDifferentiableVariables.self)
+
+let bnParameters = zip(bnKeyPaths, bns).map { parameterKeyPaths(model, $0, $1) }.reduce([], +)
+bnParameters.count
+
+# ## Export
+
+import NotebookExport
+let exporter = NotebookExport(Path.cwd/"09_optimizer.ipynb")
+print(exporter.export(usingPrefix: "FastaiNotebook_"))
+
+
diff --git a/nbs/swift/10_mixup_ls.ipynb b/nbs/swift/10_mixup_ls.ipynb
index 793aaec..95cec6f 100644
--- ./nbs/swift/10_mixup_ls.ipynb
+++ ./nbs/swift/10_mixup_ls.ipynb
@@ -504,6 +504,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/10_mixup_ls.py b/nbs/swift/10_mixup_ls.py
new file mode 100644
index 0000000..482bdac
--- /dev/null
+++ ./nbs/swift/10_mixup_ls.py
@@ -0,0 +1,156 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/FastaiNotebook_09_optimizer")' FastaiNotebook_09_optimizer
+
+// export
+import Path
+import TensorFlow
+
+import FastaiNotebook_09_optimizer
+
+%include "EnableIPythonDisplay.swift"
+IPythonDisplay.shell.enable_matplotlib("inline")
+
+# ## Load data
+
+# //TODO: switch to imagenette when possible to train
+
+let data = mnistDataBunch(flat: true)
+
+let (n,m) = (60000,784)
+let c = 10
+let nHid = 50
+
+func modelInit() -> BasicModel {return BasicModel(nIn: m, nHid: nHid, nOut: c)}
+
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: sgdOpt(lr: 0.1), modelInit: modelInit)
+let recorder = learner.makeDefaultDelegates(metrics: [accuracy])
+learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))
+
+learner.fit(1)
+
+# ### Mixup
+
+//export
+extension RandomDistribution {
+    // Returns a batch of samples.
+    func next<G: RandomNumberGenerator>(
+        _ count: Int, using generator: inout G
+    ) -> [Sample] {
+        var result: [Sample] = []
+        for _ in 0..<count {
+            result.append(next(using: &generator))
+        }
+        return result
+    }
+
+    // Returns a batch of samples, using the global Threefry RNG.
+    func next(_ count: Int) -> [Sample] {
+        return next(count, using: &ThreefryRandomNumberGenerator.global)
+    }
+}
+
+# Mixup requires one-hot encoded targets since we don't have a loss function with no reduction.
+
+//export
+extension Learner {
+    public class MixupDelegate: Delegate {
+        private var distribution: BetaDistribution
+        
+        public init(alpha: Float = 0.4){
+            distribution = BetaDistribution(alpha: alpha, beta: alpha)
+        }
+        
+        override public func batchWillStart(learner: Learner) {
+            if let xb = learner.currentInput {
+                if let yb = learner.currentTarget as? Tensor<Float>{
+                    var lambda = Tensor<Float>(distribution.next(Int(yb.shape[0])))
+                    lambda = max(lambda, 1-lambda)
+                    let shuffle = Raw.randomShuffle(value: Tensor<Int32>(0..<Int32(yb.shape[0])))
+                    let xba = Raw.gather(params: xb, indices: shuffle)
+                    let yba = Raw.gather(params: yb, indices: shuffle)
+                    lambda = lambda.expandingShape(at: 1)
+                    learner.currentInput = lambda * xb + (1-lambda) * xba
+                    learner.currentTarget = (lambda * yb + (1-lambda) * yba) as? Label
+                }
+            }
+        }
+    }
+    
+    public func makeMixupDelegate(alpha: Float = 0.4) -> MixupDelegate {
+        return MixupDelegate(alpha: alpha)
+    }
+}
+
+let (n,m) = (60000,784)
+let c = 10
+let nHid = 50
+
+# We need to one-hot encode the targets
+
+var train1 = data.train.innerDs.map { DataBatch<TF,TF>(xb: $0.xb, 
+                            yb: Raw.oneHot(indices: $0.yb, depth: TI(10), onValue: TF(1), offValue: TF(0))) }
+
+var valid1 = data.valid.innerDs.map { DataBatch<TF,TF>(xb: $0.xb, 
+                            yb: Raw.oneHot(indices: $0.yb, depth: TI(10), onValue: TF(1), offValue: TF(0))) }
+
+let data1 = DataBunch(train: train1, valid: valid1, trainLen: data.train.dsCount, 
+                  validLen: data.valid.dsCount, bs: data.train.bs)
+
+func modelInit() -> BasicModel {return BasicModel(nIn: m, nHid: nHid, nOut: c)}
+
+func accuracyFloat(_ out: TF, _ targ: TF) -> TF {
+    return TF(out.argmax(squeezingAxis: 1) .== targ.argmax(squeezingAxis: 1)).mean()
+}
+
+let learner = Learner(data: data1, lossFunc: softmaxCrossEntropy, optFunc: sgdOpt(lr: 0.1), modelInit: modelInit)
+let recorder = learner.makeRecorder()
+
+learner.delegates = [learner.makeTrainEvalDelegate(), learner.makeShowProgress(), 
+                     learner.makeAvgMetric(metrics: [accuracyFloat]), recorder,
+                     learner.makeMixupDelegate(alpha: 0.2)]
+
+learner.fit(2)
+
+# ### Labelsmoothing
+
+//export
+@differentiable(wrt: out)
+public func labelSmoothingCrossEntropy(_ out: TF, _ targ: TI, ε: Float = 0.1) -> TF {
+    let c = out.shape[1]
+    let loss = softmaxCrossEntropy(logits: out, labels: targ)
+    let logPreds = logSoftmax(out)
+    return (1-ε) * loss - (ε / Float(c)) * logPreds.mean()
+}
+
+@differentiable(wrt: out)
+func lossFunc(_ out: TF, _ targ: TI) -> TF { return labelSmoothingCrossEntropy(out, targ, ε: 0.1) }
+
+let learner = Learner(data: data, lossFunc: lossFunc, optFunc: sgdOpt(lr: 0.1), modelInit: modelInit)
+let recorder = learner.makeDefaultDelegates(metrics: [accuracy])
+learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))
+
+learner.fit(2)
+
+# ## Export
+
+import NotebookExport
+let exporter = NotebookExport(Path.cwd/"10_mixup_ls.ipynb")
+print(exporter.export(usingPrefix: "FastaiNotebook_"))
+
+
diff --git a/nbs/swift/11_imagenette.ipynb b/nbs/swift/11_imagenette.ipynb
index feb9cb3..41b10dd 100644
--- ./nbs/swift/11_imagenette.ipynb
+++ ./nbs/swift/11_imagenette.ipynb
@@ -719,6 +719,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/11_imagenette.py b/nbs/swift/11_imagenette.py
new file mode 100644
index 0000000..d3b771b
--- /dev/null
+++ ./nbs/swift/11_imagenette.py
@@ -0,0 +1,257 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/FastaiNotebook_10_mixup_ls")' FastaiNotebook_10_mixup_ls
+
+// export
+import Path
+import TensorFlow
+
+import FastaiNotebook_10_mixup_ls
+
+%include "EnableIPythonDisplay.swift"
+IPythonDisplay.shell.enable_matplotlib("inline")
+
+# ## Load data
+
+let path = downloadImagenette()
+
+let il = ItemList(fromFolder: path, extensions: ["jpeg", "jpg"])
+
+# Then we split them according to the grandparent folder. `train` for the training set (which is the default) and `val` for the validation set.
+
+let sd = SplitData(il) {grandParentSplitter(fName: $0, valid: "val")}
+
+# We define our processors for the training set and the validation set. The difference with python is that we have to specify a noop processor (with a type) when we don't want to do anything.
+
+var procLabel = CategoryProcessor()
+
+# Then we can label our data using the parent directory and those two processors.
+
+let sld = makeLabeledData(sd, fromFunc: parentLabeler, procLabel: &procLabel)
+
+# We can then convert to a databunch by specifying two function that will convert our items and our labels to `Tensor`. For the items we use `pathsToTensor` that converts the `Path` object to their string representation then `StringTensor`. For the labels,  the function `intsToTensor` just convert the indices we have in proper tensors.
+
+let rawData = sld.toDataBunch(itemToTensor: pathsToTensor, labelToTensor: intsToTensor, bs: 128)
+
+# The main difference with python is that the transforms are all applied directly on the datasets by `tf.data`. Even opening on the image is such a transform, that will take a `StringTensor` and return a tensor of `UInt8`. We have written a function that opens the image in a filename and returns it decoded and resized to `size`, which is the transform we apply to our items.
+
+let data = transformData(rawData) { openAndResize(fname: $0, size: 128) }
+
+# We can then have a look by grabbing one batch.
+
+let batch = data.train.oneBatch()!
+
+# Our batches have two attributes `xb` and `yb` that contain the inputs and targets respectively.
+
+print(batch.xb.shape)
+print(batch.yb.shape)
+
+# If we decode the labels suing our processor, we can plot images with their corresponding classes:
+
+let labels = batch.yb.scalars.map { procLabel.vocab![Int($0)] }
+showImages(batch.xb, labels: labels)
+
+# ## XResnet
+
+# We build the same xresnet we had in fastai over PyTorch. Modules in S4TF are `struct` that conform to the `Layer` protocol. You define any layer it uses as attributes that you have to properly set in the `init` function. The equivalent of `forward` in PyTorch is `callAsFunction`.
+#
+# We are using our custom fastai layers that have the prefix `FA` because they contain experimental features that might eventually be merged in S4TF. There is a NoBiasConv layer separate from the Conv Layer because S4TF doesn't yet support control flow. That means you can't have if statements or for loops in the `callAsFunction` function.
+
+//export
+public struct ConvLayer: Layer {
+    public var bn: FABatchNorm<Float>
+    public var conv: FANoBiasConv2D<Float>
+    
+    public init(_ cIn: Int, _ cOut: Int, ks: Int = 3, stride: Int = 1, zeroBn: Bool = false, act: Bool = true){
+        bn = FABatchNorm(featureCount: cOut)
+//      "activation: act ? relu : identity" fails on 0.3.1, so we use if/else
+        if act {conv = FANoBiasConv2D(cIn, cOut, ks: ks, stride: stride, activation: relu)}
+        else   {conv = FANoBiasConv2D(cIn, cOut, ks: ks, stride: stride, activation: identity)}
+        if zeroBn { bn.scale = Tensor(zeros: [cOut]) }
+    }
+    
+    @differentiable
+    public func callAsFunction(_ input: TF) -> TF {
+        // TODO: Work around https://bugs.swift.org/browse/TF-606
+        return bn.forward(conv.forward(input))
+    }
+}
+
+# However we will need some optional layers to have refactored resnet implementation: in the shortcuts, we sometimes apply an average pool or a convolution layer, but most of the time we don't do anything. To be able to have that in S4TF, we write a customized protocol called `SwitchableLayer`. It inherits from `Layer` and adds a boolean `isOn` and a differentiable `forward` function. A structure conforming to it will return the result of `forward` if `isOn` is `true`, otherwise it won't do anything.
+#
+# As we said before, Swift autodiff doesn't support control flow (yet), so `if` statements aren't differentiable. That doesn't stop us from doing interesting work though, because we can create a custom function that will manually compute the gradients of our switchable layer:
+
+# +
+//export
+//A layer that you can switch off to do the identity instead
+public protocol SwitchableLayer: Layer {
+    associatedtype Input
+    var isOn: Bool { get set }
+    
+    @differentiable func forward(_ input: Input) -> Input
+}
+
+public extension SwitchableLayer {
+    func callAsFunction(_ input: Input) -> Input {
+        return isOn ? forward(input) : input
+    }
+
+    @differentiating(callAsFunction)
+    func gradForward(_ input: Input) ->
+           (value: Input,
+            pullback: (Self.Input.TangentVector) ->
+                                  (Self.TangentVector, Self.Input.TangentVector)) {
+        if isOn {
+            return valueWithPullback(at: input) { $0.forward($1) } 
+        } else {
+            return (input, { (Self.TangentVector.zero, $0) }) 
+        }
+    }
+}
+# -
+
+# Then we use this protocol to create a `MaybeAvgPool2D` layer and a `MaybeConv` layer. The little downside is that we will have to create a fake convolution for the `MaybeConv` that are just identity, but we will just create a 1x1x1x1 weight matrix so that it doesn't take much memory.
+
+//export
+public struct MaybeAvgPool2D: SwitchableLayer {
+    var pool: FAAvgPool2D<Float>
+    @noDerivative public var isOn: Bool
+    
+    @differentiable public func forward(_ input: TF) -> TF { return pool(input) }
+    
+    public init(_ sz: Int) {
+        isOn = (sz > 1)
+        pool = FAAvgPool2D<Float>(sz)
+    }
+}
+
+//export
+public struct MaybeConv: SwitchableLayer {
+    var conv: ConvLayer
+    @noDerivative public var isOn: Bool
+    
+    @differentiable public func forward(_ input: TF) -> TF { return conv(input) }
+    
+    public init(_ cIn: Int, _ cOut: Int) {
+        isOn = (cIn > 1) || (cOut > 1)
+        conv = ConvLayer(cIn, cOut, ks: 1, act: false)
+    }
+}
+
+# With those two maybe layers, we can write a `ResBlock` as we are used to. We can have array of layers that have the same type, and such an array can be treated as if it was a normal layer.
+
+# +
+//export
+public struct ResBlock: Layer {
+    public var convs: [ConvLayer]
+    public var idConv: MaybeConv
+    public var pool: MaybeAvgPool2D
+    
+    public init(_ expansion: Int, _ ni: Int, _ nh: Int, stride: Int = 1){
+        let (nf, nin) = (nh*expansion,ni*expansion)
+        convs = (expansion==1) ? [
+            ConvLayer(nin, nh, ks: 3, stride: stride),
+            ConvLayer(nh, nf, ks: 3, zeroBn: true, act: false)
+        ] : [
+            ConvLayer(nin, nh, ks: 1),
+            ConvLayer(nh, nh, ks: 3, stride: stride),
+            ConvLayer(nh, nf, ks: 1, zeroBn: true, act: false)
+        ]
+        idConv = nin==nf ? MaybeConv(1,1) : MaybeConv(nin, nf)
+        pool = MaybeAvgPool2D(stride)
+    }
+    
+    @differentiable
+    public func callAsFunction(_ inp: TF) -> TF {
+        return relu(convs(inp) + idConv(pool(inp)))
+    }
+    
+}
+# -
+
+# Then we can create our `XResnet` pretty much the same way as in pytorch. The list comprehesions are replaced by uses of `map` or `reduce`. The `makeLayer` function is out side of the `XResNet` structure because it can't be used in the `init` otherwise (in swift, you can only use self in the init when all the attributes have been properly set, and this `makeLayer` function is used to create the attribute `blocks`).
+
+//export
+func makeLayer(_ expansion: Int, _ ni: Int, _ nf: Int, _ nBlocks: Int, stride: Int) -> [ResBlock] {
+    return Array(0..<nBlocks).map { ResBlock(expansion, $0==0 ? ni : nf, nf, stride: $0==0 ? stride : 1) }
+}
+
+//export
+public struct XResNet: Layer {
+    public var stem: [ConvLayer]
+    public var maxPool = MaxPool2D<Float>(poolSize: (3,3), strides: (2,2), padding: .same)
+    public var blocks: [ResBlock]
+    public var pool = GlobalAvgPool2D<Float>()
+    public var linear: Dense<Float>
+    
+    public init(_ expansion: Int, _ layers: [Int], cIn: Int = 3, cOut: Int = 1000){
+        var nfs = [cIn, (cIn+1)*8, 64, 64]
+        stem = (0..<3).map{ ConvLayer(nfs[$0], nfs[$0+1], stride: $0==0 ? 2 : 1)}
+        nfs = [64/expansion,64,128,256,512]
+        blocks = layers.enumerated().map { (i,l) in 
+            return makeLayer(expansion, nfs[i], nfs[i+1], l, stride: i==0 ? 1 : 2)
+        }.reduce([], +)
+        linear = Dense(inputSize: nfs.last!*expansion, outputSize: cOut)
+    }
+    
+    @differentiable
+    public func callAsFunction(_ inp: TF) -> TF {
+        return inp.compose(stem, maxPool, blocks, pool, linear)
+    }
+}
+
+//export
+public func xresnet18 (cIn: Int = 3, cOut: Int = 1000) -> XResNet { return XResNet(1, [2, 2, 2, 2], cIn: cIn, cOut: cOut) }
+public func xresnet34 (cIn: Int = 3, cOut: Int = 1000) -> XResNet { return XResNet(1, [3, 4, 6, 3], cIn: cIn, cOut: cOut) }
+public func xresnet50 (cIn: Int = 3, cOut: Int = 1000) -> XResNet { return XResNet(4, [3, 4, 6, 3], cIn: cIn, cOut: cOut) }
+public func xresnet101(cIn: Int = 3, cOut: Int = 1000) -> XResNet { return XResNet(4, [3, 4, 23, 3], cIn: cIn, cOut: cOut) }
+public func xresnet152(cIn: Int = 3, cOut: Int = 1000) -> XResNet { return XResNet(4, [3, 8, 36, 3], cIn: cIn, cOut: cOut) }
+
+# To define a `Learner` we need our data, a model initializer function, and optimizer initializer function and a loss function. The model initilializer is a simple closure that returns the model.
+
+func modelInit() -> XResNet { return xresnet18(cOut: 10) }
+
+# The optimizer function is a convenience function we wrote in notebook 09 (like we had done in the python version) that returns a `StatefulOptimizer` with all the necessary stats/step delegates.
+
+let optFunc: (XResNet) -> StatefulOptimizer<XResNet> = adamOpt(lr: 1e-3, mom: 0.9, beta: 0.99, wd: 1e-2, eps: 1e-6)
+
+# Then we can create our `Learner`. The loss function is the classic cross entropy + softmax, and is given by S4TF.
+
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+
+# In swift, callbacks are called delegates. We have written a convenience function to automatically add the basic ones we need (train/eval, the recorder, metrics progress bar). This function returns the `recorder` if we want to look at losses or do some plots later.
+#
+# Then we add the delegate to normalize our inputs with the statistics of ImageNet.
+
+let recorder = learner.makeDefaultDelegates(metrics: [accuracy])
+learner.addDelegate(learner.makeNormalize(mean: imagenetStats.mean, std: imagenetStats.std))
+
+# Then we can fit with the 1cycle policy:
+
+learner.addOneCycleDelegates(1e-3, pctStart: 0.5)
+learner.fit(5)
+
+learner.addOneCycleDelegates(1e-3, pctStart: 0.5)
+learner.fit(1)
+
+# ## Export
+
+import NotebookExport
+let exporter = NotebookExport(Path.cwd/"11_imagenette.ipynb")
+print(exporter.export(usingPrefix: "FastaiNotebook_"))
+
+
diff --git a/nbs/swift/SwiftCV/Extra/Tests.ipynb b/nbs/swift/SwiftCV/Extra/Tests.ipynb
index efe6cf7..f881c22 100644
--- ./nbs/swift/SwiftCV/Extra/Tests.ipynb
+++ ./nbs/swift/SwiftCV/Extra/Tests.ipynb
@@ -474,6 +474,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/SwiftCV/Extra/Tests.py b/nbs/swift/SwiftCV/Extra/Tests.py
new file mode 100644
index 0000000..5b51945
--- /dev/null
+++ ./nbs/swift/SwiftCV/Extra/Tests.py
@@ -0,0 +1,148 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+# ## OpenCV Integration Example
+# Note: SwiftCV package requires OpenCV installed in order to compile.
+
+%install '.package(url: "https://github.com/vvmnnnkv/SwiftCV.git", .branch("master"))' SwiftCV
+
+# ### Imports
+
+# +
+%include "EnableIPythonDisplay.swift"
+import Foundation
+import Python
+import TensorFlow
+import SwiftCV
+
+let plt = Python.import("matplotlib.pyplot")
+let np = Python.import("numpy")
+IPythonDisplay.shell.enable_matplotlib("inline")
+# -
+
+// display opencv version
+print(cvVersion())
+
+# ### Utility
+
+# +
+func show_img(_ img: Mat, _ w: Int = 7, _ h: Int = 5) {
+    // convert from OpenCV to Tensor
+    let tens = Tensor<UInt8>(cvMat: img)!
+    // and from Tensor to numpy array for matplot
+    show_img(tens.makeNumpyArray(), w, h)
+}
+
+func show_img(_ img: PythonObject, _ w: Int = 7, _ h: Int = 5) {
+    plt.figure(figsize: [w, h])
+    plt.imshow(img)
+    plt.show()
+}
+# -
+
+# ### Load image
+
+# +
+// load image in memory
+let url = "https://live.staticflickr.com/2842/11335865374_0b202e2dc6_o_d.jpg"
+let imgContent = Data(contentsOf: URL(string: url)!)
+
+// make opencv image
+var cvImg = imdecode(imgContent)
+// convert color scheme to RGB
+cvImg = cvtColor(cvImg, nil, ColorConversionCode.COLOR_BGR2RGB)
+show_img(cvImg)
+# -
+
+# ### OpenCV Transformations
+
+# #### Resize
+
+show_img(
+    resize(cvImg, nil, Size(100, 50), 0, 0, InterpolationFlag.INTER_AREA)
+)
+
+
+# #### Zoom / Crop
+
+let zoomMat = getRotationMatrix2D(Size(cvImg.cols, cvImg.rows / 2), 0, 2)
+show_img(
+    warpAffine(cvImg, nil, zoomMat, Size(600, 600))
+)
+
+# #### Rotate
+
+let rotMat = getRotationMatrix2D(Size(cvImg.cols / 2, cvImg.rows / 2), 20, 1)
+show_img(
+    warpAffine(cvImg, nil, rotMat, Size(cvImg.cols, cvImg.rows))
+)
+
+# #### Pad
+
+show_img(
+    copyMakeBorder(cvImg, nil, 40, 40, 40, 40, BorderType.BORDER_CONSTANT, RGBA(0, 127, 0, 0))
+)
+
+# #### Blur
+
+show_img(
+    GaussianBlur(cvImg, nil, Size(25, 25))
+)
+
+# #### Flip
+
+show_img(
+    flip(cvImg, nil, FlipMode.HORIZONTAL)
+)
+
+# #### Transpose
+
+show_img(
+    transpose(cvImg, nil)
+)
+
+# ### Native S4TF Tensor Operations
+
+# #### Lightning / Contrast
+
+// convert image to floats Tensor
+var imgTens = Tensor<Float>(Tensor<UInt8>(cvMat: cvImg)!) / 255
+let contr:Float = 1.8
+let lightn:Float = 0.2
+let mean = imgTens.mean()
+imgTens = (imgTens - mean) * contr + mean + lightn
+show_img(imgTens.makeNumpyArray())
+
+# #### Noise
+
+# +
+// convert image to Tensor
+let smallImg = resize(cvImg, nil, Size(150, 150))
+var imgTens = Tensor<Float>(Tensor<UInt8>(cvMat: smallImg)!) / 255
+
+// make white noise (slow! :))
+var rng = PhiloxRandomNumberGenerator(seed: UInt64(42))
+let dist = NormalDistribution<Float>(mean: 0, standardDeviation: 0.05)
+var random: [Float] = []
+for _ in 0..<imgTens.shape.contiguousSize {
+    random.append(dist.next(using: &rng))
+}
+let randTens = Tensor<Float>(shape: imgTens.shape, scalars: random)
+
+imgTens += randTens
+show_img(imgTens.makeNumpyArray())
+# -
+
+
diff --git a/nbs/swift/audio.ipynb b/nbs/swift/audio.ipynb
index fbf40d8..0d4f62f 100644
--- ./nbs/swift/audio.ipynb
+++ ./nbs/swift/audio.ipynb
@@ -935,6 +935,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/audio.py b/nbs/swift/audio.py
new file mode 100644
index 0000000..7734eae
--- /dev/null
+++ ./nbs/swift/audio.py
@@ -0,0 +1,363 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/FastaiNotebook_10_mixup_ls")' FastaiNotebook_10_mixup_ls
+
+import Foundation
+import TensorFlow
+import FastaiNotebook_10_mixup_ls
+import Path
+
+%include "EnableIPythonDisplay.swift"
+IPythonDisplay.shell.enable_matplotlib("inline")
+
+# ## Laying the groundwork
+
+# ### Downloading the data
+
+// hardcoded, should really generalise
+public func downloadSpeakers(path: Path = dataPath) -> Path {
+    let url = "http://www.openslr.org/resources/45/ST-AEDS-20180100_1-OS.tgz"
+    let fname = "ST-AEDS-20180100_1-OS"
+    let outDir = path/fname
+    let outFile = path/"\(fname).tgz"
+    try! outDir.mkdir(.p)
+    if !outFile.exists {
+        downloadFile(url, dest:outFile.string)
+        _ = "/bin/tar".shell("-xzf", outFile.string, "-C", outDir.string)
+    }
+    print(outDir, type(of: outDir))
+    return outDir
+}
+
+let path = downloadSpeakers()
+
+# ### Opening WAVs in TensorFlow
+
+# Luckily, TensorFlow has a "decodeWav" function. Let's extend the StringTensor to provide swifty access to this function, the same way we did with JPGs.
+
+public extension StringTensor {
+    // Decode a StringTensor holding a path to a WAV file into (audio: Tensor<Float>, sampleRate: Tensor<Int32>)
+    func decodeWav() -> (audio: Tensor<Float>, sampleRate: Tensor<Int32>) {
+        return Raw.decodeWav(contents: self)
+    }
+}
+
+# ### Single example file
+
+let wav = path.ls()[0].path.string
+print(wav)
+
+let (sig, sr) = StringTensor(readFile: wav).decodeWav()
+
+# ### To Spectrogram
+
+# Luckily, TensorFlow has a couple of basic signal ops, including to Spectrogram.
+
+let spec = Raw.audioSpectrogram(sig, 
+                                windowSize: 1024, stride: 256, magnitudeSquared: false)
+
+# Utility to look at a spectrogram. Note the axes don't line up with what you'd expect from a typical spectrogram - here the X axis is frequency and the Y axis is time.
+
+func showSpec(s: Tensor<Float>) {
+    plt.imshow(s.makeNumpyArray()[0], cmap: "plasma")
+    plt.show()
+    print("Shape: \(s.shape)\nMin:\(s.min()), max: \(s.max()), mean: \(s.mean()), var: \(s.variance())")
+}
+
+showSpec(s: spec)
+
+# ### To MFCCs
+
+# Note that MFCCs are different from melspectrograms. It seems TF doesn't have a melspectrogram transform built-in. MFCCs at least have been used before so it doesn't seem crazy to use them instead, although they are generally used to intentionally reduce the dimensionality of the data, so we might be throwing away info that the NN could use. On the other hand we might be making it easier for the NN to find features it would've had to spend time finding itself. We'll give it a shot. On the plus side, it means much "smaller images".
+
+# [TF defaults](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/audio_ops.cc#L147) for MFCCs:
+# ```
+#     .Attr("upper_frequency_limit: float = 4000")
+#     .Attr("lower_frequency_limit: float = 20")
+#     .Attr("filterbank_channel_count: int = 40")
+#     .Attr("dct_coefficient_count: int = 13")
+# ```
+
+let mfccs = Raw.mfcc(spectrogram: spec, sampleRate: sr, 
+                       upperFrequencyLimit: 8000, lowerFrequencyLimit: 20, 
+                       filterbankChannelCount: 40, dctCoefficientCount: 13)
+
+showSpec(s: mfccs)
+
+# ## Using fastai - datablocks, labelers, splitters etc.
+
+let il = ItemList(fromFolder: path, extensions: ["wav", "WAV"])
+
+// export
+public func randomSplitter(fName: Path, val_pct: Float) -> Bool {
+    return Float.random(in: 0.0...1.0) < val_pct
+}
+
+let sd = SplitData(il) { randomSplitter(fName: $0, val_pct: 0.2) }
+
+# We need a RegexLabeler. In python this was
+# ```python
+# def re_labeler(fn, pat): return re.findall(pat, str(fn))[0]
+# ```
+
+public func reLabeler(_ fName: Path, pat: String) -> String {
+    // Gotta slice the input using the found range, then cast that `Substring` to `String`
+    // Seems too clunky to be true but it does work
+    // Needs a guard if re doesn't match
+    return String(fName.string[fName.string.findFirst(pat: pat)!])
+}
+
+let pat = "[mf]\\d+"
+
+let wavpath = sd.train.items[0]
+
+print(reLabeler(wavpath, pat: pat))
+
+// Surely this isn't how I'm supposed to do a 'partial' in Swift??
+// Only doing it because I couldn't get SplitLabeledData to work with a 2-arg fromFunc
+func speakerLabeler (item: Path) -> String {
+    return reLabeler(item, pat: pat)
+}
+
+print(speakerLabeler(item: wavpath))
+
+# "No-op" and Category processors. According to `11_imagenette`, we need these; I don't grok why yet.
+
+var (procItem,procLabel) = (NoopProcessor<Path>(),CategoryProcessor())
+
+let sld = SplitLabeledData(sd, fromFunc: speakerLabeler, procItem: &procItem, procLabel: &procLabel)
+
+// What did that do?
+print(type(of: sld.train.items[0]),":", sld.train.items[0])
+print(type(of: sld.train.labels[0]),":", sld.train.labels[0])
+print(type(of: sld.train.rawLabel(0)),":", sld.train.rawLabel(0))
+print(type(of: sld.train.procLabel.vocab!),":", sld.train.procLabel.vocab!)
+
+let rawData = sld.toDataBunch(itemToTensor: pathsToTensor, labelToTensor: intsToTensor, bs: 64)
+
+print(rawData.train.oneBatch())
+
+# Note that the `xb` of `rawData` is full of `StringTensor`s, not `FloatTensors` - that's because we've got to load them into first WAVs, then spectrograms.
+#
+# I'm going to load and spectrogrammify them in the same transform because I don't know how to appy >1 transform at a time yet.
+
+# Also, if we put it in a Databunch now it wouldn't work because the WAVs are different lengths. We could make this easier on ourselves by pre-processing in a different notebook/step, but let's trim the tensors to a given length after we load them.
+#
+# I'm not clear on whether this ought to be done as a Processor or a Transform. Here, I'll do it in a Transform, because I'm confused by the Processor - particularly the `deprocess` methods (and also because I don't know how to apply >1 tfm).
+
+public func openAndTrimAndSpectro(fname: StringTensor, len: Int = 16000, 
+                                 windowSize: Int64 = 1024, stride: Int64 = 256, 
+                                 magnitudeSquared: Bool = false) -> TF{
+    let (sig,sr) = StringTensor(readFile: fname).decodeWav()
+    let shortSig = sig[..<len]
+    let spec = Raw.audioSpectrogram(shortSig, 
+                                windowSize: windowSize, stride: stride, 
+                                    magnitudeSquared: magnitudeSquared)
+    return spec
+}
+
+let data = transformData(rawData, tfmItem: { openAndTrimAndSpectro(fname: $0, 
+                                                                  len: 16000,
+                                                                  windowSize:1024,
+                                                                  stride:128,
+                                                                  magnitudeSquared:false) })
+
+let batch = data.train.oneBatch()!
+
+// clone of 08_data_block.showImages, except the dimension to imshow()
+public func showSpecs(_ xb: TF, labels: [String]? = nil) {
+    let (rows,cols) = (3,3)
+//     plt.figure(figsize: [9, 9])
+    for i in 0..<(rows * cols) {
+        let img = plt.subplot(rows, cols, i + 1)
+        img.axis("off")
+        let x = xb[i].makeNumpyArray()
+        img.imshow(x[0]) // <- this is why it's different to showImg, dims are different
+        if labels != nil { img.set_title(labels![i]) }
+        if (i + 1) >= (rows * cols) { break }
+    }
+    plt.tight_layout()
+    plt.show()
+}
+
+let labels = batch.yb.scalars.map { sld.train.procLabel.vocab![Int($0)] }
+showSpecs(batch.xb, labels: labels)
+
+# ## Training
+
+# ### 08_data_block method — **CRASHES THE KERNEL.**
+
+# Stolen wholesale & without thinking from `08_data_block`. 
+
+// Using CNNModel as defined in 08_data_block notebook.
+func optFunc(_ model: CNNModel) -> SGD<CNNModel> { return SGD(for: model, learningRate: 0.1) }
+func modelInit() -> CNNModel { return CNNModel(channelIn: 1, nOut: 10, filters: [64, 64, 128, 256]) }
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+let recorder = learner.makeDefaultDelegates(metrics: [accuracy])
+// learner.addDelegate(learner.makeNormalize(mean: imagenetStats.mean, std: imagenetStats.std))
+
+# Let's try to get our dataset stats for normalization, in case that makes a difference. I don't think it will, because `08_data_block` runs fine both with & without the normalizer.
+
+// learner.fit(1) // CRASHES KERNEL; TF-619
+
+# ### 11_imagenette method  — **CRASHES THE KERNEL.**
+
+# Stolen wholesale & without thinking from `11_imagenette`. 
+
+public struct ConvLayer: Layer {
+    public var bn: FABatchNorm<Float>
+    public var conv: FANoBiasConv2D<Float>
+    
+    public init(_ cIn: Int, _ cOut: Int, ks: Int = 3, stride: Int = 1, zeroBn: Bool = false, act: Bool = true){
+        bn = FABatchNorm(featureCount: cOut)
+        conv = FANoBiasConv2D(cIn, cOut, ks: ks, stride: stride, activation: act ? relu : identity)
+        if zeroBn { bn.scale = Tensor(zeros: [cOut]) }
+    }
+    
+    @differentiable
+    public func callAsFunction(_ input: TF) -> TF {
+        return bn(conv(input))
+    }
+}
+
+# +
+//A layer that you can switch off to do the identity instead
+public protocol SwitchableLayer: Layer {
+    associatedtype Input
+    var isOn: Bool {get set}
+    
+    @differentiable func forward(_ input: Input) -> Input
+}
+
+public extension SwitchableLayer {
+    func callAsFunction(_ input: Input) -> Input {
+        return isOn ? forward(input) : input
+    }
+
+    @differentiating(callAsFunction)
+    func gradForward(_ input: Input) ->
+        (value: Input, pullback: (Self.Input.TangentVector) ->
+            (Self.TangentVector, Self.Input.TangentVector)) {
+        if isOn { return valueWithPullback(at: input) { $0($1) } }
+        else { return (input, {v in return (Self.TangentVector.zero, v)}) }
+    }
+}
+# -
+
+public struct MaybeAvgPool2D: SwitchableLayer {
+    var pool: FAAvgPool2D<Float>
+    @noDerivative public var isOn = false
+    
+    @differentiable public func forward(_ input: TF) -> TF { return pool(input) }
+    
+    public init(_ sz: Int) {
+        isOn = (sz > 1)
+        pool = FAAvgPool2D<Float>(sz)
+    }
+}
+
+public struct MaybeConv: SwitchableLayer {
+    var conv: ConvLayer
+    @noDerivative public var isOn = false
+    
+    @differentiable public func forward(_ input: TF) -> TF { return conv(input) }
+    
+    public init(_ cIn: Int, _ cOut: Int) {
+        isOn = (cIn > 1) || (cOut > 1)
+        conv = ConvLayer(cIn, cOut, ks: 1, act: false)
+    }
+}
+
+# +
+public struct ResBlock: Layer {
+    public var convs: [ConvLayer]
+    public var idConv: MaybeConv
+    public var pool: MaybeAvgPool2D
+    
+    public init(_ expansion: Int, _ ni: Int, _ nh: Int, stride: Int = 1){
+        let (nf, nin) = (nh*expansion,ni*expansion)
+        convs = [ConvLayer(nin, nh, ks: 1)]
+        convs += (expansion==1) ? [
+            ConvLayer(nh, nf, ks: 3, stride: stride, zeroBn: true, act: false)
+        ] : [
+            ConvLayer(nh, nh, ks: 3, stride: stride),
+            ConvLayer(nh, nf, ks: 1, zeroBn: true, act: false)
+        ]
+        idConv = nin==nf ? MaybeConv(1,1) : MaybeConv(nin, nf)
+        pool = MaybeAvgPool2D(stride)
+    }
+    
+    @differentiable
+    public func callAsFunction(_ inp: TF) -> TF {
+        return relu(convs(inp) + idConv(pool(inp)))
+    }
+    
+}
+# -
+
+func makeLayer(_ expansion: Int, _ ni: Int, _ nf: Int, _ nBlocks: Int, stride: Int) -> [ResBlock] {
+    return Array(0..<nBlocks).map { ResBlock(expansion, $0==0 ? ni : nf, nf, stride: $0==0 ? stride : 1) }
+}
+
+# +
+public struct XResNet: Layer {
+    public var stem: [ConvLayer]
+    public var maxPool = MaxPool2D<Float>(poolSize: (3,3), strides: (2,2), padding: .same)
+    public var blocks: [ResBlock]
+    public var pool = GlobalAvgPool2D<Float>()
+    public var linear: Dense<Float>
+    
+    public init(_ expansion: Int, _ layers: [Int], cIn: Int = 3, cOut: Int = 1000){
+        var nfs = [cIn, (cIn+1)*8, 64, 64]
+        stem = Array(0..<3).map{ ConvLayer(nfs[$0], nfs[$0+1], stride: $0==0 ? 2 : 1)}
+        nfs = [64/expansion,64,128,256,512]
+        blocks = Array(layers.enumerated()).map { (i,l) in 
+            return makeLayer(expansion, nfs[i], nfs[i+1], l, stride: i==0 ? 1 : 2)
+        }.reduce([], +)
+        linear = Dense(inputSize: nfs.last!*expansion, outputSize: cOut)
+    }
+    
+    @differentiable
+    public func callAsFunction(_ inp: TF) -> TF {
+        return linear(pool(blocks(maxPool(stem(inp)))))
+    }
+    
+}
+# -
+
+func xresnet18 (cIn: Int = 3, cOut: Int = 1000) -> XResNet { return XResNet(1, [2, 2, 2, 2], cIn: cIn, cOut: cOut) }
+func xresnet34 (cIn: Int = 3, cOut: Int = 1000) -> XResNet { return XResNet(1, [3, 4, 6, 3], cIn: cIn, cOut: cOut) }
+func xresnet50 (cIn: Int = 3, cOut: Int = 1000) -> XResNet { return XResNet(4, [3, 4, 6, 3], cIn: cIn, cOut: cOut) }
+func xresnet101(cIn: Int = 3, cOut: Int = 1000) -> XResNet { return XResNet(4, [3, 4, 23, 3], cIn: cIn, cOut: cOut) }
+func xresnet152(cIn: Int = 3, cOut: Int = 1000) -> XResNet { return XResNet(4, [3, 8, 36, 3], cIn: cIn, cOut: cOut) }
+
+func modelInit() -> XResNet { return xresnet34(cIn: 1, cOut: 10) }
+
+let optFunc: (XResNet) -> StatefulOptimizer<XResNet> = adamOpt(lr: 1e-3, mom: 0.9, beta: 0.99, wd: 1e-2, eps: 1e-6)
+
+let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)
+
+let recorder = learner.makeDefaultDelegates(metrics: [accuracy])
+// learner.addDelegate(learner.makeNormalize(mean: imagenetStats.mean, std: imagenetStats.std))
+
+learner.addOneCycleDelegates(1e-3, pctStart: 0.5)
+// learner.fit(5) // CRASHES KERNEL; TF-619
+
+# ## Fin.
+
+
diff --git a/nbs/swift/c_interop_examples.ipynb b/nbs/swift/c_interop_examples.ipynb
index 87ab61c..ba4aa16 100644
--- ./nbs/swift/c_interop_examples.ipynb
+++ ./nbs/swift/c_interop_examples.ipynb
@@ -406,6 +406,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/c_interop_examples.py b/nbs/swift/c_interop_examples.py
new file mode 100644
index 0000000..76ccc89
--- /dev/null
+++ ./nbs/swift/c_interop_examples.py
@@ -0,0 +1,111 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+# ## C Integration Examples
+#
+# Notes:
+#
+# - SwiftSox package requires sox to be installed: `sudo apt install libsox-dev libsox-fmt-all sox`
+# - SwiftVips package requires vips to be installed: see `SwiftVips/install.sh` for steps
+
+%install-extra-include-command pkg-config --cflags vips
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/SwiftVips")' SwiftVips
+%install '.package(path: "$cwd/SwiftSox")' SwiftSox
+%install '.package(path: "$cwd/FastaiNotebook_08_data_block")' FastaiNotebook_08_data_block
+
+import Foundation
+import Path
+import FastaiNotebook_08_data_block
+
+# ### Sox
+
+import sox
+
+# +
+public func InitSox() {
+  if sox_format_init() != SOX_SUCCESS.rawValue { fatalError("Can not init SOX!") }
+}
+
+public func ReadSoxAudio(_ name:String)->UnsafeMutablePointer<sox_format_t> {
+  return sox_open_read(name, nil, nil, nil)
+}
+# -
+
+InitSox()
+
+let fd = ReadSoxAudio("SwiftSox/sounds/chris.mp3")
+
+let sig = fd.pointee.signal
+
+(sig.rate,sig.precision,sig.channels,sig.length)
+
+var samples = [Int32](repeating: 0, count: numericCast(sig.length))
+
+sox_read(fd, &samples, numericCast(sig.length))
+
+import Python
+
+%include "EnableIPythonDisplay.swift"
+let plt = Python.import("matplotlib.pyplot")
+let np = Python.import("numpy")
+let display = Python.import("IPython.display")
+IPythonDisplay.shell.enable_matplotlib("inline")
+
+let t = samples.makeNumpyArray()
+
+plt.figure(figsize: [12, 4])
+plt.plot(t[2000..<4000])
+plt.show()
+
+display.Audio(t, rate:sig.rate).display()
+
+# So here we're using numpy, matplotlib, ipython, all from swift! 😎
+#
+# Why limit ourselves to Python? There's a lot out there that's not in Python yet!
+
+# [next slide](https://docs.google.com/presentation/d/1dc6o2o-uYGnJeCeyvgsgyk05dBMneArxdICW5vF75oU/edit#slide=id.g512a2e238a_144_16)
+
+# ### Vips
+
+import TensorFlow
+import SwiftVips
+import CSwiftVips
+import vips
+
+vipsInit()
+
+let path = downloadImagenette()
+let allNames = fetchFiles(path: path/"train", recurse: true, extensions: ["jpeg", "jpg"])
+let fNames = Array(allNames[0..<256])
+let ns = fNames.map {$0.string}
+
+let imgpath = ns[0]
+let img = vipsLoadImage(imgpath)!
+
+func vipsToTensor(_ img:Image)->Tensor<UInt8> {
+    var sz = 0
+    let mem = vipsGet(img, &sz)
+    defer {free(mem)}
+    let shape = TensorShape(vipsShape(img))
+    return Tensor(shape: shape, scalars: UnsafeBufferPointer(start: mem, count: sz))
+}
+
+show_img(vipsToTensor(img))
+
+# ## fin
+
+
diff --git a/nbs/swift/opencv_integration_example.ipynb b/nbs/swift/opencv_integration_example.ipynb
index 81e4ec9..695e0ee 100644
--- ./nbs/swift/opencv_integration_example.ipynb
+++ ./nbs/swift/opencv_integration_example.ipynb
@@ -803,6 +803,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Swift",
    "language": "swift",
diff --git a/nbs/swift/opencv_integration_example.py b/nbs/swift/opencv_integration_example.py
new file mode 100644
index 0000000..e4f9ab8
--- /dev/null
+++ ./nbs/swift/opencv_integration_example.py
@@ -0,0 +1,182 @@
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Swift
+#     language: swift
+#     name: swift
+# ---
+
+# ## OpenCV Integration Example
+# Note: SwiftCV package requires OpenCV installed in order to compile.
+
+# Uncomment line below when using Colab (this installs OpenCV4)
+# %system SwiftCV/install/install_colab.sh
+%install-location $cwd/swift-install
+%install '.package(path: "$cwd/SwiftCV")' SwiftCV
+%install '.package(path: "$cwd/FastaiNotebook_08_data_block")' FastaiNotebook_08_data_block
+
+# ### Imports
+
+%include "EnableIPythonDisplay.swift"
+import Foundation
+import SwiftCV
+import Path
+
+import FastaiNotebook_08_data_block
+
+// display opencv version
+print(cvVersion())
+
+# ### Load image
+
+func readImage(_ path:String)->Mat {
+    let cvImg = imread(path)
+    return cvtColor(cvImg, nil, ColorConversionCode.COLOR_BGR2RGB)
+}
+
+let path = downloadImagenette(sz:"")
+let allNames = fetchFiles(path: path/"train/n03425413", recurse: false, extensions: ["jpeg", "jpg"])
+let fNames = Array(allNames[0..<256])
+let ns = fNames.map {$0.string}
+let imgpath = ns[2]
+var cvImg = readImage(imgpath)
+
+# ### Timing
+
+cvImg.size
+
+print(type(of:cvImg.dataPtr))
+
+# [next slide](https://docs.google.com/presentation/d/1dc6o2o-uYGnJeCeyvgsgyk05dBMneArxdICW5vF75oU/edit#slide=id.g512a2e238a_144_0)
+
+let ptr = UnsafeRawPointer(cvImg.dataPtr).assumingMemoryBound(to: UInt8.self)
+
+ptr[2]
+
+time(repeating:10) {_ = readImage(imgpath)}
+
+cvImg.rows
+
+import Python
+import TensorFlow
+
+let plt = Python.import("matplotlib.pyplot")
+let np = Python.import("numpy")
+IPythonDisplay.shell.enable_matplotlib("inline")
+
+func show_img(_ img: Mat, _ w: Int = 7, _ h: Int = 5) {
+    show_img(Tensor<UInt8>(cvMat: img)!, w, h)
+}
+
+show_img(cvImg)
+
+time(repeating:10) {_ = resize(cvImg, nil, Size(224, 224), 0, 0, InterpolationFlag.INTER_NEAREST)}
+
+time(repeating:10) {_ = resize(cvImg, nil, Size(224, 224), 0, 0, InterpolationFlag.INTER_LINEAR)}
+
+time(repeating:10) {_ = resize(cvImg, nil, Size(224, 224), 0, 0, InterpolationFlag.INTER_CUBIC)}
+
+time(repeating:10) {_ = resize(cvImg, nil, Size(224, 224), 0, 0, InterpolationFlag.INTER_AREA)}
+
+cvImg = resize(cvImg, nil, Size(224, 224), 0, 0, InterpolationFlag.INTER_CUBIC)
+
+func readResized(_ fn:String)->Mat {
+    return resize(readImage(fn), nil, Size(224, 224), 0, 0, InterpolationFlag.INTER_CUBIC)
+}
+
+var imgs = ns[0..<10].map(readResized)
+
+time(repeating:10) {_ = readResized(imgpath)}
+
+# +
+public protocol Countable {
+    var count:Int {get}
+}
+extension Mat  :Countable {}
+extension Array:Countable {}
+
+public extension Sequence where Element:Countable {
+    var totalCount:Int { return map{ $0.count }.reduce(0, +) }
+}
+# -
+
+func collateMats(_ imgs:[Mat])->Tensor<Float> {
+    let c = imgs.totalCount
+    let ptr = UnsafeMutableRawPointer.allocate(byteCount: c, alignment: 1)
+    defer {ptr.deallocate()}
+    var p = ptr
+    for img in imgs {
+        p.copyMemory(from: img.dataPtr, byteCount: img.count)
+        p += img.count
+    }
+    let r = UnsafeBufferPointer(start: ptr.bindMemory(to: UInt8.self, capacity: c), count: c)
+    cvImg = imgs[0]
+    let shape = TensorShape([imgs.count, cvImg.rows, cvImg.cols, cvImg.channels])
+    let res = Tensor(shape: shape, scalars: r)
+    return Tensor<Float>(res)/255.0
+}
+
+var t = collateMats(imgs)
+
+t.shape
+
+show_img(t[2])
+
+time(repeating:10) {_ = collateMats(imgs)}
+
+time { _ = ns.map(readResized) }
+
+# ### OpenCV Transformations
+
+# #### Resize
+
+show_img(
+    resize(cvImg, nil, Size(100, 50), 0, 0, InterpolationFlag.INTER_AREA)
+)
+
+# #### Zoom / Crop
+
+let zoomMat = getRotationMatrix2D(Size(cvImg.cols, cvImg.rows / 2), 0, 1)
+show_img(
+    warpAffine(cvImg, nil, zoomMat, Size(600, 600))
+)
+
+# #### Rotate
+
+let rotMat = getRotationMatrix2D(Size(cvImg.cols / 2, cvImg.rows / 2), 20, 1)
+show_img(
+    warpAffine(cvImg, nil, rotMat, Size(cvImg.cols, cvImg.rows))
+)
+
+# #### Pad
+
+show_img(
+    copyMakeBorder(cvImg, nil, 40, 40, 40, 40, BorderType.BORDER_CONSTANT, RGBA(0, 127, 0, 0))
+)
+
+# #### Blur
+
+show_img(
+    GaussianBlur(cvImg, nil, Size(25, 25))
+)
+
+# #### Flip
+
+show_img(
+    flip(cvImg, nil, FlipMode.HORIZONTAL)
+)
+
+# #### Transpose
+
+show_img(
+    transpose(cvImg, nil)
+)
+
+
diff --git a/setup.sh b/setup.sh
new file mode 100755
index 0000000..74a3fd2
--- /dev/null
+++ ./setup.sh
@@ -0,0 +1,6 @@
+#!/bin/zsh
+SCRIPT=$(realpath "$0")
+SCRIPTPATH=$(dirname "$SCRIPT")
+cd "$SCRIPTPATH"
+
+find . -type f -name \*.ipynb -exec jupytext --set-formats ipynb,py {} \;
diff --git a/zh-nbs/00_notebook_tutorial.ipynb b/zh-nbs/00_notebook_tutorial.ipynb
index 964d636..ffa1ce8 100644
--- ./zh-nbs/00_notebook_tutorial.ipynb
+++ ./zh-nbs/00_notebook_tutorial.ipynb
@@ -1188,6 +1188,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/zh-nbs/00_notebook_tutorial.py b/zh-nbs/00_notebook_tutorial.py
new file mode 100644
index 0000000..c6926a2
--- /dev/null
+++ ./zh-nbs/00_notebook_tutorial.py
@@ -0,0 +1,363 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Practical Deep Learning for Coders, v3
+
+# # 00_notebook_tutorial
+
+# **Important note:** You should always work on a duplicate of the course notebook. On the page you used to open this, tick the box next to the name of the notebook and click duplicate to easily create a new version of this notebook.<br>
+# You will get errors each time you try to update your course repository if you don't do this, and your changes will end up being erased by the original course version.<br>
+
+# **重要提示:** 你应该在课程notebook的副本上工作。在你打开notebook的页面上，勾选notebook名称旁的选择框，然后点击复制就能轻松创建一个新的notebook副本了。<br>
+# 如果你不这样做，那么当你尝试更新课程资源库时就会报错，你的改动会被课程的原始内容所覆盖。
+
+# # Welcome to Jupyter Notebooks!
+
+# # 欢迎来到Jupyter Notebooks！
+
+# If you want to learn how to use this tool you've come to the right place. This article will teach you all you need to know to use Jupyter Notebooks effectively. You only need to go through Section 1 to learn the basics and you can go into Section 2 if you want to further increase your productivity.<br>
+
+# 如果你想学习如何使用这个工具，你来对地方了。这篇文章将教你高效使用jupyter notebook的所有应知应会的内容。你只需完成第1节的内容，就能学到基础知识，如果你想进一步生提高生产率，可以去学习第2节的内容。
+
+# You might be reading this tutorial in a web page (maybe Github or the course's webpage). We strongly suggest to read this tutorial in a (yes, you guessed it) Jupyter Notebook. This way you will be able to actually *try* the different commands we will introduce here.<br>
+
+# 你可能正在通过网页来阅读这篇教程（可能是github网站，或课程的网页上）。我们强烈建议你（没错，你猜对了）在jupyter notebook中来阅读本教程。这种方式可以让你实际*尝试*在本文中介绍到的不同命令。
+
+# ## Section 1: Need to Know 
+
+# ## 第1节：需知
+
+# ### Introduction 简介
+
+# Let's build up from the basics, what is a Jupyter Notebook? Well, you are reading one. It is a document made of cells. You can write like I am writing now (markdown cells) or you can perform calculations in Python (code cells) and run them like this:<br>
+
+# 让我们从最基础的部分开始说起，Jupyter Notebook是什么? 你现在看到的就是一个notebook。它是由一些单元格(cells)组成的文档。你可以像我这样（使用markdown cells）写入内容，你也可以（使用code cells）执行Python中的计算程序并且像下面这样来运行它们：
+
+1+1
+
+# Cool huh? This combination of prose and code makes Jupyter Notebook ideal for experimentation: we can see the rationale for each experiment, the code and the results in one comprehensive document. In fast.ai, each lesson is documented in a notebook and you can later use that notebook to experiment yourself. <br>
+
+# 是不是很cool?这种将普通文本和代码结合起来的模式，使得Jupyter Notebook成为做实验的绝佳选择：在一篇综合性文档中，我们可以既可以看到每个实验的原理讲解，又可以看到对应代码，甚至还有代码运行后的结果。在fast.ai课程里，每一节课的内容都以notebook方式来呈现，随后你也可以自己使用对应的notebook来做实验。
+
+# Other renowned institutions in academy and industry use Jupyter Notebook: Google, Microsoft, IBM, Bloomberg, Berkeley and NASA among others. Even Nobel-winning economists [use Jupyter Notebooks](https://paulromer.net/jupyter-mathematica-and-the-future-of-the-research-paper/)  for their experiments and some suggest that Jupyter Notebooks will be the [new format for research papers](https://www.theatlantic.com/science/archive/2018/04/the-scientific-paper-is-obsolete/556676/).<br>
+
+# 许多在学术界和工业界久负盛名的机构也在使用Jupyter Notebook，比如Google, Microsoft，IBM，Bloomberg，Berkeley以及NASA等，甚至诺贝尔经济学奖得主也在[使用Jupyter Notebooks](https://paulromer.net/jupyter-mathematica-and-the-future-of-the-research-paper/) 来进行实验，其中有一些经济学奖得主认为Jupyter Notebook将成为[新的学术论文格式](https://www.theatlantic.com/science/archive/2018/04/the-scientific-paper-is-obsolete/556676/)。
+
+# ### Writing 写作
+
+# A type of cell in which you can write like this is called _Markdown_. [_Markdown_](https://en.wikipedia.org/wiki/Markdown) is a very popular markup language. To specify that a cell is _Markdown_ you need to click in the drop-down menu in the toolbar and select _Markdown_.<br>
+
+# _Markdown_ 是jupyter notebook里单元格的一种类型，它可以让你进行本文写作。[Markdown](https://en.wikipedia.org/wiki/Markdown) 是一种非常流行的标记语言。为了指定一个单元格为*_Markdown_*,你需要点击工具栏中的下拉菜单并且选择*_Markdown_*。
+
+# Click on the the '+' button on the left and select _Markdown_ from the toolbar.<br>
+
+# 点击左边的“+”按钮，从工具栏中选择*_Markdown_*。
+
+# Now you can type your first _Markdown_ cell. Write 'My first markdown cell' and press run.<br>
+
+# 现在你可以创建你的第一个*_Markdown_*单元格了。在单元格中输入“My first markdown cell”并点击run。
+
+# ![](http://ml.xiniuedu.com/fastai/0/1.png)
+
+# You should see something like this: <br>
+
+# 你将看到下面的内容：
+
+# My first markdown cell
+
+# Now try making your first _Code_ cell: follow the same steps as before but don't change the cell type (when you add a cell its default type is _Code_). Type something like 3/2. You should see '1.5' as output.<br>
+
+# 现在试着创建你的第一个*_Code_*单元格：遵循前面介绍的步骤，但是不要修改单元格的类型（当你添加一个单元格时，它的默认类型就是*_Code_*）。输入一些代码，比如3/2，那么你的输出为“1.5”。
+
+3/2
+
+# ### Modes 模式
+
+# If you made a mistake in your *Markdown* cell and you have already ran it, you will notice that you cannot edit it just by clicking on it. This is because you are in **Command Mode**. Jupyter Notebooks have two distinct modes:<br>
+
+# 如果你在*Markdown*单元格中犯了错误并且已经运行过此单元格，你会发现不能仅通过点击它来进行编辑。这是因为你处于**命令模式**。Jupyter notebooks有两种不同的工作模式：<br>
+
+# 1. **Edit Mode**: Allows you to edit a cell's content.<br>
+#    **编辑模式**：允许你对单个单元格的内容进行编辑。
+
+# 2. **Command Mode**: Allows you to edit the notebook as a whole and use keyboard shortcuts but not edit a cell's content. <br>
+#    **命令模式**：允许你使用键盘快捷键，将notebook作为一个整体进行编辑,但不能对单个单元格的内容进行编辑。
+
+# You can toggle between these two by either pressing <kbd>ESC</kbd> and <kbd>Enter</kbd> or clicking outside a cell or inside it (you need to 
+# double click if its a Markdown cell). You can always know which mode you're on since the current cell has a green border if in **Edit Mode** and a blue border in **Command Mode**. Try it!<br>
+
+# 你可以在这两种模式间转换，方法是同时按下<kbd>ESC</kbd>和<kbd>Enter</kbd>键，或者通过点击一个单元格的外面或者里面来切换（如果是Markdown单元格，你需要双击实现模式切换）。你总是可以通过观察当前单元格的边框颜色来判断当前单元格处于什么模式：如果边框是绿色则表示处在**编辑模式**，如果是蓝色边框则表示处在**命令模式**。试一试吧!
+
+# ### Other Important Considerations 其他重要考虑因素
+
+# 1. Your notebook is autosaved every 120 seconds. If you want to manually save it you can just press the save button on the upper left corner or press <kbd>s</kbd> in **Command Mode**.<br>
+
+# 你的notebook每过120秒就将自动保存。如果你希望手工保存，只需点击左上角的save按钮即可，或者在**命令模式**下按下<kbd>s</kbd>键。
+
+# ![](http://ml.xiniuedu.com/fastai/0/2.png)
+
+# 2. To know if your kernel is computing or not you can check the dot in your upper right corner. If the dot is full, it means that the kernel is working. If not, it is idle. You can place the mouse on it and see the state of the kernel be displayed.<br>
+
+# 如果你想知道你的kernel是否在运行中，你可以检查右上角的圆点。如果是实心的，表示kernel正在工作中，如果是空心的，则表示kernel空闲。你也可以将鼠标悬浮于圆点上，来查看kernel的状态。
+
+# ![](http://ml.xiniuedu.com/fastai/0/3.png)
+
+# 3. There are a couple of shortcuts you must know about which we use **all** the time (always in **Command Mode**). These are:<br>
+# （处于**命令模式**时)有一些我们**总是**要用的键盘快捷键，你必须掌握。如下所示：
+#
+# <kbd>Shift</kbd>+<kbd>Enter</kbd>: Runs the code or markdown on a cell<br>
+# <kbd>Shift</kbd>+<kbd>Enter</kbd>：运行一个单元格中的代码或者格式化文本
+#
+# <kbd>Up Arrow</kbd>+<kbd>Down Arrow</kbd>: Toggle across cells<br>
+# <kbd>Up Arrow</kbd>+<kbd>Down Arrow</kbd>：在单元格之间切换选择
+#
+#
+# <kbd>b</kbd>: Create new cell<br>
+# <kbd>b</kbd>： 创建一个新的单元格
+#
+# <kbd>0</kbd>+<kbd>0</kbd>: Reset Kernel<br>
+# <kbd>0</kbd>+<kbd>0</kbd>： 重置 Kernel
+#
+# You can find more shortcuts in the Shortcuts section below.<br>
+# 在下面的章节，你还会看到更多快捷键的说明。
+
+# 4. You may need to use a terminal in a Jupyter Notebook environment (for example to git pull on a repository). That is very easy to do, just press 'New' in your Home directory and 'Terminal'. Don't know how to use the Terminal? We made a tutorial for that as well. You can find it [here](https://course.fast.ai/terminal_tutorial.html).<br>
+
+# 你可能需要在Jupyter Notebook的环境中使用terminal（比如通过git pull指令拉取一个repo）。这也非常简单，只需要在你的首页点击“New”，再选择“Terminal”即可。不知道具体怎么用Terminal?我们准备了一篇教程，你可以在 [这里](https://course.fast.ai/terminal_tutorial.html) 找到。
+
+# ![](http://ml.xiniuedu.com/fastai/0/4.png)
+
+# That's it. This is all you need to know to use Jupyter Notebooks. That said, we have more tips and tricks below ↓↓↓<br>
+
+# 好了，这就是使用Jupyter Notebooks时，你需要知道的知识点。当然了，下面还会介绍更多小技巧↓↓↓
+
+# ## Section 2: Going deeper 
+
+# ## 第2节：更进一步
+
+# + [markdown] hide_input=false
+# ### Markdown formatting  设定markdown的格式
+# -
+
+# #### Italics, Bold, Strikethrough, Inline, Blockquotes and Links 
+
+# #### 斜体，粗体，删除线，内联，引用和链接
+
+# The five most important concepts to format your code appropriately when using markdown are:<br>
+
+# 当你使用markdown时，有五种最重要的格式设定，它们的作用如下：
+
+# 1. *Italics*: Surround your text with '\_' or '\*'  
+# *斜体*: 在文本两边包裹上“\_”或者“\*” <br>
+
+# 2. **Bold**: Surround your text with '\__' or '\**'  
+# **粗体**: 在文本两边包裹上“\__”或者“**”<br>
+
+# 3. `inline`: Surround your text with '\`'  
+# `内联`: 文本两边包裹上“\`”<br>
+
+# 4.  > blockquote: Place '\>' before your text.  
+# > 引用：在文本前加上前缀“\>”<br>
+
+# 5.  [Links](https://course.fast.ai/): Surround the text you want to link with '\[\]' and place the link adjacent to the text, surrounded with '()' <br>
+# [链接](https://course.fast.ai/)： 在文本两边包裹上 “\[\]”（这里是方括号）,并且紧跟着将链接文本放在“()”中
+
+# #### Headings 标题
+
+# Notice that including a hashtag before the text in a markdown cell makes the text a heading. The number of hashtags you include will determine the priority of the header ('#' is level one, '##' is level two, '###' is level three and '####' is level four). We will add three new cells with the '+' button on the left to see how every level of heading looks.<br>
+
+# 在一个markdown单元格的文本前添加一个“#”,就可将该文本设定为标题了。“#”的个数决定了文本的优先级别。（“#”表示一级标题，“##”表示二级标题，“###”表示三级标题，“####”表示四级标题）。我们通过点击“+”来添加三个新的单元格来演示各个级别的标题都是什么样子的。
+
+# Double click on some headings and find out what level they are!<br>
+
+# 双击下面的标题，看看他们都是什么级别的吧！
+
+# #### Lists 列表
+
+# There are three types of lists in markdown.<br> 
+
+# 在markdown中有三种类型的列表。
+
+# Ordered list: 有序列表
+#
+# 1. Step 1<br>A.Step 1B
+# 2. Step 3
+
+# Unordered list 无序列表
+#
+# * learning rate 学习速率
+# * cycle length 周期长度
+# * weight decay 权重衰减
+
+# Task list 任务列表
+#
+# - [x] Learn Jupyter Notebooks 学习Jupyter Notebooks
+#     - [x] Writing 写作
+#     - [x] Modes 模式
+#     - [x] Other Considerations 其他考虑因素
+# - [ ] Change the world 改变世界
+
+# Double click on each to see how they are built!  
+
+# 双击查看这些列表是怎么构建出来的！
+
+# ### Code Capabilities 代码能力
+
+# **Code** cells are different than **Markdown** cells in that they have an output cell. This means that we can *keep* the results of our code within the notebook and share them. Let's say we want to show a graph that explains the result of an experiment. We can just run the necessary cells and save the notebook. The output will be there when we open it again! Try it out by running the next four cells.<br>
+
+# **Code**单元格和**Markdown**单元格是不同类型的单元格，因为**Code**单元格中有一个输出单元格。这意味着我们可以在notebook中 *保留* 代码执行结果，并分享它们。当我们想要展示实验结果的图表时，我们只需要运行必要的单元格并保存notebook。运行结果会在我们再次打开时显示出来！试试看运行接下来的4个单元格吧！
+
+# Import necessary libraries
+from fastai.vision import * 
+import matplotlib.pyplot as plt
+
+from PIL import Image
+
+a = 1
+b = a + 1
+c = b + a + 1
+d = c + b + a + 1
+a, b, c ,d
+
+plt.plot([a,b,c,d])
+plt.show()
+
+# We can also print images while experimenting. I am watching you.<br>
+
+# 我们也可以在做实验过程中显示一些图片。（这只猫的图片就像在说）“我在看着你哦”。
+
+Image.open('images/notebook_tutorial/cat_example.jpg')
+
+# ### Running the app locally 本地运行app
+
+# You may be running Jupyter Notebook from an interactive coding environment like Gradient, Sagemaker or Salamander. You can also run a Jupyter Notebook server from your local computer. What's more, if you have installed Anaconda you don't even need to install Jupyter (if not, just `pip install jupyter`).<br>
+
+# 你可能在Gradient, Sagemaker或者Salamander，这样的交互式编码环境中运行Jupyter Notebook。你也可以在本地计算机上运行一个Jupyter Notebook服务器。此外，如果你安装了Anaconda，你甚至不用单独安装Jupyter（如果没有安装的话，只要运行一下`pip install jupyter`就可以了)。
+
+# You just need to run `jupyter notebook` in your terminal. Remember to run it from a folder that contains all the folders/files you will want to access. You will be able to open, view and edit files located within the directory in which you run this command but not files in parent directories.<br>
+
+# 你只需要在你的terminal上运行`jupyter notebook`命令即可。记住在包含你希望访问的文件夹/文件的总文件夹那里来运行这条命令。这样你就可以打开，查看和编辑，运行了`jupyter notebook`命令的文件夹中的文件了，但是记在父目录里面的文件是不能打开查看或者编辑的。<br>
+
+# If a browser tab does not open automatically once you run the command, you should CTRL+CLICK the link starting with 'https://localhost:' and this will open a new tab in your default browser.<br>
+
+# 如果你运行了上面的命令，却没有自动打开浏览器，你也可以按住CTRL键，然后点击以 “https://localhost:” 开头的链接，这样你的默认浏览器中就会打开一个新的标签页。
+
+# ### Creating a notebook 创建一个notebook
+
+# Click on 'New' in the upper left corner and 'Python 3' in the drop-down list (we are going to use a [Python kernel](https://github.com/ipython/ipython) for all our experiments).<br>
+
+# 点击左上角的“New”按钮，随后在下拉列表中选择“Python 3”（我们将在我们的所有实验中使用一个[Python内核](https://github.com/ipython/ipython)）
+#
+
+# ![](http://ml.xiniuedu.com/fastai/0/5.png)
+
+# Note: You will sometimes hear people talking about the Notebook 'kernel'. The 'kernel' is just the Python engine that performs the computations for you. <br>
+
+# 注意：你有时可能听到人们谈论Notebook “kernel”，“kernel”就是替你执行计算的Python引擎。
+
+# ### Shortcuts and tricks 快捷键和技巧
+
+# #### Command Mode Shortcuts 命令模式下的快捷键
+
+# There are a couple of useful keyboard shortcuts in `Command Mode` that you can leverage to make Jupyter Notebook faster to use. Remember that to switch back and forth between `Command Mode` and `Edit Mode` with <kbd>Esc</kbd> and <kbd>Enter</kbd>.<br>
+
+# 在`命令模式`下有一些可以提高效率的快捷键。记住在`命令模式`和`编辑`模式间来回切换的快捷键是<kbd>Esc</kbd> 和 <kbd>Enter</kbd>。<br><br>
+
+# <kbd>m</kbd>: Convert cell to Markdown 将单元格转换为Markdown单元格
+
+# <kbd>y</kbd>: Convert cell to Code 将单元格转换为Code代码单元格
+
+# <kbd>D</kbd>+<kbd>D</kbd>: Delete cell 删除单元格
+
+# <kbd>o</kbd>: Toggle between hide or show output 切换显示或者隐藏输出信息
+
+# <kbd>Shift</kbd>+<kbd>Arrow up上箭头/Arrow down下箭头</kbd>: Selects multiple cells. Once you have selected them you can operate on them like a batch (run, copy, paste etc).
+# 用于选择多个单元格。一旦你选中了多个单元格，你就可以批量操作他们（比如运行，复制，粘贴等操作）。
+
+# <kbd>Shift</kbd>+<kbd>M</kbd>: Merge selected cells. 合并选中的单元格为一个单元格
+
+# <kbd>Shift</kbd>+<kbd>Tab</kbd>: [press once] Tells you which parameters to pass on a function 
+# [按键一次]提示函数有哪些参数
+
+# <kbd>Shift</kbd>+<kbd>Tab</kbd>: [press three times] Gives additional information on the method 
+# [按键三次] 提示这个方法的更多信息
+
+# #### Cell Tricks 单元格小技巧
+
+from fastai import*
+from fastai.vision import *
+
+# There are also some tricks that you can code into a cell.<br> 
+
+# 这里还有一些在单元格编码的一些小技巧。
+
+# `?function-name`: Shows the definition and docstring for that function <br>
+
+# `?function-name`：显示该函数的定义和文档信息
+
+# ?ImageDataBunch
+
+# `??function-name`: Shows the source code for that function<br>
+
+# `??function-name`：显示函数的源代码
+
+??ImageDataBunch
+
+# `doc(function-name)`: Shows the definition, docstring **and links to the documentation** of the function
+# (only works with fastai library imported)<br>
+
+# `doc(function-name)`：显示定义、文档信息以及**详细文档的链接**（只有在import导入了fastai库之后才能工作）
+
+doc(ImageDataBunch)
+
+# #### Line Magics 
+
+# Line magics are functions that you can run on cells and take as an argument the rest of the line from where they are called. You call them by placing a '%' sign before the command. The most useful ones are:<br>
+
+# Line magics是可以在单元格中运行并且将该行的其他信息作为参数的函数。通过在命令之前添加一个“%”来调用他们。最有用的是以下几个：
+
+# `%matplotlib inline`: This command ensures that all matplotlib plots will be plotted in the output cell within the notebook and will be kept in the notebook when saved.<br>
+
+# `%matplotlib inline`：该命令确保所有的matplotlib图表都将绘制在notebook的输出单元格中，并且在保存时一并保留在notebook中。
+
+# `%reload_ext autoreload`, `%autoreload 2`: Reload all modules before executing a new line. If a module is edited, it is not necessary to rerun the import commands, the modules will be reloaded automatically.<br>
+
+# `%reload_ext autoreload`,`%autoreload 2`：这两条命令指示在执行新的行代码时重新加载所有的模块。如果一个模块修改过了，没有必要再次运行import命令，模块将自动重新加载。
+
+# These three commands are always called together at the beginning of every notebook. <br>
+
+# 通常这3条命令在每一个notebook的起始部分被一起调用。
+
+# %matplotlib inline
+# %reload_ext autoreload
+# %autoreload 2
+
+# `%timeit`: Runs a line a ten thousand times and displays the average time it took to run it.<br>
+
+# `%timeit`：这个命令将会运行一行代码1000次并且显示平均运行的时间。
+
+# %timeit [i+1 for i in range(1000)]
+
+# `%debug`: Allows to inspect a function which is showing an error using the [Python debugger](https://docs.python.org/3/library/pdb.html).<br>
+
+# `%debug`：允许你使用[Python调试器](https://docs.python.org/3/library/pdb.html)来检查报错的函数。
+
+for i in range(1000):
+    a = i+1
+    b = 'string'
+    c = b+1
+
+# %debug
diff --git a/zh-nbs/Lesson1_pets.ipynb b/zh-nbs/Lesson1_pets.ipynb
index 8675b1a..76ae3d7 100644
--- ./zh-nbs/Lesson1_pets.ipynb
+++ ./zh-nbs/Lesson1_pets.ipynb
@@ -1553,6 +1553,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/zh-nbs/Lesson1_pets.py b/zh-nbs/Lesson1_pets.py
new file mode 100644
index 0000000..c756982
--- /dev/null
+++ ./zh-nbs/Lesson1_pets.py
@@ -0,0 +1,255 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Practical Deep Learning for Coders, v3
+
+# # Lesson 1_pets
+
+# # What's your pet 
+
+# # 你的宠物是什么
+
+# Welcome to lesson 1! For those of you who are using a Jupyter Notebook for the first time, you can learn about this useful tool in a tutorial we prepared specially for you; click `File`->`Open` now and click `00_notebook_tutorial.ipynb`. <br>
+
+# 欢迎来到第1课！如果你是首次使用Jupyter Notebook，你可以通过阅读我们特别为你准备的教程来学习这个有用的工具：即刻点击`File`->`Open`然后点击`00_notebook_tutorial.ipynb`即可。
+
+# In this lesson we will build our first image classifier from scratch, and see if we can achieve world-class results. Let's dive in!<br>
+
+# 在这节课中，我们将从零开始构建我们自己的首个图像分类器，看看是否能够取得世界级的成果。就让我们一探究竟吧！
+
+# Every notebook starts with the following three lines; they ensure that any edits to libraries you make are reloaded here automatically, and also that any charts or images displayed are shown in this notebook.<br>
+
+# 每一个notebook都由下面三行开始；它们确保你对库代码进行任何编辑，这些代码都将自动重新加载，并且任何图表或图片能在notebook中展示。
+
+# %reload_ext autoreload
+# %autoreload 2
+# %matplotlib inline
+
+# We import all the necessary packages. We are going to work with the [fastai V1 library](http://www.fast.ai/2018/10/02/fastai-ai/) which sits on top of [Pytorch 1.0](https://hackernoon.com/pytorch-1-0-468332ba5163). The fastai library provides many useful functions that enable us to quickly and easily build neural networks and train our models.<br>
+
+# 我们首先导入所有需要的包，然后就可以使用构建于 [Pytorch 1.0](https://hackernoon.com/pytorch-1-0-468332ba5163)之上的 [fastai V1库](http://www.fast.ai/2018/10/02/fastai-ai/)。fastai库提供了大量有用的函数，可以帮助我们简单快捷地构建神经网络，并且训练出我们的模型。
+
+from fastai.vision import *
+from fastai.metrics import error_rate
+
+# If you're using a computer with an unusually small GPU, you may get an out of memory error when running this notebook. If this happens, click Kernel->Restart, uncomment the 2nd line below to use a smaller *batch size* (you'll learn all about what this means during the course), and try again.<br>
+
+# 如果你的计算机GPU内存非常小，在运行notebook时，你很可能会碰到内存溢出的错误。如果这种情况发生了，点击Kernel->Restart，然后去除对下面第2行的注释，使用更小的*batch size*（批次大小，你将在课程中全面了解其含义），然后再试着运行一下。
+
+bs = 64
+# bs = 16   # uncomment this line if you run out of memory even after clicking Kernel->Restart
+
+# ## Looking at the data 浏览一下数据
+
+# We are going to use the [Oxford-IIIT Pet Dataset](http://www.robots.ox.ac.uk/~vgg/data/pets/) by [O. M. Parkhi et al., 2012](http://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf) which features 12 cat breeds and 25 dogs breeds. Our model will need to learn to differentiate between these 37 distinct categories. According to their paper, the best accuracy they could get in 2012 was 59.21%, using a complex model that was specific to pet detection, with separate "Image", "Head", and "Body" models for the pet photos. Let's see how accurate we can be using deep learning!<br>
+
+# 我们将使用由 [O. M. Parkhi et al., 2012](http://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf)引用的[Oxford-IIIT Pet Dataset](http://www.robots.ox.ac.uk/~vgg/data/pets/)数据集，这个数据集中有12个品种的猫和25个品种的狗。我们的模型需要学会正确区分这37个品种。根据上述学者的论文描述，他们能取得的最佳分类准确率是59.21%。这个结果是基于一个专门检测宠物品种的复杂模型得到的，这个模型对宠物的照片分别建立了“肖像”、“头部”以及“躯干”的独立模型。让我们来看看，使用深度学习能够达到什么样的准确率吧!
+
+# We are going to use the `untar_data` function to which we must pass a URL as an argument and which will download and extract the data.<br>
+
+# 我们将通过引用一个URL参数来调用`untar_data`函数，这样就可以下载和解压相应数据。
+
+help(untar_data)
+
+path = untar_data(URLs.PETS); path
+
+path.ls()
+
+path_anno = path/'annotations'
+path_img = path/'images'
+
+# The first thing we do when we approach a problem is to take a look at the data. We *always* need to understand very well what the problem is and what the data looks like before we can figure out how to solve it. Taking a look at the data means understanding how the data directories are structured, what the labels are and what some sample images look like.<br>
+
+# 当我们解决特定问题时，首先要做的是浏览一下数据。在想清楚最终如何解决一个问题之前，我们 *总是* 要深入理解问题和数据究竟意味着什么。浏览一下数据，意味着理解数据的内容和结构，分类标签是什么，样本图片是什么样的。
+
+# The main difference between the handling of image classification datasets is the way labels are stored. In this particular dataset, labels are stored in the filenames themselves. We will need to extract them to be able to classify the images into the correct categories. Fortunately, the fastai library has a handy function made exactly for this, `ImageDataBunch.from_name_re` gets the labels from the filenames using a [regular expression](https://docs.python.org/3.6/library/re.html).<br>
+
+# 图片分类数据处理方式的最主要区别是标签存储方式。在这个数据集中，标签本身就存在于文件名之中。我们需要将标签信息提取出来，从而将这些图片分门别类。幸运的是，fastai库提供了一个非常好用的函数来实现这一点，`ImageDataBunch.from_name_re`函数通过使用[正则表达式](https://docs.python.org/3.6/library/re.html) 从文件名中提取标签信息。
+
+fnames = get_image_files(path_img)
+fnames[:5]
+
+np.random.seed(2)
+pat = r'/([^/]+)_\d+.jpg$'
+
+data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=bs
+                                  ).normalize(imagenet_stats)
+
+data.show_batch(rows=3, figsize=(7,6))
+
+print(data.classes)
+len(data.classes),data.c
+
+# ## Training: resnet34    
+
+# ## 训练：resnet34
+
+# Now we will start training our model. We will use a [convolutional neural network](http://cs231n.github.io/convolutional-networks/) backbone and a fully connected head with a single hidden layer as a classifier. Don't know what these things mean? Not to worry, we will dive deeper in the coming lessons. For the moment you need to know that we are building a model which will take images as input and will output the predicted probability for each of the categories (in this case, it will have 37 outputs).<br>
+
+# 现在我们将要开始训练模型了。我们将使用一个 [卷积神经网络](http://cs231n.github.io/convolutional-networks/) 作为主干结构，衔接一个单隐藏层的全连接头部，构成分类器模型。不理解这些是什么意思吗?不用担心，我们在接下来的课程中会做更深入的讲解。当下，你只需要理解，我们正在构建一个模型，这个模型接收图片作为输入，并且能够输出各个品种的预测概率（在我们这个案例中，共有37个数）。
+
+# We will train for 4 epochs (4 cycles through all our data).<br>
+
+# 我们将训练4个epochs（即4个遍历所有数据的循环）。
+
+learn = cnn_learner(data, models.resnet34, metrics=error_rate)
+
+learn.model
+
+learn.fit_one_cycle(4)
+
+learn.save('stage-1')
+
+# ## Results 结果
+
+# Let's see what results we have got. <br>
+
+# 让我们来看看结果如何。
+
+# We will first see which were the categories that the model most confused with one another. We will try to see if what the model predicted was reasonable or not. In this case the mistakes look reasonable (none of the mistakes seems obviously naive). This is an indicator that our classifier is working correctly. <br>
+
+# 我们首先看到的是模型最混淆的品种。我们要试着思考一下，模型预测的是否合理。在这个案例里，模型的错误分类看起来是合理的（没有犯明显的低级错误）。这意味着我们的分类器做的不错。
+
+# Furthermore, when we plot the confusion matrix, we can see that the distribution is heavily skewed: the model makes the same mistakes over and over again but it rarely confuses other categories. This suggests that it just finds it difficult to distinguish some specific categories between each other; this is normal behaviour.<br>
+
+# 而且，当我们绘制出（识别品种）的混淆矩阵时，我们可以看到数据分布是偏态的：模型会不断地重复相同的错误，但是很少会混淆其他品种。这意味着模型难以正确区分一些具体的品种；这是正常表现。
+
+# +
+interp = ClassificationInterpretation.from_learner(learn)
+
+losses,idxs = interp.top_losses()
+
+len(data.valid_ds)==len(losses)==len(idxs)
+# -
+
+interp.plot_top_losses(9, figsize=(15,11))
+
+doc(interp.plot_top_losses)
+
+interp.plot_confusion_matrix(figsize=(12,12), dpi=60)
+
+interp.most_confused(min_val=2)
+
+# ## Unfreezing, fine-tuning, and learning rates 
+
+# ## 解冻，调优和学习率
+
+# Since our model is working as we expect it to, we will *unfreeze* our model and train some more.<br>
+
+# 既然我们的模型表现符合我们的预期，我们将*解冻* 模型并继续训练。
+
+learn.unfreeze()
+
+learn.fit_one_cycle(1)
+
+learn.load('stage-1');
+
+learn.lr_find()
+
+learn.recorder.plot()
+
+learn.unfreeze()
+learn.fit_one_cycle(2, max_lr=slice(1e-6,1e-4))
+
+# That's a pretty accurate model! <br>
+
+# 相当准确的一个模型!
+
+# ## Training: resnet50 
+
+# ## 训练:resnet50
+
+# Now we will train in the same way as before but with one caveat: instead of using resnet34 as our backbone we will use resnet50 (resnet34 is a 34 layer residual network while resnet50 has 50 layers. It will be explained later in the course and you can learn the details in the [resnet paper](https://arxiv.org/pdf/1512.03385.pdf)).<br>
+
+# 现在我们将用和之前一样的方式训练模型，但是有一点需要提醒：这次我们将不再使用resnet34作为我们的主干结构，而是使用resnet50。（resnet34是一个34层的残差网络，而resnet50则有50层。本课程后续还会解释，你也可以通过研读[resnet残差网络论文](https://arxiv.org/pdf/1512.03385.pdf)来学习更多的细节）。
+
+# Basically, resnet50 usually performs better because it is a deeper network with more parameters. Let's see if we can achieve a higher performance here. To help it along, let's us use larger images too, since that way the network can see more detail. We reduce the batch size a bit since otherwise this larger network will require more GPU memory.<br>
+
+# 通常情况下，resnet50会表现出更好的性能，因为它是一个参数更多、层次更深的网络。我们来看看是否可以获得更好的成绩。为了帮助模型学习，我们使用更大尺寸的图片，这样就会让我们的网络看到更多细节特征。我们稍稍降低batch size，否则这个更大的网络会需要更多的GPU内存（以至于无法使用）。
+
+data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(),
+                                   size=299, bs=bs//2).normalize(imagenet_stats)
+
+learn = cnn_learner(data, models.resnet50, metrics=error_rate)
+
+learn.lr_find()
+learn.recorder.plot()
+
+learn.fit_one_cycle(8)
+
+learn.save('stage-1-50')
+
+# It's astonishing that it's possible to recognize pet breeds so accurately! Let's see if full fine-tuning helps:<br>
+
+# 真是不可思议，模型的宠物品种识别准确率如此之高！我们来看看能否再做一些有帮助的调优：
+
+learn.unfreeze()
+learn.fit_one_cycle(3, max_lr=slice(1e-6,1e-4))
+
+# If it doesn't, you can always go back to your previous model.<br>
+
+# 就算调优没有帮助，你也总是能够回退到之前的模型。
+
+learn.load('stage-1-50');
+
+interp = ClassificationInterpretation.from_learner(learn)
+
+interp.most_confused(min_val=2)
+
+# ## Other data formats 
+
+# ## 其他的数据格式
+
+path = untar_data(URLs.MNIST_SAMPLE); path
+
+tfms = get_transforms(do_flip=False)
+data = ImageDataBunch.from_folder(path, ds_tfms=tfms, size=26)
+
+data.show_batch(rows=3, figsize=(5,5))
+
+learn = cnn_learner(data, models.resnet18, metrics=accuracy)
+learn.fit(2)
+
+df = pd.read_csv(path/'labels.csv')
+df.head()
+
+data = ImageDataBunch.from_csv(path, ds_tfms=tfms, size=28)
+
+data.show_batch(rows=3, figsize=(5,5))
+data.classes
+
+data = ImageDataBunch.from_df(path, df, ds_tfms=tfms, size=24)
+data.classes
+
+fn_paths = [path/name for name in df['name']]; fn_paths[:2]
+
+pat = r"/(\d)/\d+\.png$"
+data = ImageDataBunch.from_name_re(path, fn_paths, pat=pat, ds_tfms=tfms, size=24)
+data.classes
+
+data = ImageDataBunch.from_name_func(path, fn_paths, ds_tfms=tfms, size=24,
+        label_func = lambda x: '3' if '/3/' in str(x) else '7')
+data.classes
+
+labels = [('3' if '/3/' in str(x) else '7') for x in fn_paths]
+labels[:5]
+
+data = ImageDataBunch.from_lists(path, fn_paths, labels=labels, ds_tfms=tfms, size=24)
+data.classes
+
+#
+
+#
diff --git a/zh-nbs/Lesson2_download.ipynb b/zh-nbs/Lesson2_download.ipynb
index 374c07a..8c973d4 100644
--- ./zh-nbs/Lesson2_download.ipynb
+++ ./zh-nbs/Lesson2_download.ipynb
@@ -1834,6 +1834,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/zh-nbs/Lesson2_download.py b/zh-nbs/Lesson2_download.py
new file mode 100644
index 0000000..627db46
--- /dev/null
+++ ./zh-nbs/Lesson2_download.py
@@ -0,0 +1,396 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Practical Deep Learning for Coders, v3
+
+# # Lesson 2_download
+
+# + [markdown] hide_input=false
+# # Creating your own dataset from Google Images
+# # 从Google Images创建你自己的数据集
+#
+# -
+
+# *作者: Francisco Ingham 和 Jeremy Howard. 灵感来源于[Adrian Rosebrock](https://www.pyimagesearch.com/2017/12/04/how-to-create-a-deep-learning-dataset-using-google-images/)*
+
+# + [markdown] hide_input=true
+# In this tutorial we will see how to easily create an image dataset through Google Images. **Note**: You will have to repeat these steps for any new category you want to Google (e.g once for dogs and once for cats).<br>
+# -
+
+# 在本教程中，我们将看到如何从Goolge Images中轻松地创建一个图片数据集。 **注意**：从Google搜集任何你想要的新品类，你都必须重复这些步骤（比如，狗的数据集，还有猫的数据集，你就得把这些步骤各执行一遍）。
+
+# + hide_input=false
+from fastai.vision import *
+# -
+
+# ## Get a list of URLs 获取URL的列表
+
+# ### Search and scroll 搜索并翻看
+
+# Go to [Google Images](http://images.google.com) and search for the images you are interested in. The more specific you are in your Google Search, the better the results and the less manual pruning you will have to do.<br>
+
+# 打开[Google Images](http://images.google.com)页面，搜索你感兴趣的图片。你在搜索框中输入的信息越精确，那么搜索的结果就越好，而需要你手动处理的工作就越少。
+
+# Scroll down until you've seen all the images you want to download, or until you see a button that says 'Show more results'. All the images you scrolled past are now available to download. To get more, click on the button, and continue scrolling. The maximum number of images Google Images shows is 700.<br>
+
+# 往下翻页直到你看到所有你想下载的图片，或者直到你看到一个“显示更多结果”的按钮为止。你刚翻看过的所有图片都是可下载的。为了获得更多的图片，点击“显示更多结果”按钮，继续翻看。Goolge Images最多可以显示700张图片。
+
+# It is a good idea to put things you want to exclude into the search query, for instance if you are searching for the Eurasian wolf, "canis lupus lupus", it might be a good idea to exclude other variants:<br>
+
+# 在搜索请求框中增加一些你想排除在外的信息是个好主意。比如，如果你要搜canis lupus lupus这一类欧亚混血狼，最好筛除掉别的种类（这样返回的结果才比较靠谱）
+
+#    "canis lupus lupus" -dog -arctos -familiaris -baileyi -occidentalis
+
+# You can also limit your results to show only photos by clicking on *Tools* and selecting *Photos* from the *Type* dropdown.<br>
+
+# 你也可以限制搜索的结果，让搜索结果只显示照片，通过点击*工具*从*Type*里选择*照片*进行下载。
+
+# ### Download into file 下载到文件中
+
+# Now you must run some Javascript code in your browser which will save the URLs of all the images you want for you dataset.<br>
+
+# 现在你需要在浏览器中运行一些javascript代码，浏览器将保存所有你想要放入数据集的图片的URL地址。
+
+# Press <kbd>Ctrl</kbd><kbd>Shift</kbd><kbd>J</kbd> in Windows/Linux and <kbd>Cmd</kbd><kbd>Opt</kbd><kbd>J</kbd> in Mac, and a small window the javascript 'Console' will appear. That is where you will paste the JavaScript commands.<br>
+
+# (浏览器窗口下)windows/linux系统按<kbd>Ctrl</kbd><kbd>Shift</kbd><kbd>J</kbd>，Mac系统按 <kbd>Cmd</kbd><kbd>Opt</kbd><kbd>J</kbd>,就会弹出javascript的“控制台”面板，在这个面板中，你可以把相关的javascript命令粘贴进去。
+
+# You will need to get the urls of each of the images. Before running the following commands, you may want to disable ads block add-ons (YouBlock) in Chrome. Otherwise window.open() coomand doesn't work. Then you can run the following commands:<br>
+
+# 你需要获得每个图片对应的url。在运行下面的代码之前，你可能需要在Chrome中禁用广告拦截插件，否则window.open()函数将不能工作。然后你就可以运行下面的代码：
+
+# ```javascript
+# urls = Array.from(document.querySelectorAll('.rg_di .rg_meta')).map(el=>JSON.parse(el.textContent).ou);
+# window.open('data:text/csv;charset=utf-8,' + escape(urls.join('\n')));
+# ```
+
+# ### Create directory and upload urls file into your server 
+
+# ### 创建一个目录并将url文件上传到服务器上
+
+# Choose an appropriate name for your labeled images. You can run these steps multiple times to create different labels.<br>
+
+# 为带标签的图片选择一个合适的名字，你可以多次执行下面的步骤来创建不同的标签。
+
+folder = 'black'
+file = 'urls_black.csv'
+
+folder = 'teddys'
+file = 'urls_teddys.csv'
+
+folder = 'grizzly'
+file = 'urls_grizzly.csv'
+
+# You will need to run this cell **once per each category**.<br>
+
+# 下面的单元格，**每一个品种运行一次**。
+
+path = Path('data/bears')
+dest = path/folder
+dest.mkdir(parents=True, exist_ok=True)
+
+path.ls()
+
+# Finally, upload your urls file. You just need to press 'Upload' in your working directory and select your file, then click 'Upload' for each of the displayed files.<br>
+
+# 最后,上传你的url文件。你只需要在工作区点击“Upload”按钮，然后选择你要上传的文件，再点击“Upload”即可。
+
+# ![](http://ml.xiniuedu.com/fastai/2/1.png)
+
+# ## Download images 下载图片
+
+# Now you will need to download your images from their respective urls.<br>
+
+# 现在，你要做的是从图片对应的url地址下载这些图片。
+
+# fast.ai has a function that allows you to do just that. You just have to specify the urls filename as well as the destination folder and this function will download and save all images that can be opened. If they have some problem in being opened, they will not be saved.<br>
+
+# fast.ai提供了一个函数来完成这个工作。你只需要指定url地址文件名和目标文件夹，这个函数就能自动下载和保存可打开的图片。如果图片本身无法打开的话，对应图片也不会被保存.
+
+# Let's download our images! Notice you can choose a maximum number of images to be downloaded. In this case we will not download all the urls.<br>
+
+# 我们开始下载图片吧！注意你可以设定需要下载的最大图片数量，这样我们就不会下载所有url地址了。
+
+# You will need to run this line **once for every category**.<br>
+
+# 下面这行代码，**每一个品种运行一次**。
+
+classes = ['teddys','grizzly','black']
+
+download_images(path/file, dest, max_pics=200)
+
+# If you have problems download, try with `max_workers=0` to see exceptions:
+download_images(path/file, dest, max_pics=20, max_workers=0)
+
+# Then we can remove any images that can't be opened: <br>
+
+# 然后我们可以删除任何不能打开的图片:
+
+for c in classes:
+    print(c)
+    verify_images(path/c, delete=True, max_size=500)
+
+# ## View data 浏览数据
+
+np.random.seed(42)
+data = ImageDataBunch.from_folder(path, train=".", valid_pct=0.2,
+        ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)
+
+# +
+# If you already cleaned your data, run this cell instead of the one before
+# 如果你已经清洗过你的数据,直接运行这格代码而不是上面的
+# np.random.seed(42)
+# data = ImageDataBunch.from_csv(path, folder=".", valid_pct=0.2, csv_labels='cleaned.csv',
+#         ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)
+# -
+
+# Good! Let's take a look at some of our pictures then.<br>
+
+# 好！我们浏览一些照片。
+
+data.classes
+
+data.show_batch(rows=3, figsize=(7,8))
+
+data.classes, data.c, len(data.train_ds), len(data.valid_ds)
+
+# ## Train model 训练模型
+
+learn = cnn_learner(data, models.resnet34, metrics=error_rate)
+
+learn.fit_one_cycle(4)
+
+learn.save('stage-1')
+
+learn.unfreeze()
+
+learn.lr_find()
+
+learn.recorder.plot()
+
+learn.fit_one_cycle(2, max_lr=slice(3e-5,3e-4))
+
+learn.save('stage-2')
+
+# ## Interpretation 结果解读
+
+learn.load('stage-2');
+
+interp = ClassificationInterpretation.from_learner(learn)
+
+interp.plot_confusion_matrix()
+
+# ## Cleaning Up 清理
+#
+# Some of our top losses aren't due to bad performance by our model. There are images in our data set that shouldn't be.<br>
+
+# 某些最大误差，不是由于模型的性能差导致的，而是由于数据集中的有些图片本身存在问题才导致的。
+
+# Using the `ImageCleaner` widget from `fastai.widgets` we can prune our top losses, removing photos that don't belong.<br>
+
+# 从`fastai.widgets`库中导入并使用`ImageCleaner`小工具，我们就可以剔除那些归类错误的图片，从而减少预测失误。
+
+from fastai.widgets import *
+
+# First we need to get the file paths from our top_losses. We can do this with `.from_toplosses`. We then feed the top losses indexes and corresponding dataset to `ImageCleaner`.<br>
+
+# 首先，我们可以借助`.from_toplosses`，从top_losses中获取我们需要的文件路径。随后喂给`ImageCleaner`误差高的索引以及对应的数据集参数。
+
+# Notice that the widget will not delete images directly from disk but it will create a new csv file `cleaned.csv` from where you can create a new ImageDataBunch with the corrected labels to continue training your model.<br>
+
+# 需要注意的是，这些小工具本身并不会直接从磁盘删除图片，它会创建一个新的csv文件`cleaned.csv`，通过这个文件，你可以新创建一个包含准确标签信息的ImageDataBunch（图片数据堆），并继续训练你的模型。
+
+# In order to clean the entire set of images, we need to create a new dataset without the split. The video lecture demostrated the use of the `ds_type` param which no longer has any effect. See [the thread](https://forums.fast.ai/t/duplicate-widget/30975/10) for more details.<br>
+
+# 为了清空整个图片集，我们需要创建一个新的未经分拆的数据集。视频课程里演示的`ds_type` 参数的用法已经不再有效。参照 [the thread](https://forums.fast.ai/t/duplicate-widget/30975/10) 来获取更多细节。
+
+db = (ImageList.from_folder(path)
+                   .no_split()
+                   .label_from_folder()
+                   .transform(get_transforms(), size=224)
+                   .databunch()
+     )
+
+# +
+# If you already cleaned your data using indexes from `from_toplosses`,<br><br>
+# 如果你已经从`from_toplosses`使用indexes清理了你的数据
+# run this cell instead of the one before to proceed with removing duplicates.<br><br>
+# 运行这个单元格里面的代码(而非上面单元格的内容)以便继续删除重复项
+# Otherwise all the results of the previous step would be overwritten by<br><br>
+# 否则前一个步骤中的结果都会被覆盖
+# the new run of `ImageCleaner`.<br><br>
+# 下面就是要运行的`ImageCleaner`代码,请把下面的注释去掉开始运行
+
+# db = (ImageList.from_csv(path, 'cleaned.csv', folder='.')
+#                    .no_split()
+#                    .label_from_df()
+#                    .transform(get_transforms(), size=224)
+#                    .databunch()
+#      )
+# -
+
+# Then we create a new learner to use our new databunch with all the images.<br>
+
+# 接下来，我们要创建一个新的学习器来使用包含全部图片的新数据堆。
+
+# +
+learn_cln = cnn_learner(db, models.resnet34, metrics=error_rate)
+
+learn_cln.load('stage-2');
+# -
+
+ds, idxs = DatasetFormatter().from_toplosses(learn_cln)
+
+# Make sure you're running this notebook in Jupyter Notebook, not Jupyter Lab. That is accessible via [/tree](/tree), not [/lab](/lab). Running the `ImageCleaner` widget in Jupyter Lab is [not currently supported](https://github.com/fastai/fastai/issues/1539).<br>
+
+# 确保你在Jupyter Notebook环境下运行这个notebook，而不是在Jupyter Lab中运行。我们可以通过[/tree](/tree)来访问（notebook），而不是[/lab](/lab)。[目前还不支持](https://github.com/fastai/fastai/issues/1539)在Jupyter Lab中运行`ImageCleaner`小工具。<br>
+
+ImageCleaner(ds, idxs, path)
+
+# Flag photos for deletion by clicking 'Delete'. Then click 'Next Batch' to delete flagged photos and keep the rest in that row. `ImageCleaner` will show you a new row of images until there are no more to show. In this case, the widget will show you images until there are none left from `top_losses.ImageCleaner(ds, idxs)`.<br>
+
+# 点击“Delete”标记待删除的照片，然后再点击“Next Batch”来删除已标记的照片，同时保持其他图片仍在原来的位置。`ImageCleaner`将显示一行新的图片，直到没有更多的图片可以展示。在这种情况下，小工具程序会为你展示图片，直到从`top_losses.ImageCleaner(ds, idxs)`没有更多图片输出为止。
+
+# You can also find duplicates in your dataset and delete them! To do this, you need to run `.from_similars` to get the potential duplicates' ids and then run `ImageCleaner` with `duplicates=True`. The API works in a similar way as with misclassified images: just choose the ones you want to delete and click 'Next Batch' until there are no more images left.<br>
+
+# 你会发现在你的数据集中存在重复图片，一定要删除他们！为了做到这一点，你需要运行`.from_similars`来获取有潜在重复可能的图片的id，然后运行`ImageCleaner`并使用`duplicate=True`作为参数。API的工作方式和（处理）错误分类的图片相类似：你只要选中那些你想删除的图片，然后点击'Next Batch'直到没有更多的图片遗留为止。
+
+# Make sure to recreate the databunch and `learn_cln` from the `cleaned.csv` file. Otherwise the file would be overwritten from scratch, loosing all the results from cleaning the data from toplosses.<br>
+
+# 确保你从`cleaned.csv`文件中重新创建了数据堆和`learn_cln`，否则文件会被完全覆盖，你将丢失所有从失误排行里清洗数据后的结果。
+
+ds, idxs = DatasetFormatter().from_similars(learn_cln)
+
+ImageCleaner(ds, idxs, path, duplicates=True)
+
+# Remember to recreate your ImageDataBunch from your `cleaned.csv` to include the changes you made in your data!<br>
+
+# 记住从你的`cleaned.csv`中重新创建ImageDatabunch，以便包含你对数据的所有变更!
+
+# ## Putting your model in production 部署模型
+
+# First thing first, let's export the content of our `Learner` object for production:<br>
+
+# 首先，导出我们训练好的`Learner`对象内容，为部署做好准备：
+
+learn.export()
+
+# This will create a file named 'export.pkl' in the directory where we were working that contains everything we need to deploy our model (the model, the weights but also some metadata like the classes or the transforms/normalization used).<br>
+
+# 这个命令会在我们处理模型的目录里创建名为export.pk1的文件，该文件中包含了用于部署模型的所有信息（模型、权重，以及一些元数据，如一些类或用到的变换/归一化处理等）。
+
+# You probably want to use CPU for inference, except at massive scale (and you almost certainly don't need to train in real-time). If you don't have a GPU that happens automatically. You can test your model on CPU like so:<br>
+
+# 你可能想用CPU来进行推断，除了大规模的（几乎可以肯定你不需要实时训练模型)，（所以）如果你没有GPU资源，你也可以使用CPU来对模型做简单的测试：
+
+defaults.device = torch.device('cpu')
+
+img = open_image(path/'black'/'00000021.jpg')
+img
+
+# We create our `Learner` in production enviromnent like this, jsut make sure that `path` contains the file 'export.pkl' from before.<br>
+
+# 我们在这样的生产环境下创建`学习器`，只需确保`path`参数包含了前面生成好的“export.pk1”文件。
+
+learn = load_learner(path)
+
+pred_class,pred_idx,outputs = learn.predict(img)
+pred_class
+
+# So you might create a route something like this ([thanks](https://github.com/simonw/cougar-or-not) to Simon Willison for the structure of this code):<br>
+
+# 你可能需要像下面的代码这样，创建一个路径, ([谢谢](https://github.com/simonw/cougar-or-not)Simon Willison提供了这些代码的架构）：
+
+# ```python
+# @app.route("/classify-url", methods=["GET"])
+# async def classify_url(request):
+#     bytes = await get_bytes(request.query_params["url"])
+#     img = open_image(BytesIO(bytes))
+#     _,_,losses = learner.predict(img)
+#     return JSONResponse({
+#         "predictions": sorted(
+#             zip(cat_learner.data.classes, map(float, losses)),
+#             key=lambda p: p[1],
+#             reverse=True
+#         )
+#     })
+# ```
+
+# (This example is for the [Starlette](https://www.starlette.io/) web app toolkit.)<br>
+
+# （这个例子适用于 [Starlette](https://www.starlette.io/)的web app工具包）
+
+# ## Things that can go wrong 可能出错的地方
+
+# - Most of the time things will train fine with the defaults   
+# 大多数时候使用默认参数就能训练出好模型
+
+# - There's not much you really need to tune (despite what you've heard!)   
+# 没有太多需要你去调整的（尽管你可能听到过一些）
+
+# - Most likely are   
+# 可能就是(下面的参数)
+#   - Learning rate   学习率
+#   - Number of epochs   epochs的数目
+
+# ### Learning rate (LR) too high 学习率(LR)太高
+
+learn = cnn_learner(data, models.resnet34, metrics=error_rate)
+
+learn.fit_one_cycle(1, max_lr=0.5)
+
+# ### Learning rate (LR) too low 学习率(LR)太低
+
+learn = cnn_learner(data, models.resnet34, metrics=error_rate)
+
+# Previously we had this result:<br> 
+
+# 前面的代码运行后，我们得到如下结果：
+
+# ```
+# Total time: 00:57
+# epoch  train_loss  valid_loss  error_rate
+# 1      1.030236    0.179226    0.028369    (00:14)
+# 2      0.561508    0.055464    0.014184    (00:13)
+# 3      0.396103    0.053801    0.014184    (00:13)
+# 4      0.316883    0.050197    0.021277    (00:15)
+# ```
+
+learn.fit_one_cycle(5, max_lr=1e-5)
+
+learn.recorder.plot_losses()
+
+# As well as taking a really long time, it's getting too many looks at each image, so may overfit.<br>
+
+# 不仅运行耗时过长，而且模型对每一个图片都太过注重细节，因此可能过拟合。
+
+# ### Too few epochs     epochs过少
+
+learn = cnn_learner(data, models.resnet34, metrics=error_rate, pretrained=False)
+
+learn.fit_one_cycle(1)
+
+# ### Too many epochs     epochs过多
+
+np.random.seed(42)
+data = ImageDataBunch.from_folder(path, train=".", valid_pct=0.9, bs=32, 
+        ds_tfms=get_transforms(do_flip=False, max_rotate=0, max_zoom=1, max_lighting=0, max_warp=0
+                              ),size=224, num_workers=4).normalize(imagenet_stats)
+
+learn = cnn_learner(data, models.resnet50, metrics=error_rate, ps=0, wd=0)
+learn.unfreeze()
+
+learn.fit_one_cycle(40, slice(1e-6,1e-4))
diff --git a/zh-nbs/Lesson2_sgd.ipynb b/zh-nbs/Lesson2_sgd.ipynb
index 0c31a55..e312c85 100644
--- ./zh-nbs/Lesson2_sgd.ipynb
+++ ./zh-nbs/Lesson2_sgd.ipynb
@@ -18707,6 +18707,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/zh-nbs/Lesson2_sgd.py b/zh-nbs/Lesson2_sgd.py
new file mode 100644
index 0000000..334b334
--- /dev/null
+++ ./zh-nbs/Lesson2_sgd.py
@@ -0,0 +1,140 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Practical Deep Learning for Coders, v3
+
+# # Lesson 2_sgd
+
+# %matplotlib inline
+from fastai.basics import *
+
+# In this part of the lecture we will explain Stochastic Gradient Descent (SGD) which is an **optimization** method commonly used in neural networks We will ilustrate the concepts with concrete examples.<br>
+
+# 在这部分，我们将会解释随机梯度下降算法(SGD)，它在神经网络应用中是常用的**优化**算法。我们将通过实例来解释其原理和概念。
+
+# #  Linear Regression problem 线性回归问题
+
+# The goal of linear regression is to fit a line to a set of points. <br>
+
+# 线性回归的目标是将一条直线拟合到一组点。
+
+n=100
+
+x = torch.ones(n,2) 
+x[:,0].uniform_(-1.,1)
+x[:5]
+
+a = tensor(3.,2); a
+
+y = x@a + torch.rand(n)
+
+plt.scatter(x[:,0], y);
+
+
+# You want to find **parameters** (weights) `a` such that you minimize the *error* between the points and the line `x@a`. Note that here `a` is unknown. For a regression problem the most common *error function* or *loss function* is the **mean squared error**. <br>
+
+# 你希望找到这样的 **参数**（权重） `a`，使得数据点和直线`x@a`之间的 *误差* 尽可能小。需要注意的是这里`a`是未知的。对于回归问题最常用的 *误差函数* 或者说 *损失函数* 是 **均方误差(MSE)** 。
+
+def mse(y_hat, y): return ((y_hat-y)**2).mean()
+
+
+# Suppose we believe `a = (-1.0,1.0)` then we can compute `y_hat` which is our *prediction* and then compute our error.<br>
+
+# 假设我们取`a = (-1.0,1.0)`，那么我们就可以计算 *预测值* `y_hat` ，随后我们可以算出误差来。
+
+a = tensor(-1.,1)
+
+y_hat = x@a
+mse(y_hat, y)
+
+plt.scatter(x[:,0],y)
+plt.scatter(x[:,0],y_hat);
+
+# So far we have specified the *model* (linear regression) and the *evaluation criteria* (or *loss function*). Now we need to handle *optimization*; that is, how do we find the best values for `a`? How do we find the best *fitting* linear regression.<br>
+
+# 到现在我们已经指定了 *模型*  的类型（线性回归），以及 *评估标准* （或者说 *损失函数* ），接下来我们需要处理 *优化*  过程；即，我们如何才能找到最优的`a`呢？我们如何才能找到 *拟合*  最好的线性回归模型呢？
+
+# # Gradient Descent 梯度下降
+
+# We would like to find the values of `a` that minimize `mse_loss`.<br>
+#
+# 我们希望找到最小化`mse_loss`值的`a`的值。
+#
+# **Gradient descent** is an algorithm that minimizes functions. Given a function defined by a set of parameters, gradient descent starts with an initial set of parameter values and iteratively moves toward a set of parameter values that minimize the function. This iterative minimization is achieved by taking steps in the negative direction of the function gradient.<br>
+#
+# **梯度下降** 是一个用于优化函数的算法。给定一个由一组参数决定的函数，梯度下降从一组初始的参数值开始，不断向能够最小化函数值的参数值迭代。这个迭代式最小化的结果是，通过向函数梯度的负方向不断递进而得到的。
+#
+# Here is gradient descent implemented in [PyTorch](http://pytorch.org/). <br>
+#
+# 这里是 [PyTorch](http://pytorch.org/)中梯度下降算法实施的细节。
+
+a = nn.Parameter(a); a
+
+
+def update():
+    y_hat = x@a
+    loss = mse(y, y_hat)
+    if t % 10 == 0: print(loss)
+    loss.backward()
+    with torch.no_grad():
+        a.sub_(lr * a.grad)
+        a.grad.zero_()
+
+
+lr = 1e-1
+for t in range(100): update()
+
+plt.scatter(x[:,0],y)
+plt.scatter(x[:,0],x@a);
+
+# ## Animate it! 过程动画化
+
+from matplotlib import animation, rc
+rc('animation', html='jshtml')
+
+# +
+a = nn.Parameter(tensor(-1.,1))
+
+fig = plt.figure()
+plt.scatter(x[:,0], y, c='orange')
+line, = plt.plot(x[:,0], x@a)
+plt.close()
+
+def animate(i):
+    update()
+    line.set_ydata(x@a)
+    return line,
+
+animation.FuncAnimation(fig, animate, np.arange(0, 100), interval=20)
+# -
+
+# In practice, we don't calculate on the whole file at once, but we use *mini-batches*.<br>
+#
+# 实际上，我们并没有立刻计算整个数据集，相反，我们采用 *mini-batches（小批次）* 的策略。
+
+# ## Vocab 术语
+
+# - Learning rate 学习率
+# - Epoch 轮次
+# - Minibatch 小批次
+# - SGD 随机梯度下降法
+# - Model / Architecture 模型/架构
+# - Parameters 参数
+# - Loss function 损失函数
+#
+# For classification problems, we use *cross entropy loss*, also known as *negative log likelihood loss*. This penalizes incorrect confident predictions, and correct unconfident predictions.<br>
+#
+# 对于分类问题，我们使用 *交叉熵损失* ，也被称为 *负对数似然损失* 。该损失函数将惩罚那些置信高的错误预测和置信低的正确预测。
diff --git a/zh-nbs/Lesson3_camvid.ipynb b/zh-nbs/Lesson3_camvid.ipynb
index 26dc15e..5151119 100644
--- ./zh-nbs/Lesson3_camvid.ipynb
+++ ./zh-nbs/Lesson3_camvid.ipynb
@@ -1057,6 +1057,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/zh-nbs/Lesson3_camvid.py b/zh-nbs/Lesson3_camvid.py
new file mode 100644
index 0000000..9831919
--- /dev/null
+++ ./zh-nbs/Lesson3_camvid.py
@@ -0,0 +1,192 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Practical Deep Learning for Coders, v3
+
+# # Lesson3_camvid
+
+# ## Image segmentation with CamVid
+# ## 用CamVid数据集进行图像分割
+
+# %reload_ext autoreload
+# %autoreload 2
+# %matplotlib inline
+
+from fastai.vision import *
+from fastai.callbacks.hooks import *
+from fastai.utils.mem import *
+
+path = untar_data(URLs.CAMVID)
+path.ls()
+
+path_lbl = path/'labels'
+path_img = path/'images'
+
+# ## Subset classes 子集类
+
+# +
+# path = Path('./data/camvid-small')
+
+# def get_y_fn(x): return Path(str(x.parent)+'annot')/x.name
+
+# codes = array(['Sky', 'Building', 'Pole', 'Road', 'Sidewalk', 'Tree',
+#     'Sign', 'Fence', 'Car', 'Pedestrian', 'Cyclist', 'Void'])
+
+# src = (SegmentationItemList.from_folder(path)
+#        .split_by_folder(valid='val')
+#        .label_from_func(get_y_fn, classes=codes))
+
+# bs=8
+# data = (src.transform(get_transforms(), tfm_y=True)
+#         .databunch(bs=bs)
+#         .normalize(imagenet_stats))
+# -
+
+# ## Data 数据
+
+fnames = get_image_files(path_img)
+fnames[:3]
+
+lbl_names = get_image_files(path_lbl)
+lbl_names[:3]
+
+img_f = fnames[0]
+img = open_image(img_f)
+img.show(figsize=(5,5))
+
+get_y_fn = lambda x: path_lbl/f'{x.stem}_P{x.suffix}'
+
+mask = open_mask(get_y_fn(img_f))
+mask.show(figsize=(5,5), alpha=1)
+
+src_size = np.array(mask.shape[1:])
+src_size,mask.data
+
+codes = np.loadtxt(path/'codes.txt', dtype=str); codes
+
+# ## Datasets 数据集
+
+# +
+size = src_size//2
+
+free = gpu_mem_get_free_no_cache()
+# the max size of bs depends on the available GPU RAM
+if free > 8200: bs=8
+else:           bs=4
+print(f"using bs={bs}, have {free}MB of GPU RAM free")
+# -
+
+src = (SegmentationItemList.from_folder(path_img)
+       .split_by_fname_file('../valid.txt')
+       .label_from_func(get_y_fn, classes=codes))
+
+data = (src.transform(get_transforms(), size=size, tfm_y=True)
+        .databunch(bs=bs)
+        .normalize(imagenet_stats))
+
+data.show_batch(2, figsize=(10,7))
+
+data.show_batch(2, figsize=(10,7), ds_type=DatasetType.Valid)
+
+# ## Model 模型
+
+# +
+name2id = {v:k for k,v in enumerate(codes)}
+void_code = name2id['Void']
+
+def acc_camvid(input, target):
+    target = target.squeeze(1)
+    mask = target != void_code
+    return (input.argmax(dim=1)[mask]==target[mask]).float().mean()
+
+
+# -
+
+metrics=acc_camvid
+# metrics=accuracy
+
+wd=1e-2
+
+learn = unet_learner(data, models.resnet34, metrics=metrics, wd=wd)
+
+lr_find(learn)
+learn.recorder.plot()
+
+lr=3e-3
+
+learn.fit_one_cycle(10, slice(lr), pct_start=0.9)
+
+learn.save('stage-1')
+
+learn.load('stage-1');
+
+learn.show_results(rows=3, figsize=(8,9))
+
+learn.unfreeze()
+
+lrs = slice(lr/400,lr/4)
+
+learn.fit_one_cycle(12, lrs, pct_start=0.8)
+
+learn.save('stage-2');
+
+# ## Go big 用更大的数据集进行训练
+
+# You may have to restart your kernel and come back to this stage if you run out of memory, and may also need to decrease `bs`.<br>
+# 如果内存不够的话，你可能需要重启你的计算内核，然后再返回这一步，同时可能要减少 `bs` 的设定。
+
+# +
+learn.destroy()
+
+size = src_size
+
+free = gpu_mem_get_free_no_cache()
+# the max size of bs depends on the available GPU RAM
+if free > 8200: bs=3
+else:           bs=1
+print(f"using bs={bs}, have {free}MB of GPU RAM free")
+# -
+
+data = (src.transform(get_transforms(), size=size, tfm_y=True)
+        .databunch(bs=bs)
+        .normalize(imagenet_stats))
+
+learn = unet_learner(data, models.resnet34, metrics=metrics, wd=wd)
+
+learn.load('stage-2');
+
+lr_find(learn)
+learn.recorder.plot()
+
+lr=1e-3
+
+learn.fit_one_cycle(10, slice(lr), pct_start=0.8)
+
+learn.save('stage-1-big')
+
+learn.load('stage-1-big');
+
+learn.unfreeze()
+
+lrs = slice(1e-6,lr/10)
+
+learn.fit_one_cycle(10, lrs)
+
+learn.save('stage-2-big')
+
+learn.load('stage-2-big');
+
+learn.show_results(rows=3, figsize=(10,10))
diff --git a/zh-nbs/Lesson3_camvid_tiramisu.ipynb b/zh-nbs/Lesson3_camvid_tiramisu.ipynb
index d148a92..db64e3b 100644
--- ./zh-nbs/Lesson3_camvid_tiramisu.ipynb
+++ ./zh-nbs/Lesson3_camvid_tiramisu.ipynb
@@ -1315,6 +1315,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/zh-nbs/Lesson3_camvid_tiramisu.py b/zh-nbs/Lesson3_camvid_tiramisu.py
new file mode 100644
index 0000000..17a9ee8
--- /dev/null
+++ ./zh-nbs/Lesson3_camvid_tiramisu.py
@@ -0,0 +1,170 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Practical Deep Learning for Coders, v3
+
+# # Lesson 3_camvid_tiramisu
+
+# # Image segmentation with CamVid
+# # 用CamVid数据集进行图像分割
+
+# %reload_ext autoreload
+# %autoreload 2
+# %matplotlib inline
+
+from fastai import *
+from fastai.vision import *
+from fastai.callbacks.hooks import *
+
+# The One Hundred Layer Tiramisu paper used a modified version of Camvid, with smaller images and few classes. You can get it from the CamVid directory of this repo:<br>
+# One Hundred Layer Tiramisu这篇论文使用了改进版的CamVid数据集，该数据集图片更小、类别更少。你可以在以下库中的CamVid目录里找到它：
+#
+#     git clone https://github.com/alexgkendall/SegNet-Tutorial.git
+
+path = Path('./data/camvid-tiramisu')
+
+path.ls()
+
+# ## Data
+# ## 数据
+
+fnames = get_image_files(path/'val')
+fnames[:3]
+
+lbl_names = get_image_files(path/'valannot')
+lbl_names[:3]
+
+img_f = fnames[0]
+img = open_image(img_f)
+img.show(figsize=(5,5))
+
+
+# +
+def get_y_fn(x): return Path(str(x.parent)+'annot')/x.name
+
+codes = array(['Sky', 'Building', 'Pole', 'Road', 'Sidewalk', 'Tree',
+    'Sign', 'Fence', 'Car', 'Pedestrian', 'Cyclist', 'Void'])
+# -
+
+mask = open_mask(get_y_fn(img_f))
+mask.show(figsize=(5,5), alpha=1)
+
+src_size = np.array(mask.shape[1:])
+src_size,mask.data
+
+# ## Datasets
+# ## 数据集
+
+bs,size = 8,src_size//2
+
+src = (SegmentationItemList.from_folder(path)
+       .split_by_folder(valid='val')
+       .label_from_func(get_y_fn, classes=codes))
+
+data = (src.transform(get_transforms(), tfm_y=True)
+        .databunch(bs=bs)
+        .normalize(imagenet_stats))
+
+data.show_batch(2, figsize=(10,7))
+
+# ## Model
+# ## 模型
+
+# +
+name2id = {v:k for k,v in enumerate(codes)}
+void_code = name2id['Void']
+
+def acc_camvid(input, target):
+    target = target.squeeze(1)
+    mask = target != void_code
+    return (input.argmax(dim=1)[mask]==target[mask]).float().mean()
+
+
+# -
+
+metrics=acc_camvid
+wd=1e-2
+
+learn = unet_learner(data, models.resnet34, metrics=metrics, wd=wd, bottle=True)
+
+lr_find(learn)
+learn.recorder.plot()
+
+lr=2e-3
+
+learn.fit_one_cycle(10, slice(lr), pct_start=0.8)
+
+learn.save('stage-1')
+
+learn.load('stage-1');
+
+learn.unfreeze()
+
+lrs = slice(lr/100,lr)
+
+learn.fit_one_cycle(12, lrs, pct_start=0.8)
+
+learn.save('stage-2');
+
+# ## Go big
+# ## 用更大的数据集进行训练
+
+learn=None
+gc.collect()
+
+# You may have to restart your kernel and come back to this stage if you run out of memory, and may also need to decrease `bs`.<br>
+# 如果内存不够的话，你可能需要重启你的计算内核再返回这一步，同时可能要减少`bs`的设定。
+
+size = src_size
+bs=8
+
+data = (src.transform(get_transforms(), size=size, tfm_y=True)
+        .databunch(bs=bs)
+        .normalize(imagenet_stats))
+
+learn = unet_learner(data, models.resnet34, metrics=metrics, wd=wd, bottle=True).load('stage-2');
+
+lr_find(learn)
+learn.recorder.plot()
+
+lr=1e-3
+
+learn.fit_one_cycle(10, slice(lr), pct_start=0.8)
+
+learn.save('stage-1-big')
+
+learn.load('stage-1-big');
+
+learn.unfreeze()
+
+lrs = slice(lr/1000,lr/10)
+
+learn.fit_one_cycle(10, lrs)
+
+learn.save('stage-2-big')
+
+learn.load('stage-2-big');
+
+learn.show_results(rows=3, figsize=(9,11))
+
+# ## fin 
+# ## 小结
+
+# +
+# start: 480x360
+# -
+
+print(learn.summary())
diff --git a/zh-nbs/Lesson3_head_pose.ipynb b/zh-nbs/Lesson3_head_pose.ipynb
index 3f0391c..2133386 100644
--- ./zh-nbs/Lesson3_head_pose.ipynb
+++ ./zh-nbs/Lesson3_head_pose.ipynb
@@ -472,6 +472,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/zh-nbs/Lesson3_head_pose.py b/zh-nbs/Lesson3_head_pose.py
new file mode 100644
index 0000000..e7d59a2
--- /dev/null
+++ ./zh-nbs/Lesson3_head_pose.py
@@ -0,0 +1,122 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Practical Deep Learning for Coders, v3
+
+# # Lesson3_head_pose
+
+# # Regression with BIWI head pose dataset<br> 
+# # 用BIWI头部姿势数据集进行回归建模
+
+# This is a more advanced example to show how to create custom datasets and do regression with images. Our task is to find the center of the head in each image. The data comes from the [BIWI head pose dataset](https://data.vision.ee.ethz.ch/cvl/gfanelli/head_pose/head_forest.html#db), thanks to Gabriele Fanelli et al. We have converted the images to jpeg format, so you should download the converted dataset from [this link](https://s3.amazonaws.com/fast-ai-imagelocal/biwi_head_pose.tgz).<br>
+
+# 这个案例是一个更高级的示例，它展示了如何创建自定义数据集，并且对图像进行回归建模。 我们的任务是在每个图片中确定头部的中心位置。数据来自[BIWI头部姿势数据集](https://data.vision.ee.ethz.ch/cvl/gfanelli/head_pose/head_forest.html#db)。感谢Gabriele Fanelli等人的努力。我们已经把图片转化为jpeg格式，因此你应该从[这里](https://s3.amazonaws.com/fast-ai-imagelocal/biwi_head_pose.tgz)下载转化好的数据。
+
+# %reload_ext autoreload
+# %autoreload 2
+# %matplotlib inline
+
+from fastai.vision import *
+
+# ## Getting and converting the data
+# ## 数据获取与格式转换
+
+path = untar_data(URLs.BIWI_HEAD_POSE)
+
+cal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6); cal
+
+fname = '09/frame_00667_rgb.jpg'
+
+
+def img2txt_name(f): return path/f'{str(f)[:-7]}pose.txt'
+
+
+img = open_image(path/fname)
+img.show()
+
+ctr = np.genfromtxt(img2txt_name(fname), skip_header=3); ctr
+
+
+# +
+def convert_biwi(coords):
+    c1 = coords[0] * cal[0][0]/coords[2] + cal[0][2]
+    c2 = coords[1] * cal[1][1]/coords[2] + cal[1][2]
+    return tensor([c2,c1])
+
+def get_ctr(f):
+    ctr = np.genfromtxt(img2txt_name(f), skip_header=3)
+    return convert_biwi(ctr)
+
+def get_ip(img,pts): return ImagePoints(FlowField(img.size, pts), scale=True)
+
+
+# -
+
+get_ctr(fname)
+
+ctr = get_ctr(fname)
+img.show(y=get_ip(img, ctr), figsize=(6, 6))
+
+# ## Creating a dataset
+# ## 创建一个数据集
+
+data = (PointsItemList.from_folder(path)
+        .split_by_valid_func(lambda o: o.parent.name=='13')
+        .label_from_func(get_ctr)
+        .transform(get_transforms(), tfm_y=True, size=(120,160))
+        .databunch().normalize(imagenet_stats)
+       )
+
+data.show_batch(3, figsize=(9,6))
+
+# ## Train model
+# ## 训练模型
+
+learn = cnn_learner(data, models.resnet34)
+
+learn.lr_find()
+learn.recorder.plot()
+
+lr = 2e-2
+
+learn.fit_one_cycle(5, slice(lr))
+
+learn.save('stage-1')
+
+learn.load('stage-1');
+
+learn.show_results()
+
+# ## Data augmentation
+# ## 数据增强
+
+# +
+tfms = get_transforms(max_rotate=20, max_zoom=1.5, max_lighting=0.5, max_warp=0.4, p_affine=1., p_lighting=1.)
+
+data = (PointsItemList.from_folder(path)
+        .split_by_valid_func(lambda o: o.parent.name=='13')
+        .label_from_func(get_ctr)
+        .transform(tfms, tfm_y=True, size=(120,160))
+        .databunch().normalize(imagenet_stats)
+       )
+
+
+# +
+def _plot(i,j,ax):
+    x,y = data.train_ds[0]
+    x.show(ax, y=y)
+
+plot_multi(_plot, 3, 3, figsize=(8,6))
diff --git a/zh-nbs/Lesson3_imdb.ipynb b/zh-nbs/Lesson3_imdb.ipynb
index ab80cc0..5194b63 100644
--- ./zh-nbs/Lesson3_imdb.ipynb
+++ ./zh-nbs/Lesson3_imdb.ipynb
@@ -1368,6 +1368,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/zh-nbs/Lesson3_imdb.py b/zh-nbs/Lesson3_imdb.py
new file mode 100644
index 0000000..60a3995
--- /dev/null
+++ ./zh-nbs/Lesson3_imdb.py
@@ -0,0 +1,313 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Practical Deep Learning for Coders, v3
+
+# # Lesson3_imdb
+
+# # IMDB影评数据
+
+# %reload_ext autoreload
+# %autoreload 2
+# %matplotlib inline
+
+from fastai.text import *
+
+# ## Preparing the data 准备数据
+
+# First let's download the dataset we are going to study. The [dataset](http://ai.stanford.edu/~amaas/data/sentiment/) has been curated by Andrew Maas et al. and contains a total of 100,000 reviews on IMDB. 25,000 of them are labelled as positive and negative for training, another 25,000 are labelled for testing (in both cases they are highly polarized). The remaning 50,000 is an additional unlabelled data (but we will find a use for it nonetheless).
+#
+# 首先，让我们先下载需要使用的数据。 [IMDB](http://ai.stanford.edu/~amaas/data/sentiment/) 数据集由Andrew Maas等人收集，里面有10万条IMDB网站上的影评。其中2.5万条是积极的评论, 2.5万条是消极的评论, 另外2.5万条是用作测试的评论(这些数据两极分化得很厉害)，剩余的5万条是额外的未标记的数据（以后我们会将这些数据用做其他用途）。
+#
+# We'll begin with a sample we've prepared for you, so that things run quickly before going over the full dataset.
+#
+# 我们一起来看一下提前准备好的样本，这样会比跑遍整个数据集快一些。
+
+path = untar_data(URLs.IMDB_SAMPLE)
+path.ls()
+
+# It only contains one csv file, let's have a look at it.
+#
+# 例子里只包含了一个csv文档，我们一起来看一下里面的数据。
+
+df = pd.read_csv(path/'texts.csv')
+df.head()
+
+df['text'][1]
+
+# It contains one line per review, with the label ('negative' or 'positive'), the text and a flag to determine if it should be part of the validation set or the training set. If we ignore this flag, we can create a DataBunch containing this data in one line of code:
+#
+# 文档里的每一行都是一个影评，影评附有标签（“负面”或是“正面”）、评论文字以及一个标明是属于训练集还是验证集的标签，如果我们忽略这个（标明所属数据集的）标签，我们可以有下面这行代码来产生一个 DataBunch（数据堆）：
+
+data_lm = TextDataBunch.from_csv(path, 'texts.csv')
+
+# By executing this line a process was launched that took a bit of time. Let's dig a bit into it. Images could be fed (almost) directly into a model because they're just a big array of pixel values that are floats between 0 and 1. A text is composed of words, and we can't apply mathematical functions to them directly. We first have to convert them to numbers. This is done in two differents steps: tokenization and numericalization. A `TextDataBunch` does all of that behind the scenes for you.
+#
+# 运行这行代码会启动一个需要稍微花点时间的程序，让我们来更深入地了解一下。图像本质上是一个巨大的像素值数列，这个数列由0到1 之间的数字组成，因此图像数据基本上可以直接输入到模型中。但是，一段文字是由词组成的，而我们不能直接对词运用数学函数。那么我们首先需要将这些信息转化为数字。这一过程需要通过两部完成：分词和数值化。`TextDataBunch`在幕后为您完成所有这些工作。
+#
+# Before we delve into the explanations, let's take the time to save the things that were calculated.
+#
+# 在我们开始讲解内容之前，让我们先花点时间将计算好的数据存档。
+#
+
+data_lm.save()
+
+# Next time we launch this notebook, we can skip the cell above that took a bit of time (and that will take a lot more when you get to the full dataset) and load those results like this:
+#
+# 下次我们启动这个notebook, 可以直接跳过之前稍费时间的单元格，直接用下面的代码载入之前保存的结果（如果你载入的是全部数据，之前这些步骤会花费更多时间）：
+
+data = load_data(path)
+
+# ### Tokenization 分词
+
+# The first step of processing we make the texts go through is to split the raw sentences into words, or more exactly tokens. The easiest way to do this would be to split the string on spaces, but we can be smarter:
+#
+# 处理数据的第一步是将文字分拆成单词, 或者更确切地说, 标准词(tokens)。最简单的方式是基于空格对句子进行分拆, 但我们能更智能地分词：
+#
+# - we need to take care of punctuation
+# <br>我们需要考虑标点
+#
+# - some words are contractions of two different words, like isn't or don't
+# <br>有些词是由两个不同的词缩写的，比如isn't或don't
+#
+# - we may need to clean some parts of our texts, if there's HTML code for instance
+# <br>我们可能需要清理文本的某些部分，比如文字中可能会有HTML代码
+#
+# To see what the tokenizer had done behind the scenes, let's have a look at a few texts in a batch.<br>
+# 为了明白分词器幕后是如何工作的，让我们来看一下数据堆中的一些文本。
+
+data = TextClasDataBunch.from_csv(path, 'texts.csv')
+data.show_batch()
+
+# The texts are truncated at 100 tokens for more readability. We can see that it did more than just split on space and punctuation symbols: 
+#
+# 为了更简洁易读，我们将所有评论删节到100个词。我们可以看到文字标记化算法不仅仅是基于空格和标点进行了分词：
+#
+# - the "'s" are grouped together in one token
+# <br>所有“'s”都被合并为一个标准词
+#
+#
+# - the contractions are separated like this: "did", "n't"
+# <br>词语的缩写被分开，比如“did” 和 “n't”
+#
+#
+# - content has been cleaned for any HTML symbol and lower cased
+# <br>所有包含HTML连接的内容被清理，并且所有文字都采用小写
+#
+#
+# - there are several special tokens (all those that begin by xx), to replace unknown tokens (see below) or to introduce different text fields (here we only have one).
+# <br>为了代替未知的标准词（如下）或者引入不同的文本字段(这里我们只有一个)，（在结果中可以看到）有一些特殊的标准词（它们都以xx开头）
+
+# ### Numericalization 数值化
+
+# Once we have extracted tokens from our texts, we convert to integers by creating a list of all the words used. We only keep the ones that appear at least twice with a maximum vocabulary size of 60,000 (by default) and replace the ones that don't make the cut by the unknown token `UNK`.
+#
+# 一旦我们从文本中完成了标准词提取，就会生成一个包含所有词汇的列表，将标准词转化成整数。这里我们只保留至少出现两次的标准词，并设置词库上限为60,000(默认设置), 同时将所有不能分进行分词的词标记为“未知标准词” `UNK`。
+#
+# The correspondance from ids to tokens is stored in the `vocab` attribute of our datasets, in a dictionary called `itos` (for int to string).
+#
+# id和标准词的关系存储在数据集的`vocab`属性中，在字典 `itos` 中（由int类型转换成string类型）。
+
+data.vocab.itos[:10]
+
+# And if we look at what a what's in our datasets, we'll see the tokenized text as a representation:
+#
+# 如果我们查看数据集里的“what's”的形式，我们会看到如下经过分词后的文本： 
+
+data.train_ds[0][0]
+
+# But the underlying data is all numbers
+#
+# 但实际上，底层的数据形式都是数字
+
+data.train_ds[0][0].data[:10]
+
+# ### With the data block API 用data block API处理文字
+
+# We can use the data block API with NLP and have a lot more flexibility than what the default factory methods offer. In the previous example for instance, the data was randomly split between train and validation instead of reading the third column of the csv.
+#
+# 活地处理各种情况。比如在之前的例子中，数据随机分为训练集和验证集，而非通过读取csv中第三列的标签来分组。
+#
+# With the data block API though, we have to manually call the tokenize and numericalize steps. This allows more flexibility, and if you're not using the defaults from fastai, the various arguments to pass will appear in the step they're revelant, so it'll be more readable.
+#
+# 不过如果要使用数据块API，我们需要手动完成分词和数值化的各个步骤。这样可以更加灵活。如果你没有使用fastai工具包里的默认设置，你也可以像下面的步骤一样进行各种设置，并且代码可读性也更高。
+
+data = (TextList.from_csv(path, 'texts.csv', cols='text')
+                .split_from_df(col=2)
+                .label_from_df(cols=0)
+                .databunch())
+
+# ## Language model 语言模型
+
+# Note that language models can use a lot of GPU, so you may need to decrease batchsize here.
+#
+# 需要注意的是语言文字模型会用掉许多GPU，因此你可能会需要减小每个批次的样本容量。
+
+bs=48
+
+# Now let's grab the full dataset for what follows.
+#
+# 现在我们为接下来的步骤获取完整的数据集。
+
+path = untar_data(URLs.IMDB)
+path.ls()
+
+(path/'train').ls()
+
+# The reviews are in a training and test set following an imagenet structure. The only difference is that there is an `unsup` folder on top of `train` and `test` that contains the unlabelled data.
+#
+# 现在影评遵循imagenet的结构分到了训练集和测试集中。唯一的区别是，在`测试集`和`训练集`上会有个包括未标记数据的`unsup`文件夹。
+#
+# We're not going to train a model that classifies the reviews from scratch. Like in computer vision, we'll use a model pretrained on a bigger dataset (a cleaned subset of wikipedia called [wikitext-103](https://einstein.ai/research/blog/the-wikitext-long-term-dependency-language-modeling-dataset)). That model has been trained to guess what the next word is, its input being all the previous words. It has a recurrent structure and a hidden state that is updated each time it sees a new word. This hidden state thus contains information about the sentence up to that point.
+#
+# 我们不需要从无到有地训练一个影评分类模型。就像计算机视觉模型一样，我们将使用一个在更大训练集上预训练好的模型（在维基上有一个清洗好的子集  [wikitext-103](https://einstein.ai/research/blog/the-wikitext-long-term-dependency-language-modeling-dataset) ）。这个模型被训练来猜测下一个词是什么，它的输入数据是之前已有的词汇。该模型采用循环神经网络结构，并且有一个每次看到新词都会更新的隐层状态。 隐层状态里包含的信息，是文本中到截止这个点之前的所有句子。
+#
+# We are going to use that 'knowledge' of the English language to build our classifier, but first, like for computer vision, we need to fine-tune the pretrained model to our particular dataset. Because the English of the reviews left by people on IMDB isn't the same as the English of wikipedia, we'll need to adjust the parameters of our model by a little bit. Plus there might be some words that would be extremely common in the reviews dataset but would be barely present in wikipedia, and therefore might not be part of the vocabulary the model was trained on.
+#
+# 我们用这样的预训练模型信息来创建我们的分类器。但首先，正如计算机视觉一样，我们需要对预训练的模型进行调参来适应我们的这个数据集。由于IMDB上影评的英语语言和维基百科上的英语语言风格不尽相同，我们需要将参数进行一定的调整。另外，可能会有些词在影评数据中出现的频率较高，但在维基百科上基本没出现过，因此可能和模型预训练时用的词库不太一样。
+
+# This is where the unlabelled data is going to be useful to us, as we can use it to fine-tune our model. Let's create our data object with the data block API (next line takes a few minutes).
+#
+# 我们可以用未标记的数据进行模型微调，这就是未标记数据具有价值的地方。让我们通过数据块API来建立一个数据对象。（下行会花费数分钟的时间）
+
+data_lm = (TextList.from_folder(path)
+           #Inputs: all the text files in path
+            .filter_by_folder(include=['train', 'test', 'unsup']) 
+           #We may have other temp folders that contain text files so we only keep what's in train and test
+            .split_by_rand_pct(0.1)
+           #We randomly split and keep 10% (10,000 reviews) for validation
+            .label_for_lm()           
+           #We want to do a language model so we label accordingly
+            .databunch(bs=bs))
+data_lm.save('data_lm.pkl')
+
+# We have to use a special kind of `TextDataBunch` for the language model, that ignores the labels (that's why we put 0 everywhere), will shuffle the texts at each epoch before concatenating them all together (only for training, we don't shuffle for the validation set) and will send batches that read that text in order with targets that are the next word in the sentence.
+#
+# 对于语言模型，我们需要用一个特殊的`TextDataBunch`，它会忽略标签（这就是为什么我们给所有地方都设置为0的原因），在将每个轮次的文字合并在一起之前打乱所有的文字（仅限于模型训练，我们不会对验证集进行混洗），并会分批次按顺序读取文字和接下来对应的单词。
+#
+# The line before being a bit long, we want to load quickly the final ids by using the following cell.
+#
+# 之前的代码会有点长，我们可以用下面的代码用id快速导入对应的文字。
+
+data_lm = load_data(path, 'data_lm.pkl', bs=bs)
+
+data_lm.show_batch()
+
+# We can then put this in a learner object very easily with a model loaded with the pretrained weights. They'll be downloaded the first time you'll execute the following line and stored in `~/.fastai/models/` (or elsewhere if you specified different paths in your config file).
+#
+# 我们可以很轻易地将模型和预训练的权重结合为一个学习器对象。在你第一次运行下面的代码时，所有模型的信息会下载并存储到`~/.fastai/models/` 或者其他由你的config文件指定的地方。
+
+learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)
+
+learn.lr_find()
+
+learn.recorder.plot(skip_end=15)
+
+learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))
+
+learn.save('fit_head')
+
+learn.load('fit_head');
+
+# To complete the fine-tuning, we can then unfeeze and launch a new training.
+#
+# 要完成微调，我们可以解冻模型并开启新的训练。
+
+learn.unfreeze()
+
+learn.fit_one_cycle(10, 1e-3, moms=(0.8,0.7))
+
+learn.save('fine_tuned')
+
+# How good is our model? Well let's try to see what it predicts after a few given words.
+#
+# 我们的模型表现怎么样呢？ 嗯，让我们来看看在几个词过后模型预测出的词是怎样的。
+
+learn.load('fine_tuned');
+
+TEXT = "I liked this movie because"
+N_WORDS = 40
+N_SENTENCES = 2
+
+print("\n".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))
+
+# We have to save not only the model, but also its encoder, the part that's responsible for creating and updating the hidden state. For the next part, we don't care about the part that tries to guess the next word.
+#
+# 我们不但保存了模型，而且保存了它的编码器，（也就是）负责创建和更新隐层状态（的部分）。剩下的负责猜词的部分，我们就不管了。
+#
+#
+
+learn.save_encoder('fine_tuned_enc')
+
+# ## Classifier 分类器
+
+# Now, we'll create a new data object that only grabs the labelled data and keeps those labels. Again, this line takes a bit of time.
+#
+# 现在我们要创建一个新的数据对象，仅抓取有标签的数据并且保留标签。这个步骤可能会需要一点时间。
+
+path = untar_data(URLs.IMDB)
+
+# +
+data_clas = (TextList.from_folder(path, vocab=data_lm.vocab)
+             #grab all the text files in path
+             .split_by_folder(valid='test')
+             #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)
+             .label_from_folder(classes=['neg', 'pos'])
+             #label them all with their folders
+             .databunch(bs=bs))
+
+data_clas.save('data_clas.pkl')
+# -
+
+data_clas = load_data(path, 'data_clas.pkl', bs=bs)
+
+data_clas.show_batch()
+
+# We can then create a model to classify those reviews and load the encoder we saved before.
+#
+# 我们可以建立一个模型来对影评进行分类，并且导入之前存储好的编码器。
+
+learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)
+learn.load_encoder('fine_tuned_enc')
+
+learn.lr_find()
+
+learn.recorder.plot()
+
+learn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7))
+
+learn.save('first')
+
+learn.load('first');
+
+learn.freeze_to(-2)
+learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))
+
+learn.save('second')
+
+learn.load('second');
+
+learn.freeze_to(-3)
+learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))
+
+learn.save('third')
+
+learn.load('third');
+
+learn.unfreeze()
+learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))
+
+learn.predict("I really loved that movie, it was awesome!")
diff --git a/zh-nbs/Lesson3_planet.ipynb b/zh-nbs/Lesson3_planet.ipynb
index e99483f..e44d826 100644
--- ./zh-nbs/Lesson3_planet.ipynb
+++ ./zh-nbs/Lesson3_planet.ipynb
@@ -1065,6 +1065,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/zh-nbs/Lesson3_planet.py b/zh-nbs/Lesson3_planet.py
new file mode 100644
index 0000000..6509c64
--- /dev/null
+++ ./zh-nbs/Lesson3_planet.py
@@ -0,0 +1,236 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Practical Deep Learning for Coders, v3
+
+# # Lesson3_planet
+
+# ## Multi-label prediction with Planet Amazon dataset
+# ## 基于Planet Amazon数据集的多标签分类预测
+
+# %reload_ext autoreload
+# %autoreload 2
+# %matplotlib inline
+
+from fastai.vision import *
+
+# ## Getting the data 数据获取
+
+# The planet dataset isn't available on the [fastai dataset page](https://course.fast.ai/datasets) due to copyright restrictions. 
+#
+# 由于版权原因，我们不能将亚马逊雨林的数据集放在fastai网页的数据集页面。
+#
+# You can download it from Kaggle however. Let's see how to do this by using the [Kaggle API](https://github.com/Kaggle/kaggle-api) as it's going to be pretty useful to you if you want to join a competition or use other Kaggle datasets later on.
+#
+# 不过，你可以通过Kaggle网站来进行下载。让我们来看一下怎样通过 Kaggle API 下载数据。这项技能很重要，因为未来你可能会需要使用这个API来下载竞赛数据，或者使用其他Kaggle的数据集。
+#
+# First, install the Kaggle API by uncommenting the following line and executing it, or by executing it in your terminal (depending on your platform you may need to modify this slightly to either add `source activate fastai` or similar, or prefix `pip` with a path. Have a look at how `conda install` is called for your platform in the appropriate *Returning to work* section of https://course.fast.ai/. (Depending on your environment, you may also need to append "--user" to the command.)
+#
+# 首先，要安装Kaggle API需要取消下一行的注释并运行代码，或者在terminal里执行（这取决于你的运行系统，你可能需要对这个代码进行一点修改。你可能会需要添加`source activate fastai`来略作修改，也可以在下面的代码之前加入`pip`, 还可以在代码之后加上`--user`。 在你的系统中究竟该怎样使用`conda install`，可以参照https://course.fast.ai/ 页面的*Returning to work* 部分）。
+
+# +
+# ! {sys.executable} -m pip install kaggle --upgrade
+# -
+
+# Then you need to upload your credentials from Kaggle on your instance. Login to kaggle and click on your profile picture on the top left corner, then 'My account'. Scroll down until you find a button named 'Create New API Token' and click on it. This will trigger the download of a file named 'kaggle.json'.
+#
+# 接下来你需要在你的代码中上传你的身份验证资料。你需要登入Kaggle，在左上角点击你的头像，选择`我的账户`，然后向下滑动，直到你找到`创建新API许可权`，并点击这个按钮。这样会产生一个自动下载的名为`kaggle.json`的文件。
+#
+# Upload this file to the directory this notebook is running in, by clicking "Upload" on your main Jupyter page, then uncomment and execute the next two commands (or run them in a terminal). For Windows, uncomment the last two commands.
+#
+# 在Jupyter的主页上点击`Upload`，将下载好的文件上传到这个notebook运行的路径中。然后运行下面两行代码，你也可以直接在terminal里运行。如果你是windows用户，只运行后面两行代码即可。
+
+# +
+# # ! mkdir -p ~/.kaggle/
+# # ! mv kaggle.json ~/.kaggle/
+
+# For Windows, uncomment these two commands
+# # ! mkdir %userprofile%\.kaggle
+# # ! move kaggle.json %userprofile%\.kaggle
+# -
+
+# You're all set to download the data from [planet competition](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space). You **first need to go to its main page and accept its rules**, and run the two cells below (uncomment the shell commands to download and unzip the data). If you get a `403 forbidden` error it means you haven't accepted the competition rules yet (you have to go to the competition page, click on *Rules* tab, and then scroll to the bottom to find the *accept* button).
+#
+# 现在你可以开始从[planet competition](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space)下载数据。**需要注意的是，你要先去kaggle主页上接受相应的条款**，之后运行下面的两行代码，取消注释并且解压数据。如果你看到`403 forbidden`的字样，这代表你还没有接受比赛的条款。你需要去这个比赛的主页，点击*Rules* 按钮，下拉到页面最下方并点击*accept* 按钮。
+
+path = Config.data_path()/'planet'
+path.mkdir(parents=True, exist_ok=True)
+path
+
+# +
+# # ! kaggle competitions download -c planet-understanding-the-amazon-from-space -f train-jpg.tar.7z -p {path}  
+# # ! kaggle competitions download -c planet-understanding-the-amazon-from-space -f train_v2.csv -p {path}  
+# # ! unzip -q -n {path}/train_v2.csv.zip -d {path}
+# -
+
+# To extract the content of this file, we'll need 7zip, so uncomment the following line if you need to install it (or run `sudo apt install p7zip-full` in your terminal).
+#
+# 我们需要7zip来提取所有文件，因此如果你需要安装7zip, 你可以取消下面这一行代码的注释，然后运行就可以安装了（或者如果你是苹果系统用户，可以在terminal里运行`sudo apt install p7zip-full`）。
+
+# +
+# # ! conda install --yes --prefix {sys.prefix} -c haasad eidl7zip
+# -
+
+# And now we can unpack the data (uncomment to run - this might take a few minutes to complete).
+#
+# 我们可以运行下面的代码来解压数据（取消注释再运行——这可能需要几分钟才能完成）。
+
+# +
+# ! 7za -bd -y -so x {path}/train-jpg.tar.7z | tar xf - -C {path.as_posix()}
+# -
+
+# ## Multiclassification多标签分类问题
+
+# Contrary to the pets dataset studied in last lesson, here each picture can have multiple labels. If we take a look at the csv file containing the labels (in 'train_v2.csv' here) we see that each 'image_name' is associated to several tags separated by spaces.
+#
+# 与上节课学习的宠物数据集相比，这节课的数据集里每张图片都有多个标签。如果我们看一下导入的csv数据（在“train_v2.csv”这里），就可以看见每个图片名都有好几个由空格分开的标签。
+
+df = pd.read_csv(path/'train_v2.csv')
+df.head()
+
+# To put this in a `DataBunch` while using the [data block API](https://docs.fast.ai/data_block.html), we then need to using `ImageList` (and not `ImageDataBunch`). This will make sure the model created has the proper loss function to deal with the multiple classes.
+#
+# 我们将这些数据和标签用 [data block API](https://docs.fast.ai/data_block.html) 转化成`DataBunch`，接着需要使用`ImageList` （而不是`ImageDataBunch`）。这样做可以保证模型有正确的损失函数来处理多标签的问题。
+
+tfms = get_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0.)
+
+# We use parentheses around the data block pipeline below, so that we can use a multiline statement without needing to add '\\'.
+#
+# 我们在下面的代码前后使用括号，这样可以很方便的写入多行代码而不需要给每行末尾加“\\”。
+
+np.random.seed(42)
+src = (ImageList.from_csv(path, 'train_v2.csv', folder='train-jpg', suffix='.jpg')
+       .split_by_rand_pct(0.2)
+       .label_from_df(label_delim=' '))
+
+data = (src.transform(tfms, size=128)
+        .databunch().normalize(imagenet_stats))
+
+# `show_batch` still works, and show us the different labels separated by `;`.
+
+data.show_batch(rows=3, figsize=(12,9))
+
+# To create a `Learner` we use the same function as in lesson 1. Our base architecture is resnet50 again, but the metrics are a little bit differeent: we use `accuracy_thresh` instead of `accuracy`. In lesson 1, we determined the predicition for a given class by picking the final activation that was the biggest, but here, each activation can be 0. or 1. `accuracy_thresh` selects the ones that are above a certain threshold (0.5 by default) and compares them to the ground truth.
+#
+# 我们用第一课里同样的函数来创建一个`Learner`。我们的基础架构依然是resnet50, 但这次使用的度量函数有点不同：我们会使用`accuracy_thresh`来代替 `accuracy`。在第一课里，我们采用的分组标签是给定品种的最终激活函数的最大值，但是在这里，每个激活函数的值可以是0或1，由`accuracy_thresh`选取所有高于某个“阈值”（默认为0.5）的图像，然后与真实的标签做对比。
+#
+# As for Fbeta, it's the metric that was used by Kaggle on this competition. See [here](https://en.wikipedia.org/wiki/F1_score) for more details.
+#
+# 至于Fbeta, 它是这项Kaggle比赛使用的测度。欲知详情，可以看[这里](https://en.wikipedia.org/wiki/F1_score)
+
+arch = models.resnet50
+
+acc_02 = partial(accuracy_thresh, thresh=0.2)
+f_score = partial(fbeta, thresh=0.2)
+learn = cnn_learner(data, arch, metrics=[acc_02, f_score])
+
+# We use the LR Finder to pick a good learning rate.
+#
+# 我们使用LR Finder来选取一个好的学习率。
+
+learn.lr_find()
+
+learn.recorder.plot()
+
+# Then we can fit the head of our network.
+#
+# 接下来，我们可以用找好的学习率来训练神经网络模型了。
+
+learn.fit_one_cycle(5, slice(lr))
+
+learn.save('stage-1-rn50')
+
+# ...And fine-tune the whole model:
+#
+# 接下来给整个模型调参：
+
+learn.unfreeze()
+
+learn.lr_find()
+learn.recorder.plot()
+
+learn.fit_one_cycle(5, slice(1e-5, lr/5))
+
+learn.save('stage-2-rn50')
+
+# +
+data = (src.transform(tfms, size=256)
+        .databunch().normalize(imagenet_stats))
+
+learn.data = data
+data.train_ds[0][0].shape
+# -
+
+learn.freeze()
+
+learn.lr_find()
+learn.recorder.plot()
+
+lr=1e-2/2
+
+learn.fit_one_cycle(5, slice(lr))
+
+learn.save('stage-1-256-rn50')
+
+learn.unfreeze()
+
+learn.fit_one_cycle(5, slice(1e-5, lr/5))
+
+learn.recorder.plot_losses()
+
+learn.save('stage-2-256-rn50')
+
+# You won't really know how you're going until you submit to Kaggle, since the leaderboard isn't using the same subset as we have for training. But as a guide, 50th place (out of 938 teams) on the private leaderboard was a score of `0.930`.
+#
+# 正如我们训练时做的那样，排名榜单上使用了不同的数据子集, 如果你没有在Kaggle提交你的模型，你就无法知道自己的模型表现得怎么样。不过作为一个参考，`0.930`在非公开的榜单上大约是在938个团队里排到第50名。
+
+learn.export()
+
+# ## fin
+
+# (This section will be covered in part 2 - please don't ask about it just yet! :) )
+#
+# （这个部分在part2里已经被覆盖——就别在意这些细节啦 :)）
+
+# +
+# #! kaggle competitions download -c planet-understanding-the-amazon-from-space -f test-jpg.tar.7z -p {path}  
+#! 7za -bd -y -so x {path}/test-jpg.tar.7z | tar xf - -C {path}
+# #! kaggle competitions download -c planet-understanding-the-amazon-from-space -f test-jpg-additional.tar.7z -p {path}  
+#! 7za -bd -y -so x {path}/test-jpg-additional.tar.7z | tar xf - -C {path}
+# -
+
+test = ImageList.from_folder(path/'test-jpg').add(ImageList.from_folder(path/'test-jpg-additional'))
+len(test)
+
+learn = load_learner(path, test=test)
+preds, _ = learn.get_preds(ds_type=DatasetType.Test)
+
+thresh = 0.2
+labelled_preds = [' '.join([learn.data.classes[i] for i,p in enumerate(pred) if p > thresh]) for pred in preds]
+
+labelled_preds[:5]
+
+fnames = [f.name[:-4] for f in learn.data.test_ds.items]
+
+df = pd.DataFrame({'image_name':fnames, 'tags':labelled_preds}, columns=['image_name', 'tags'])
+
+df.to_csv(path/'submission.csv', index=False)
+
+# ! kaggle competitions submit planet-understanding-the-amazon-from-space -f {path/'submission.csv'} -m "My submission"
+
+# Private Leaderboard score: 0.9296 (around 80th)
+#
+# 内部排名榜单得分：0.9296（约第80位）
diff --git a/zh-nbs/Lesson4_collab.ipynb b/zh-nbs/Lesson4_collab.ipynb
index b73b8cd..6713733 100644
--- ./zh-nbs/Lesson4_collab.ipynb
+++ ./zh-nbs/Lesson4_collab.ipynb
@@ -1276,6 +1276,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/zh-nbs/Lesson4_collab.py b/zh-nbs/Lesson4_collab.py
new file mode 100644
index 0000000..da36da2
--- /dev/null
+++ ./zh-nbs/Lesson4_collab.py
@@ -0,0 +1,149 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Practical Deep Learning for Coders, v3
+
+# # Lesson4_collab
+
+from fastai.collab import *
+from fastai.tabular import *
+
+# # Collaborative filtering example
+# # 协同过滤案例
+
+# `collab` models use data in a `DataFrame` of user, items, and ratings.
+#
+# `collab`模型使用的是`DataFrame`中的一个（包含）用户、电影和评分的数据集。
+
+user,item,title = 'userId','movieId','title'
+
+path = untar_data(URLs.ML_SAMPLE)
+path
+
+ratings = pd.read_csv(path/'ratings.csv')
+ratings.head()
+
+# That's all we need to create and train a model:
+#
+# 以上就是我们用来训练模型的全部（数据）：
+
+data = CollabDataBunch.from_df(ratings, seed=42)
+
+y_range = [0,5.5]
+
+learn = collab_learner(data, n_factors=50, y_range=y_range)
+
+learn.fit_one_cycle(3, 5e-3)
+
+# ## Movielens 100k    
+
+# Let's try with the full Movielens 100k data dataset, available from http://files.grouplens.org/datasets/movielens/ml-100k.zip
+#
+# 让我们尝试一下用Movielens的全部数据进行建模。
+
+path=Config.data_path()/'ml-100k'
+
+ratings = pd.read_csv(path/'u.data', delimiter='\t', header=None,
+                      names=[user,item,'rating','timestamp'])
+ratings.head()
+
+movies = pd.read_csv(path/'u.item',  delimiter='|', encoding='latin-1', header=None,
+                    names=[item, 'title', 'date', 'N', 'url', *[f'g{i}' for i in range(19)]])
+movies.head()
+
+len(ratings)
+
+rating_movie = ratings.merge(movies[[item, title]])
+rating_movie.head()
+
+data = CollabDataBunch.from_df(rating_movie, seed=42, valid_pct=0.1, item_name=title)
+
+data.show_batch()
+
+y_range = [0,5.5]
+
+learn = collab_learner(data, n_factors=40, y_range=y_range, wd=1e-1)
+
+learn.lr_find()
+learn.recorder.plot(skip_end=15)
+
+learn.fit_one_cycle(5, 5e-3)
+
+learn.save('dotprod')
+
+# Here's [some benchmarks](https://www.librec.net/release/v1.3/example.html) on the same dataset for the popular Librec system for collaborative filtering. They show best results based on RMSE of 0.91, which corresponds to an MSE of `0.91**2 = 0.83`.
+#
+# [这里](https://www.librec.net/release/v1.3/example.html) 是一些在同一数据集上建模的基准数据。在表格中我们可以看到最好的模型的RMSE是0.91，对应的MSE是`0.91**2 = 0.83`。
+
+# ## Interpretation
+# ## 模型释义
+
+# ### Setup  调用
+
+learn.load('dotprod');
+
+learn.model
+
+g = rating_movie.groupby(title)['rating'].count()
+top_movies = g.sort_values(ascending=False).index.values[:1000]
+top_movies[:10]
+
+# ### Movie bias
+# ### 电影模型的偏差
+
+movie_bias = learn.bias(top_movies, is_item=True)
+movie_bias.shape
+
+mean_ratings = rating_movie.groupby(title)['rating'].mean()
+movie_ratings = [(b, i, mean_ratings.loc[i]) for i,b in zip(top_movies,movie_bias)]
+
+item0 = lambda o:o[0]
+
+sorted(movie_ratings, key=item0)[:15]
+
+sorted(movie_ratings, key=lambda o: o[0], reverse=True)[:15]
+
+# ### Movie weights
+# ### 电影模型权重
+
+movie_w = learn.weight(top_movies, is_item=True)
+movie_w.shape
+
+movie_pca = movie_w.pca(3)
+movie_pca.shape
+
+fac0,fac1,fac2 = movie_pca.t()
+movie_comp = [(f, i) for f,i in zip(fac0, top_movies)]
+
+sorted(movie_comp, key=itemgetter(0), reverse=True)[:10]
+
+sorted(movie_comp, key=itemgetter(0))[:10]
+
+movie_comp = [(f, i) for f,i in zip(fac1, top_movies)]
+
+sorted(movie_comp, key=itemgetter(0), reverse=True)[:10]
+
+sorted(movie_comp, key=itemgetter(0))[:10]
+
+idxs = np.random.choice(len(top_movies), 50, replace=False)
+idxs = list(range(50))
+X = fac0[idxs]
+Y = fac2[idxs]
+plt.figure(figsize=(15,15))
+plt.scatter(X, Y)
+for i, x, y in zip(top_movies[idxs], X, Y):
+    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)
+plt.show()
diff --git a/zh-nbs/Lesson4_tabular.ipynb b/zh-nbs/Lesson4_tabular.ipynb
index 7ed21a1..e972c1a 100644
--- ./zh-nbs/Lesson4_tabular.ipynb
+++ ./zh-nbs/Lesson4_tabular.ipynb
@@ -331,6 +331,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/zh-nbs/Lesson4_tabular.py b/zh-nbs/Lesson4_tabular.py
new file mode 100644
index 0000000..e425761
--- /dev/null
+++ ./zh-nbs/Lesson4_tabular.py
@@ -0,0 +1,56 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Practical Deep Learning for Coders, v3
+
+# # Lesson4_tabular
+
+# # Tabular models
+# # Tabular(表格)模型
+
+from fastai.tabular import *
+
+# Tabular data should be in a Pandas `DataFrame`.
+#
+# Tabular数据是Pandas里的`DataFrame`。
+
+path = untar_data(URLs.ADULT_SAMPLE)
+df = pd.read_csv(path/'adult.csv')
+
+dep_var = 'salary'
+cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']
+cont_names = ['age', 'fnlwgt', 'education-num']
+procs = [FillMissing, Categorify, Normalize]
+
+test = TabularList.from_df(df.iloc[800:1000].copy(), path=path, cat_names=cat_names, cont_names=cont_names)
+
+data = (TabularList.from_df(df, path=path, cat_names=cat_names, cont_names=cont_names, procs=procs)
+                           .split_by_idx(list(range(800,1000)))
+                           .label_from_df(cols=dep_var)
+                           .add_test(test)
+                           .databunch())
+
+data.show_batch(rows=10)
+
+learn = tabular_learner(data, layers=[200,100], metrics=accuracy)
+
+learn.fit(1, 1e-2)
+
+# ## Inference  预测
+
+row = df.iloc[0]
+
+learn.predict(row)
diff --git a/zh-nbs/Lesson5_sgd_mnist.ipynb b/zh-nbs/Lesson5_sgd_mnist.ipynb
index 9ebcbca..e1a99b0 100644
--- ./zh-nbs/Lesson5_sgd_mnist.ipynb
+++ ./zh-nbs/Lesson5_sgd_mnist.ipynb
@@ -670,6 +670,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/zh-nbs/Lesson5_sgd_mnist.py b/zh-nbs/Lesson5_sgd_mnist.py
new file mode 100644
index 0000000..e0dd1a7
--- /dev/null
+++ ./zh-nbs/Lesson5_sgd_mnist.py
@@ -0,0 +1,158 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Practical Deep Learning for Coders, v3
+
+# # Lesson5_sgd_mnist
+
+# %matplotlib inline
+from fastai.basics import *
+
+# # MNIST SGD
+# # 随机梯度下降
+
+# Get the 'pickled' MNIST dataset from http://deeplearning.net/data/mnist/mnist.pkl.gz. We're going to treat it as a standard flat dataset with fully connected layers, rather than using a CNN.
+#
+# 从[这里](http://deeplearning.net/data/mnist/mnist.pkl.gz) 下载pickled MNIST数据集。我们将用标准的平面文件全连接处理数据，而不是用卷积神经网络(CNN)。
+
+path = Config().data_path()/'mnist'
+
+path.ls()
+
+with gzip.open(path/'mnist.pkl.gz', 'rb') as f:
+    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')
+
+plt.imshow(x_train[0].reshape((28,28)), cmap="gray")
+x_train.shape
+
+x_train,y_train,x_valid,y_valid = map(torch.tensor, (x_train,y_train,x_valid,y_valid))
+n,c = x_train.shape
+x_train.shape, y_train.min(), y_train.max()
+
+# In lesson2-sgd we did these things ourselves:
+#
+# 在第二节的sgd例子中，我们定义了以下的函数：
+#
+# ```python
+# x = torch.ones(n,2) 
+# def mse(y_hat, y): return ((y_hat-y)**2).mean()
+# y_hat = x@a
+# ```
+#
+# Now instead we'll use PyTorch's functions to do it for us, and also to handle mini-batches (which we didn't do last time, since our dataset was so small).
+#
+# 现在我们用PyTorch的函数来帮我们完成这个工作，并且进行数据的迷你批次处理（上次因为数据集比较小，我们没有这样做）。
+
+bs=64
+train_ds = TensorDataset(x_train, y_train)
+valid_ds = TensorDataset(x_valid, y_valid)
+data = DataBunch.create(train_ds, valid_ds, bs=bs)
+
+x,y = next(iter(data.train_dl))
+x.shape,y.shape
+
+
+class Mnist_Logistic(nn.Module):
+    def __init__(self):
+        super().__init__()
+        self.lin = nn.Linear(784, 10, bias=True)
+
+    def forward(self, xb): return self.lin(xb)
+
+
+model = Mnist_Logistic().cuda()
+
+model
+
+model.lin
+
+model(x).shape
+
+[p.shape for p in model.parameters()]
+
+lr=2e-2
+
+loss_func = nn.CrossEntropyLoss()
+
+
+def update(x,y,lr):
+    wd = 1e-5
+    y_hat = model(x)
+    # weight decay
+    w2 = 0.
+    for p in model.parameters(): w2 += (p**2).sum()
+    # add to regular loss
+    loss = loss_func(y_hat, y) + w2*wd
+    loss.backward()
+    with torch.no_grad():
+        for p in model.parameters():
+            p.sub_(lr * p.grad)
+            p.grad.zero_()
+    return loss.item()
+
+
+losses = [update(x,y,lr) for x,y in data.train_dl]
+
+plt.plot(losses);
+
+
+class Mnist_NN(nn.Module):
+    def __init__(self):
+        super().__init__()
+        self.lin1 = nn.Linear(784, 50, bias=True)
+        self.lin2 = nn.Linear(50, 10, bias=True)
+
+    def forward(self, xb):
+        x = self.lin1(xb)
+        x = F.relu(x)
+        return self.lin2(x)
+
+
+model = Mnist_NN().cuda()
+
+losses = [update(x,y,lr) for x,y in data.train_dl]
+
+plt.plot(losses);
+
+model = Mnist_NN().cuda()
+
+
+def update(x,y,lr):
+    opt = optim.Adam(model.parameters(), lr)
+    y_hat = model(x)
+    loss = loss_func(y_hat, y)
+    loss.backward()
+    opt.step()
+    opt.zero_grad()
+    return loss.item()
+
+
+losses = [update(x,y,1e-3) for x,y in data.train_dl]
+
+plt.plot(losses);
+
+learn = Learner(data, Mnist_NN(), loss_func=loss_func, metrics=accuracy)
+
+# %debug
+
+learn.lr_find()
+learn.recorder.plot()
+
+learn.fit_one_cycle(1, 1e-2)
+
+learn.recorder.plot_lr(show_moms=True)
+
+learn.recorder.plot_losses()
diff --git a/zh-nbs/Lesson6_pets_more.ipynb b/zh-nbs/Lesson6_pets_more.ipynb
index b0839b0..1b1b8d5 100644
--- ./zh-nbs/Lesson6_pets_more.ipynb
+++ ./zh-nbs/Lesson6_pets_more.ipynb
@@ -1335,6 +1335,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/zh-nbs/Lesson6_pets_more.py b/zh-nbs/Lesson6_pets_more.py
new file mode 100644
index 0000000..603162c
--- /dev/null
+++ ./zh-nbs/Lesson6_pets_more.py
@@ -0,0 +1,201 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Practical Deep Learning for Coders, v3
+
+# # Lesson6_pets_more
+
+# # pets revisited
+# # 宠物分类问题回顾
+
+# +
+# %reload_ext autoreload
+# %autoreload 2
+# %matplotlib inline
+
+from fastai.vision import *
+# -
+
+bs = 64
+
+path = untar_data(URLs.PETS)/'images'
+
+# ## Data augmentation 数据增强
+
+tfms = get_transforms(max_rotate=20, max_zoom=1.3, max_lighting=0.4, max_warp=0.4,
+                      p_affine=1., p_lighting=1.)
+
+doc(get_transforms)
+
+src = ImageList.from_folder(path).split_by_rand_pct(0.2, seed=2)
+
+
+def get_data(size, bs, padding_mode='reflection'):
+    return (src.label_from_re(r'([^/]+)_\d+.jpg$')
+           .transform(tfms, size=size, padding_mode=padding_mode)
+           .databunch(bs=bs).normalize(imagenet_stats))
+
+
+data = get_data(224, bs, 'zeros')
+
+
+# +
+def _plot(i,j,ax):
+    x,y = data.train_ds[3]
+    x.show(ax, y=y)
+
+plot_multi(_plot, 3, 3, figsize=(8,8))
+# -
+
+data = get_data(224,bs)
+
+plot_multi(_plot, 3, 3, figsize=(8,8))
+
+# ## Train a model 训练网络
+
+gc.collect()
+learn = cnn_learner(data, models.resnet34, metrics=error_rate, bn_final=True)
+
+learn.fit_one_cycle(3, slice(1e-2), pct_start=0.8)
+
+learn.unfreeze()
+learn.fit_one_cycle(2, max_lr=slice(1e-6,1e-3), pct_start=0.8)
+
+data = get_data(352,bs)
+learn.data = data
+
+learn.fit_one_cycle(2, max_lr=slice(1e-6,1e-4))
+
+learn.save('352')
+
+# ## Convolution kernel 卷积核
+
+data = get_data(352,16)
+
+learn = cnn_learner(data, models.resnet34, metrics=error_rate, bn_final=True).load('352')
+
+idx=0
+x,y = data.valid_ds[idx]
+x.show()
+data.valid_ds.y[idx]
+
+k = tensor([
+    [0.  ,-5/3,1],
+    [-5/3,-5/3,1],
+    [1.  ,1   ,1],
+]).expand(1,3,3,3)/6
+
+k
+
+k.shape
+
+t = data.valid_ds[0][0].data; t.shape
+
+t[None].shape
+
+edge = F.conv2d(t[None], k)
+
+show_image(edge[0], figsize=(5,5));
+
+data.c
+
+learn.model
+
+print(learn.summary())
+
+# ## Heatmap 热力图
+
+m = learn.model.eval();
+
+xb,_ = data.one_item(x)
+xb_im = Image(data.denorm(xb)[0])
+xb = xb.cuda()
+
+from fastai.callbacks.hooks import *
+
+
+def hooked_backward(cat=y):
+    with hook_output(m[0]) as hook_a: 
+        with hook_output(m[0], grad=True) as hook_g:
+            preds = m(xb)
+            preds[0,int(cat)].backward()
+    return hook_a,hook_g
+
+
+hook_a,hook_g = hooked_backward()
+
+acts  = hook_a.stored[0].cpu()
+acts.shape
+
+avg_acts = acts.mean(0)
+avg_acts.shape
+
+
+def show_heatmap(hm):
+    _,ax = plt.subplots()
+    xb_im.show(ax)
+    ax.imshow(hm, alpha=0.6, extent=(0,352,352,0),
+              interpolation='bilinear', cmap='magma');
+
+
+show_heatmap(avg_acts)
+
+# ## Grad-CAM
+
+# Paper: [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/abs/1610.02391) <br><br>
+# 论文: [Grad-CAM: 基于神经网络梯度局部化的可视化解释](https://arxiv.org/abs/1610.02391)
+
+grad = hook_g.stored[0][0].cpu()
+grad_chan = grad.mean(1).mean(1)
+grad.shape,grad_chan.shape
+
+mult = (acts*grad_chan[...,None,None]).mean(0)
+
+show_heatmap(mult)
+
+fn = path/'../other/bulldog_maine.jpg' #Replace with your own image
+
+x = open_image(fn); x
+
+xb,_ = data.one_item(x)
+xb_im = Image(data.denorm(xb)[0])
+xb = xb.cuda()
+
+hook_a,hook_g = hooked_backward()
+
+# +
+acts = hook_a.stored[0].cpu()
+grad = hook_g.stored[0][0].cpu()
+
+grad_chan = grad.mean(1).mean(1)
+mult = (acts*grad_chan[...,None,None]).mean(0)
+# -
+
+show_heatmap(mult)
+
+data.classes[0]
+
+hook_a,hook_g = hooked_backward(0)
+
+# +
+acts = hook_a.stored[0].cpu()
+grad = hook_g.stored[0][0].cpu()
+
+grad_chan = grad.mean(1).mean(1)
+mult = (acts*grad_chan[...,None,None]).mean(0)
+# -
+
+show_heatmap(mult)
diff --git a/zh-nbs/Lesson6_rossmann.ipynb b/zh-nbs/Lesson6_rossmann.ipynb
index 00bc3da..933b159 100644
--- ./zh-nbs/Lesson6_rossmann.ipynb
+++ ./zh-nbs/Lesson6_rossmann.ipynb
@@ -1795,6 +1795,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/zh-nbs/Lesson6_rossmann.py b/zh-nbs/Lesson6_rossmann.py
new file mode 100644
index 0000000..0e4ab88
--- /dev/null
+++ ./zh-nbs/Lesson6_rossmann.py
@@ -0,0 +1,151 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Practical Deep Learning for Coders, v3
+
+# # Lesson6_rossmann
+
+# %reload_ext autoreload
+# %autoreload 2
+
+from fastai.tabular import *
+
+# # Rossmann
+# # 连锁店销量预估
+
+# ## Data preparation 数据准备
+
+# To create the feature-engineered train_clean and test_clean from the Kaggle competition data, run `rossman_data_clean.ipynb`. One important step that deals with time series is this:<br><br>
+# 为了从Kaggle竞赛数据集中创建经过特征工程处理的train_clean和test_clean，运行`rossman_data_clean.ipynb`，其中一个处理时间序列数据的重要步骤如下：
+#
+# ```python
+# add_datepart(train, "Date", drop=False)
+# add_datepart(test, "Date", drop=False)
+# ```
+
+path = Config().data_path()/'rossmann'
+train_df = pd.read_pickle(path/'train_clean')
+
+train_df.head().T
+
+n = len(train_df); n
+
+# ### Experimenting with a sample 样例数据实验
+
+idx = np.random.permutation(range(n))[:2000]
+idx.sort()
+small_train_df = train_df.iloc[idx[:1000]]
+small_test_df = train_df.iloc[idx[1000:]]
+small_cont_vars = ['CompetitionDistance', 'Mean_Humidity']
+small_cat_vars =  ['Store', 'DayOfWeek', 'PromoInterval']
+small_train_df = small_train_df[small_cat_vars + small_cont_vars + ['Sales']]
+small_test_df = small_test_df[small_cat_vars + small_cont_vars + ['Sales']]
+
+small_train_df.head()
+
+small_test_df.head()
+
+categorify = Categorify(small_cat_vars, small_cont_vars)
+categorify(small_train_df)
+categorify(small_test_df, test=True)
+
+small_test_df.head()
+
+small_train_df.PromoInterval.cat.categories
+
+small_train_df['PromoInterval'].cat.codes[:5]
+
+fill_missing = FillMissing(small_cat_vars, small_cont_vars)
+fill_missing(small_train_df)
+fill_missing(small_test_df, test=True)
+
+small_train_df[small_train_df['CompetitionDistance_na'] == True]
+
+# ### Preparing full data set 准备全部数据集
+
+train_df = pd.read_pickle(path/'train_clean')
+test_df = pd.read_pickle(path/'test_clean')
+
+len(train_df),len(test_df)
+
+procs=[FillMissing, Categorify, Normalize]
+
+# +
+cat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',
+    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',
+    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',
+    'SchoolHoliday_fw', 'SchoolHoliday_bw']
+
+cont_vars = ['CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',
+   'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', 
+   'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',
+   'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday']
+# -
+
+dep_var = 'Sales'
+df = train_df[cat_vars + cont_vars + [dep_var,'Date']].copy()
+
+test_df['Date'].min(), test_df['Date'].max()
+
+cut = train_df['Date'][(train_df['Date'] == train_df['Date'][len(test_df)])].index.max()
+cut
+
+valid_idx = range(cut)
+
+df[dep_var].head()
+
+data = (TabularList.from_df(df, path=path, cat_names=cat_vars, cont_names=cont_vars, procs=procs,)
+                .split_by_idx(valid_idx)
+                .label_from_df(cols=dep_var, label_cls=FloatList, log=True)
+                .add_test(TabularList.from_df(test_df, path=path, cat_names=cat_vars, cont_names=cont_vars))
+                .databunch())
+
+doc(FloatList)
+
+# ## Model 模型
+
+max_log_y = np.log(np.max(train_df['Sales'])*1.2)
+y_range = torch.tensor([0, max_log_y], device=defaults.device)
+
+learn = tabular_learner(data, layers=[1000,500], ps=[0.001,0.01], emb_drop=0.04, 
+                        y_range=y_range, metrics=exp_rmspe)
+
+learn.model
+
+len(data.train_ds.cont_names)
+
+learn.lr_find()
+
+learn.recorder.plot()
+
+learn.fit_one_cycle(5, 1e-3, wd=0.2)
+
+learn.save('1')
+
+learn.recorder.plot_losses(last=-1)
+
+learn.load('1');
+
+learn.fit_one_cycle(5, 3e-4)
+
+learn.fit_one_cycle(5, 3e-4)
+
+# (10th place in the competition was 0.108)
+
+test_preds=learn.get_preds(DatasetType.Test)
+test_df["Sales"]=np.exp(test_preds[0].data).numpy().T[0]
+test_df[["Id","Sales"]]=test_df[["Id","Sales"]].astype("int")
+test_df[["Id","Sales"]].to_csv("rossmann_submission.csv",index=False)
diff --git a/zh-nbs/Lesson7_human_numbers.ipynb b/zh-nbs/Lesson7_human_numbers.ipynb
index 6b618ec..e1b6838 100644
--- ./zh-nbs/Lesson7_human_numbers.ipynb
+++ ./zh-nbs/Lesson7_human_numbers.ipynb
@@ -1489,6 +1489,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/zh-nbs/Lesson7_human_numbers.py b/zh-nbs/Lesson7_human_numbers.py
new file mode 100644
index 0000000..3f1a75f
--- /dev/null
+++ ./zh-nbs/Lesson7_human_numbers.py
@@ -0,0 +1,256 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Practical Deep Learning for Coders, v2
+
+# # Lesson7_human_numbers
+
+# # Human numbers 
+# # 人类数字问题
+
+from fastai.text import *
+
+bs=64
+
+# ## Data 数据
+
+path = untar_data(URLs.HUMAN_NUMBERS)
+path.ls()
+
+
+def readnums(d): return [', '.join(o.strip() for o in open(path/d).readlines())]
+
+
+train_txt = readnums('train.txt'); train_txt[0][:80]
+
+valid_txt = readnums('valid.txt'); valid_txt[0][-80:]
+
+# +
+train = TextList(train_txt, path=path)
+valid = TextList(valid_txt, path=path)
+
+src = ItemLists(path=path, train=train, valid=valid).label_for_lm()
+data = src.databunch(bs=bs)
+# -
+
+train[0].text[:80]
+
+len(data.valid_ds[0][0].data)
+
+data.bptt, len(data.valid_dl)
+
+13017/70/bs
+
+it = iter(data.valid_dl)
+x1,y1 = next(it)
+x2,y2 = next(it)
+x3,y3 = next(it)
+it.close()
+
+x1.numel()+x2.numel()+x3.numel()
+
+x1.shape,y1.shape
+
+x2.shape,y2.shape
+
+x1[:,0]
+
+y1[:,0]
+
+v = data.valid_ds.vocab
+
+v.textify(x1[0])
+
+v.textify(y1[0])
+
+v.textify(x2[0])
+
+v.textify(x3[0])
+
+v.textify(x1[1])
+
+v.textify(x2[1])
+
+v.textify(x3[1])
+
+v.textify(x3[-1])
+
+data.show_batch(ds_type=DatasetType.Valid)
+
+# ## Single fully connected model 单全连接模型
+
+data = src.databunch(bs=bs, bptt=3)
+
+x,y = data.one_batch()
+x.shape,y.shape
+
+nv = len(v.itos); nv
+
+nh=64
+
+
+def loss4(input,target): return F.cross_entropy(input, target[:,-1])
+def acc4 (input,target): return accuracy(input, target[:,-1])
+
+
+class Model0(nn.Module):
+    def __init__(self):
+        super().__init__()
+        self.i_h = nn.Embedding(nv,nh)  # green arrow
+        self.h_h = nn.Linear(nh,nh)     # brown arrow
+        self.h_o = nn.Linear(nh,nv)     # blue arrow
+        self.bn = nn.BatchNorm1d(nh)
+        
+    def forward(self, x):
+        h = self.bn(F.relu(self.h_h(self.i_h(x[:,0]))))
+        if x.shape[1]>1:
+            h = h + self.i_h(x[:,1])
+            h = self.bn(F.relu(self.h_h(h)))
+        if x.shape[1]>2:
+            h = h + self.i_h(x[:,2])
+            h = self.bn(F.relu(self.h_h(h)))
+        return self.h_o(h)
+
+
+learn = Learner(data, Model0(), loss_func=loss4, metrics=acc4)
+
+learn.fit_one_cycle(6, 1e-4)
+
+
+# ## Same thing with a loop 使用一个循环来完成同样的功能
+
+class Model1(nn.Module):
+    def __init__(self):
+        super().__init__()
+        self.i_h = nn.Embedding(nv,nh)  # green arrow
+        self.h_h = nn.Linear(nh,nh)     # brown arrow
+        self.h_o = nn.Linear(nh,nv)     # blue arrow
+        self.bn = nn.BatchNorm1d(nh)
+        
+    def forward(self, x):
+        h = torch.zeros(x.shape[0], nh).to(device=x.device)
+        for i in range(x.shape[1]):
+            h = h + self.i_h(x[:,i])
+            h = self.bn(F.relu(self.h_h(h)))
+        return self.h_o(h)
+
+
+learn = Learner(data, Model1(), loss_func=loss4, metrics=acc4)
+
+learn.fit_one_cycle(6, 1e-4)
+
+# ## Multi fully connected model   复全连接网络
+
+data = src.databunch(bs=bs, bptt=20)
+
+x,y = data.one_batch()
+x.shape,y.shape
+
+
+class Model2(nn.Module):
+    def __init__(self):
+        super().__init__()
+        self.i_h = nn.Embedding(nv,nh)
+        self.h_h = nn.Linear(nh,nh)
+        self.h_o = nn.Linear(nh,nv)
+        self.bn = nn.BatchNorm1d(nh)
+        
+    def forward(self, x):
+        h = torch.zeros(x.shape[0], nh).to(device=x.device)
+        res = []
+        for i in range(x.shape[1]):
+            h = h + self.i_h(x[:,i])
+            h = F.relu(self.h_h(h))
+            res.append(self.h_o(self.bn(h)))
+        return torch.stack(res, dim=1)
+
+
+learn = Learner(data, Model2(), metrics=accuracy)
+
+learn.fit_one_cycle(10, 1e-4, pct_start=0.1)
+
+
+# ## Maintain state 状态维护
+
+class Model3(nn.Module):
+    def __init__(self):
+        super().__init__()
+        self.i_h = nn.Embedding(nv,nh)
+        self.h_h = nn.Linear(nh,nh)
+        self.h_o = nn.Linear(nh,nv)
+        self.bn = nn.BatchNorm1d(nh)
+        self.h = torch.zeros(bs, nh).cuda()
+        
+    def forward(self, x):
+        res = []
+        h = self.h
+        for i in range(x.shape[1]):
+            h = h + self.i_h(x[:,i])
+            h = F.relu(self.h_h(h))
+            res.append(self.bn(h))
+        self.h = h.detach()
+        res = torch.stack(res, dim=1)
+        res = self.h_o(res)
+        return res
+
+
+learn = Learner(data, Model3(), metrics=accuracy)
+
+learn.fit_one_cycle(20, 3e-3)
+
+
+# ## nn.RNN
+
+class Model4(nn.Module):
+    def __init__(self):
+        super().__init__()
+        self.i_h = nn.Embedding(nv,nh)
+        self.rnn = nn.RNN(nh,nh, batch_first=True)
+        self.h_o = nn.Linear(nh,nv)
+        self.bn = BatchNorm1dFlat(nh)
+        self.h = torch.zeros(1, bs, nh).cuda()
+        
+    def forward(self, x):
+        res,h = self.rnn(self.i_h(x), self.h)
+        self.h = h.detach()
+        return self.h_o(self.bn(res))
+
+
+learn = Learner(data, Model4(), metrics=accuracy)
+
+learn.fit_one_cycle(20, 3e-3)
+
+
+# ## 2-layer GRU 2层GRU
+
+class Model5(nn.Module):
+    def __init__(self):
+        super().__init__()
+        self.i_h = nn.Embedding(nv,nh)
+        self.rnn = nn.GRU(nh, nh, 2, batch_first=True)
+        self.h_o = nn.Linear(nh,nv)
+        self.bn = BatchNorm1dFlat(nh)
+        self.h = torch.zeros(2, bs, nh).cuda()
+        
+    def forward(self, x):
+        res,h = self.rnn(self.i_h(x), self.h)
+        self.h = h.detach()
+        return self.h_o(self.bn(res))
+
+
+learn = Learner(data, Model5(), metrics=accuracy)
+
+learn.fit_one_cycle(10, 1e-2)
diff --git a/zh-nbs/Lesson7_resnet_mnist.ipynb b/zh-nbs/Lesson7_resnet_mnist.ipynb
index 1089815..e1fd4af 100644
--- ./zh-nbs/Lesson7_resnet_mnist.ipynb
+++ ./zh-nbs/Lesson7_resnet_mnist.ipynb
@@ -1157,6 +1157,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/zh-nbs/Lesson7_resnet_mnist.py b/zh-nbs/Lesson7_resnet_mnist.py
new file mode 100644
index 0000000..022d56a
--- /dev/null
+++ ./zh-nbs/Lesson7_resnet_mnist.py
@@ -0,0 +1,187 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Practical Deep Learning for Coders, v3
+
+# # Lesson7_resnet_mnist
+
+# # MNIST CNN
+
+# %reload_ext autoreload
+# %autoreload 2
+# %matplotlib inline
+
+from fastai.vision import *
+
+# ### Data 数据
+
+path = untar_data(URLs.MNIST)
+
+path.ls()
+
+il = ImageList.from_folder(path, convert_mode='L')
+
+il.items[0]
+
+defaults.cmap='binary'
+
+il
+
+il[0].show()
+
+sd = il.split_by_folder(train='training', valid='testing')
+
+sd
+
+(path/'training').ls()
+
+ll = sd.label_from_folder()
+
+ll
+
+x,y = ll.train[0]
+
+x.show()
+print(y,x.shape)
+
+tfms = ([*rand_pad(padding=3, size=28, mode='zeros')], [])
+
+ll = ll.transform(tfms)
+
+bs = 128
+
+# not using imagenet_stats because not using pretrained model
+data = ll.databunch(bs=bs).normalize()
+
+x,y = data.train_ds[0]
+
+x.show()
+print(y)
+
+
+def _plot(i,j,ax): data.train_ds[0][0].show(ax, cmap='gray')
+plot_multi(_plot, 3, 3, figsize=(8,8))
+
+xb,yb = data.one_batch()
+xb.shape,yb.shape
+
+data.show_batch(rows=3, figsize=(5,5))
+
+
+# ### Basic CNN with batchnorm 批量正规化（归一化）的基础版CNN模型
+
+def conv(ni,nf): return nn.Conv2d(ni, nf, kernel_size=3, stride=2, padding=1)
+
+
+model = nn.Sequential(
+    conv(1, 8), # 14
+    nn.BatchNorm2d(8),
+    nn.ReLU(),
+    conv(8, 16), # 7
+    nn.BatchNorm2d(16),
+    nn.ReLU(),
+    conv(16, 32), # 4
+    nn.BatchNorm2d(32),
+    nn.ReLU(),
+    conv(32, 16), # 2
+    nn.BatchNorm2d(16),
+    nn.ReLU(),
+    conv(16, 10), # 1
+    nn.BatchNorm2d(10),
+    Flatten()     # remove (1,1) grid
+)
+
+learn = Learner(data, model, loss_func = nn.CrossEntropyLoss(), metrics=accuracy)
+
+print(learn.summary())
+
+xb = xb.cuda()
+
+model(xb).shape
+
+learn.lr_find(end_lr=100)
+
+learn.recorder.plot()
+
+learn.fit_one_cycle(3, max_lr=0.1)
+
+
+# ### Refactor 重构
+
+def conv2(ni,nf): return conv_layer(ni,nf,stride=2)
+
+
+model = nn.Sequential(
+    conv2(1, 8),   # 14
+    conv2(8, 16),  # 7
+    conv2(16, 32), # 4
+    conv2(32, 16), # 2
+    conv2(16, 10), # 1
+    Flatten()      # remove (1,1) grid
+)
+
+learn = Learner(data, model, loss_func = nn.CrossEntropyLoss(), metrics=accuracy)
+
+learn.fit_one_cycle(10, max_lr=0.1)
+
+
+# ### Resnet-ish
+
+class ResBlock(nn.Module):
+    def __init__(self, nf):
+        super().__init__()
+        self.conv1 = conv_layer(nf,nf)
+        self.conv2 = conv_layer(nf,nf)
+        
+    def forward(self, x): return x + self.conv2(self.conv1(x))
+
+
+help(res_block)
+
+model = nn.Sequential(
+    conv2(1, 8),
+    res_block(8),
+    conv2(8, 16),
+    res_block(16),
+    conv2(16, 32),
+    res_block(32),
+    conv2(32, 16),
+    res_block(16),
+    conv2(16, 10),
+    Flatten()
+)
+
+
+def conv_and_res(ni,nf): return nn.Sequential(conv2(ni, nf), res_block(nf))
+
+
+model = nn.Sequential(
+    conv_and_res(1, 8),
+    conv_and_res(8, 16),
+    conv_and_res(16, 32),
+    conv_and_res(32, 16),
+    conv2(16, 10),
+    Flatten()
+)
+
+learn = Learner(data, model, loss_func = nn.CrossEntropyLoss(), metrics=accuracy)
+
+learn.lr_find(end_lr=100)
+learn.recorder.plot()
+
+learn.fit_one_cycle(12, max_lr=0.05)
+
+print(learn.summary())
diff --git a/zh-nbs/Lesson7_superres.ipynb b/zh-nbs/Lesson7_superres.ipynb
index d341f3c..7f9af5a 100644
--- ./zh-nbs/Lesson7_superres.ipynb
+++ ./zh-nbs/Lesson7_superres.ipynb
@@ -1305,6 +1305,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/zh-nbs/Lesson7_superres.py b/zh-nbs/Lesson7_superres.py
new file mode 100644
index 0000000..716f963
--- /dev/null
+++ ./zh-nbs/Lesson7_superres.py
@@ -0,0 +1,207 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Practical Deep Learning for Coders, v3
+
+# # Lesson7_superres
+
+# # Super resolution
+
+# # 分辨率增强模型
+
+# +
+import fastai
+from fastai.vision import *
+from fastai.callbacks import *
+from fastai.utils.mem import *
+
+from torchvision.models import vgg16_bn
+# -
+
+path = untar_data(URLs.PETS)
+path_hr = path/'images'
+path_lr = path/'small-96'
+path_mr = path/'small-256'
+
+il = ImageList.from_folder(path_hr)
+
+
+def resize_one(fn, i, path, size):
+    dest = path/fn.relative_to(path_hr)
+    dest.parent.mkdir(parents=True, exist_ok=True)
+    img = PIL.Image.open(fn)
+    targ_sz = resize_to(img, size, use_min=True)
+    img = img.resize(targ_sz, resample=PIL.Image.BILINEAR).convert('RGB')
+    img.save(dest, quality=60)
+
+
+# create smaller image sets the first time this nb is run
+sets = [(path_lr, 96), (path_mr, 256)]
+for p,size in sets:
+    if not p.exists(): 
+        print(f"resizing to {size} into {p}")
+        parallel(partial(resize_one, path=p, size=size), il.items)
+
+# +
+bs,size=32,128
+arch = models.resnet34
+
+src = ImageImageList.from_folder(path_lr).split_by_rand_pct(0.1, seed=42)
+
+
+# -
+
+def get_data(bs,size):
+    data = (src.label_from_func(lambda x: path_hr/x.name)
+           .transform(get_transforms(max_zoom=2.), size=size, tfm_y=True)
+           .databunch(bs=bs).normalize(imagenet_stats, do_y=True))
+
+    data.c = 3
+    return data
+
+
+data = get_data(bs,size)
+
+data.show_batch(ds_type=DatasetType.Valid, rows=2, figsize=(9,9))
+
+# ## Feature loss 特征损失
+
+t = data.valid_ds[0][1].data
+t = torch.stack([t,t])
+
+
+def gram_matrix(x):
+    n,c,h,w = x.size()
+    x = x.view(n, c, -1)
+    return (x @ x.transpose(1,2))/(c*h*w)
+
+
+gram_matrix(t)
+
+base_loss = F.l1_loss
+
+vgg_m = vgg16_bn(True).features.cuda().eval()
+requires_grad(vgg_m, False)
+
+blocks = [i-1 for i,o in enumerate(children(vgg_m)) if isinstance(o,nn.MaxPool2d)]
+blocks, [vgg_m[i] for i in blocks]
+
+
+class FeatureLoss(nn.Module):
+    def __init__(self, m_feat, layer_ids, layer_wgts):
+        super().__init__()
+        self.m_feat = m_feat
+        self.loss_features = [self.m_feat[i] for i in layer_ids]
+        self.hooks = hook_outputs(self.loss_features, detach=False)
+        self.wgts = layer_wgts
+        self.metric_names = ['pixel',] + [f'feat_{i}' for i in range(len(layer_ids))
+              ] + [f'gram_{i}' for i in range(len(layer_ids))]
+
+    def make_features(self, x, clone=False):
+        self.m_feat(x)
+        return [(o.clone() if clone else o) for o in self.hooks.stored]
+    
+    def forward(self, input, target):
+        out_feat = self.make_features(target, clone=True)
+        in_feat = self.make_features(input)
+        self.feat_losses = [base_loss(input,target)]
+        self.feat_losses += [base_loss(f_in, f_out)*w
+                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]
+        self.feat_losses += [base_loss(gram_matrix(f_in), gram_matrix(f_out))*w**2 * 5e3
+                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]
+        self.metrics = dict(zip(self.metric_names, self.feat_losses))
+        return sum(self.feat_losses)
+    
+    def __del__(self): self.hooks.remove()
+
+
+feat_loss = FeatureLoss(vgg_m, blocks[2:5], [5,15,2])
+
+# ## Train 训练
+
+wd = 1e-3
+learn = unet_learner(data, arch, wd=wd, loss_func=feat_loss, callback_fns=LossMetrics,
+                     blur=True, norm_type=NormType.Weight)
+gc.collect();
+
+learn.lr_find()
+learn.recorder.plot()
+
+lr = 1e-3
+
+
+def do_fit(save_name, lrs=slice(lr), pct_start=0.9):
+    learn.fit_one_cycle(10, lrs, pct_start=pct_start)
+    learn.save(save_name)
+    learn.show_results(rows=1, imgsize=5)
+
+
+do_fit('1a', slice(lr*10))
+
+learn.unfreeze()
+
+do_fit('1b', slice(1e-5,lr))
+
+data = get_data(12,size*2)
+
+learn.data = data
+learn.freeze()
+gc.collect()
+
+learn.load('1b');
+
+do_fit('2a')
+
+learn.unfreeze()
+
+do_fit('2b', slice(1e-6,1e-4), pct_start=0.3)
+
+# ## Test 测试
+
+learn = None
+gc.collect();
+
+256/320*1024
+
+256/320*1600
+
+free = gpu_mem_get_free_no_cache()
+# the max size of the test image depends on the available GPU RAM 
+if free > 8000: size=(1280, 1600) # >  8GB RAM
+else:           size=( 820, 1024) # <= 8GB RAM
+print(f"using size={size}, have {free}MB of GPU RAM free")
+
+learn = unet_learner(data, arch, loss_func=F.l1_loss, blur=True, norm_type=NormType.Weight)
+
+data_mr = (ImageImageList.from_folder(path_mr).split_by_rand_pct(0.1, seed=42)
+          .label_from_func(lambda x: path_hr/x.name)
+          .transform(get_transforms(), size=size, tfm_y=True)
+          .databunch(bs=1).normalize(imagenet_stats, do_y=True))
+data_mr.c = 3
+
+learn.load('2b');
+
+learn.data = data_mr
+
+fn = data_mr.valid_ds.x.items[0]; fn
+
+img = open_image(fn); img.shape
+
+p,img_hr,b = learn.predict(img)
+
+show_image(img, figsize=(18,15), interpolation='nearest');
+
+Image(img_hr).show(figsize=(18,15))
diff --git a/zh-nbs/Lesson7_superres_gan.ipynb b/zh-nbs/Lesson7_superres_gan.ipynb
index e61cc72..08c5fdc 100644
--- ./zh-nbs/Lesson7_superres_gan.ipynb
+++ ./zh-nbs/Lesson7_superres_gan.ipynb
@@ -1166,6 +1166,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/zh-nbs/Lesson7_superres_gan.py b/zh-nbs/Lesson7_superres_gan.py
new file mode 100644
index 0000000..15fddd7
--- /dev/null
+++ ./zh-nbs/Lesson7_superres_gan.py
@@ -0,0 +1,214 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Practical Deep Learning for Coders, v3
+
+# # Lesson7_superres_gan
+
+# # Pretrained GAN 
+
+# # 预训练生成对抗网络
+
+import fastai
+from fastai.vision import *
+from fastai.callbacks import *
+from fastai.vision.gan import *
+
+path = untar_data(URLs.PETS)
+path_hr = path/'images'
+path_lr = path/'crappy'
+
+# ## Crappified data  噪化数据
+
+# Prepare the input data by crappifying images. 
+
+# 使用噪化的图片准备输入数据
+
+from crappify import *
+
+# Uncomment the first time you run this notebook. 
+
+# 如果首次运行这个notebook，请把注释打开
+
+# +
+#il = ImageList.from_folder(path_hr)
+#parallel(crappifier(path_lr, path_hr), il.items)
+# -
+
+# For gradual resizing we can change the commented line here. 
+
+# 如果需要对图片进行渐变修改或者缩放尺寸请打开下面的注释
+
+bs,size=32, 128
+# bs,size = 24,160
+#bs,size = 8,256
+arch = models.resnet34
+
+# ## Pre-train generator 预训练数据生成器
+
+# Now let's pretrain the generator.
+
+# 现在我们来预训练数据生成器
+
+arch = models.resnet34
+src = ImageImageList.from_folder(path_lr).split_by_rand_pct(0.1, seed=42)
+
+
+def get_data(bs,size):
+    data = (src.label_from_func(lambda x: path_hr/x.name)
+           .transform(get_transforms(max_zoom=2.), size=size, tfm_y=True)
+           .databunch(bs=bs).normalize(imagenet_stats, do_y=True))
+
+    data.c = 3
+    return data
+
+
+data_gen = get_data(bs,size)
+
+data_gen.show_batch(4)
+
+wd = 1e-3
+
+y_range = (-3.,3.)
+
+loss_gen = MSELossFlat()
+
+
+def create_gen_learner():
+    return unet_learner(data_gen, arch, wd=wd, blur=True, norm_type=NormType.Weight,
+                         self_attention=True, y_range=y_range, loss_func=loss_gen)
+
+
+learn_gen = create_gen_learner()
+
+learn_gen.fit_one_cycle(2, pct_start=0.8)
+
+learn_gen.unfreeze()
+
+learn_gen.fit_one_cycle(3, slice(1e-6,1e-3))
+
+learn_gen.show_results(rows=4)
+
+learn_gen.save('gen-pre2')
+
+# ## Save generated images 保存生成的图片
+
+learn_gen.load('gen-pre2');
+
+name_gen = 'image_gen'
+path_gen = path/name_gen
+
+# +
+# shutil.rmtree(path_gen)
+# -
+
+path_gen.mkdir(exist_ok=True)
+
+
+def save_preds(dl):
+    i=0
+    names = dl.dataset.items
+    
+    for b in dl:
+        preds = learn_gen.pred_batch(batch=b, reconstruct=True)
+        for o in preds:
+            o.save(path_gen/names[i].name)
+            i += 1
+
+
+save_preds(data_gen.fix_dl)
+
+PIL.Image.open(path_gen.ls()[0])
+
+# ## Train critic 训练评价网络
+
+learn_gen=None
+gc.collect()
+
+
+# Pretrain the critic on crappy vs not crappy. 
+
+# 使用噪化和非噪化数据来预训练批评者网络
+
+def get_crit_data(classes, bs, size):
+    src = ImageList.from_folder(path, include=classes).split_by_rand_pct(0.1, seed=42)
+    ll = src.label_from_folder(classes=classes)
+    data = (ll.transform(get_transforms(max_zoom=2.), size=size)
+           .databunch(bs=bs).normalize(imagenet_stats))
+    data.c = 3
+    return data
+
+
+data_crit = get_crit_data([name_gen, 'images'], bs=bs, size=size)
+
+data_crit.show_batch(rows=3, ds_type=DatasetType.Train, imgsize=3)
+
+loss_critic = AdaptiveLoss(nn.BCEWithLogitsLoss())
+
+
+def create_critic_learner(data, metrics):
+    return Learner(data, gan_critic(), metrics=metrics, loss_func=loss_critic, wd=wd)
+
+
+learn_critic = create_critic_learner(data_crit, accuracy_thresh_expand)
+
+learn_critic.fit_one_cycle(6, 1e-3)
+
+learn_critic.save('critic-pre2')
+
+# ## GAN
+
+# Now we'll combine those pretrained model in a GAN. 
+#
+# 现在我们将这些经过预训练的模型整合到GAN里。
+
+learn_crit=None
+learn_gen=None
+gc.collect()
+
+data_crit = get_crit_data(['crappy', 'images'], bs=bs, size=size)
+
+learn_crit = create_critic_learner(data_crit, metrics=None).load('critic-pre2')
+
+learn_gen = create_gen_learner().load('gen-pre2')
+
+# To define a GAN Learner, we just have to specify the learner objects foor the generator and the critic. The switcher is a callback that decides when to switch from discriminator to generator and vice versa. Here we do as many iterations of the discriminator as needed to get its loss back < 0.5 then one iteration of the generator.<br><br>
+# 为了定义一个GAN学习器，我们需要为生成器和评价网络指定学习组件对象。switcher（切换器）是一个回调（装置）,这个装置决定何时在鉴别器和生成器之间转换。这里我们针对鉴别器训练足够多的iterations（迭代次数）直到其损失函数值小于0.5，然后再对生成器训练一次。
+#
+# The loss of the critic is given by `learn_crit.loss_func`. We take the average of this loss function on the batch of real predictions (target 1) and the batch of fake predicitions (target 0). <br><br>
+# 评价网络的损失由`learn_crit.loss_func`获得。基于正预测值(目标为1)和负预测值(目标为0)的迭代训练会针对这个损失函数取均值。
+#
+# The loss of the generator is weighted sum (weights in `weights_gen`) of `learn_crit.loss_func` on the batch of fake (passed throught the critic to become predictions) with a target of 1, and the `learn_gen.loss_func` applied to the output (batch of fake) and the target (corresponding batch of superres images).<br><br>
+# 生成器网络的损失则是`learn_crit.loss_func`和`learn_gen.loss_func`的加权和（权重在`weights_gen`中），（其中）`learn_crit.loss_func`是基于目标为1的fake（失败的）批次（通过评价网络成为预测值），`learn_gen.loss_func`则应用到fake批次的输出及目标上（对应一批超分辨率图像）。
+
+switcher = partial(AdaptiveGANSwitcher, critic_thresh=0.65)
+learn = GANLearner.from_learners(learn_gen, learn_crit, weights_gen=(1.,50.), show_img=False, switcher=switcher,
+                                 opt_func=partial(optim.Adam, betas=(0.,0.99)), wd=wd)
+learn.callback_fns.append(partial(GANDiscriminativeLR, mult_lr=5.))
+
+lr = 1e-4
+
+learn.fit(40,lr)
+
+learn.save('gan-1c')
+
+learn.data=get_data(16,192)
+
+learn.fit(10,lr/2)
+
+learn.show_results(rows=16)
+
+learn.save('gan-1c')
diff --git a/zh-nbs/Lesson7_superres_imagenet.ipynb b/zh-nbs/Lesson7_superres_imagenet.ipynb
index 9fbcd79..6e0aace 100644
--- ./zh-nbs/Lesson7_superres_imagenet.ipynb
+++ ./zh-nbs/Lesson7_superres_imagenet.ipynb
@@ -531,6 +531,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/zh-nbs/Lesson7_superres_imagenet.py b/zh-nbs/Lesson7_superres_imagenet.py
new file mode 100644
index 0000000..cf8e891
--- /dev/null
+++ ./zh-nbs/Lesson7_superres_imagenet.py
@@ -0,0 +1,183 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Practical Deep Learning for Coders, v3
+
+# # Lesson7_superres_imagenet
+
+# # Super resolution on Imagenet Imagenet    
+
+# # 分辨率增强模型
+
+import fastai
+from fastai.vision import *
+from fastai.callbacks import *
+from fastai.utils.mem import *
+
+from torchvision.models import vgg16_bn
+
+torch.cuda.set_device(0)
+
+# +
+path = Path('data/imagenet')
+path_hr = path/'train'
+path_lr = path/'small-64/train'
+path_mr = path/'small-256/train'
+
+# note: this notebook relies on models created by lesson7-superres.ipynb
+path_pets = untar_data(URLs.PETS)
+# -
+
+il = ImageList.from_folder(path_hr)
+
+
+def resize_one(fn, i, path, size):
+    dest = path/fn.relative_to(path_hr)
+    dest.parent.mkdir(parents=True, exist_ok=True)
+    img = PIL.Image.open(fn)
+    targ_sz = resize_to(img, size, use_min=True)
+    img = img.resize(targ_sz, resample=PIL.Image.BILINEAR).convert('RGB')
+    img.save(dest, quality=60)
+
+
+assert path.exists(), f"need imagenet dataset @ {path}"
+# create smaller image sets the first time this nb is run
+sets = [(path_lr, 64), (path_mr, 256)]
+for p,size in sets:
+    if not p.exists(): 
+        print(f"resizing to {size} into {p}")
+        parallel(partial(resize_one, path=p, size=size), il.items)
+
+# +
+free = gpu_mem_get_free_no_cache()
+# the max size of the test image depends on the available GPU RAM 
+if free > 8200: bs,size=16,256  
+else:           bs,size=8,256
+print(f"using bs={bs}, size={size}, have {free}MB of GPU RAM free")
+
+arch = models.resnet34
+# sample = 0.1
+sample = False
+
+tfms = get_transforms()
+# -
+
+src = ImageImageList.from_folder(path_lr)
+
+if sample: src = src.filter_by_rand(sample, seed=42)
+
+src = src.split_by_rand_pct(0.1, seed=42)
+
+
+def get_data(bs,size):
+    data = (src.label_from_func(lambda x: path_hr/x.relative_to(path_lr))
+           .transform(get_transforms(max_zoom=2.), size=size, tfm_y=True)
+           .databunch(bs=bs).normalize(imagenet_stats, do_y=True))
+
+    data.c = 3
+    return data
+
+
+data = get_data(bs,size)
+
+
+# ## Feature loss   特征损失
+
+def gram_matrix(x):
+    n,c,h,w = x.size()
+    x = x.view(n, c, -1)
+    return (x @ x.transpose(1,2))/(c*h*w)
+
+
+vgg_m = vgg16_bn(True).features.cuda().eval()
+requires_grad(vgg_m, False)
+blocks = [i-1 for i,o in enumerate(children(vgg_m)) if isinstance(o,nn.MaxPool2d)]
+
+# +
+base_loss = F.l1_loss
+
+class FeatureLoss(nn.Module):
+    def __init__(self, m_feat, layer_ids, layer_wgts):
+        super().__init__()
+        self.m_feat = m_feat
+        self.loss_features = [self.m_feat[i] for i in layer_ids]
+        self.hooks = hook_outputs(self.loss_features, detach=False)
+        self.wgts = layer_wgts
+        self.metric_names = ['pixel',] + [f'feat_{i}' for i in range(len(layer_ids))
+              ] + [f'gram_{i}' for i in range(len(layer_ids))]
+
+    def make_features(self, x, clone=False):
+        self.m_feat(x)
+        return [(o.clone() if clone else o) for o in self.hooks.stored]
+    
+    def forward(self, input, target):
+        out_feat = self.make_features(target, clone=True)
+        in_feat = self.make_features(input)
+        self.feat_losses = [base_loss(input,target)]
+        self.feat_losses += [base_loss(f_in, f_out)*w
+                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]
+        self.feat_losses += [base_loss(gram_matrix(f_in), gram_matrix(f_out))*w**2 * 5e3
+                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]
+        self.metrics = dict(zip(self.metric_names, self.feat_losses))
+        return sum(self.feat_losses)
+    
+    def __del__(self): self.hooks.remove()
+
+
+# -
+
+feat_loss = FeatureLoss(vgg_m, blocks[2:5], [5,15,2])
+
+# ## Train   训练
+
+wd = 1e-3
+learn = unet_learner(data, arch, wd=wd, loss_func=feat_loss, callback_fns=LossMetrics, blur=True, norm_type=NormType.Weight)
+gc.collect();
+
+learn.unfreeze()
+
+# relies on first running lesson7-superres.ipynb which created the following model
+learn.load((path_pets/'small-96'/'models'/'2b').absolute());
+
+learn.fit_one_cycle(1, slice(1e-6,1e-4))
+
+learn.save('imagenet')
+
+learn.show_results(rows=3, imgsize=5)
+
+learn.recorder.plot_losses()
+
+# ## Test 测试
+
+_=learn.load('imagenet')
+
+data_mr = (ImageImageList.from_folder(path_mr).split_by_rand_pct(0.1, seed=42)
+          .label_from_func(lambda x: path_hr/x.relative_to(path_mr))
+          .transform(get_transforms(), size=(820,1024), tfm_y=True)
+          .databunch(bs=2).normalize(imagenet_stats, do_y=True))
+
+learn.data = data_mr
+
+# here put some image you want to enhance, e.g. the original notebook uses a single video frame from a powerpoint presentation on dropout paper
+fn = path_pets/'other'/'dropout.jpg'
+
+img = open_image(fn); img.shape
+
+_,img_hr,b = learn.predict(img)
+
+show_image(img, figsize=(18,15), interpolation='nearest');
+
+Image(img_hr).show(figsize=(18,15))
diff --git a/zh-nbs/Lesson7_wgan.ipynb b/zh-nbs/Lesson7_wgan.ipynb
index bed0d4b..7dec78c 100644
--- ./zh-nbs/Lesson7_wgan.ipynb
+++ ./zh-nbs/Lesson7_wgan.ipynb
@@ -518,6 +518,9 @@
   }
  ],
  "metadata": {
+  "jupytext": {
+   "formats": "ipynb,py"
+  },
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
diff --git a/zh-nbs/Lesson7_wgan.py b/zh-nbs/Lesson7_wgan.py
new file mode 100644
index 0000000..2726428
--- /dev/null
+++ ./zh-nbs/Lesson7_wgan.py
@@ -0,0 +1,113 @@
+# -*- coding: utf-8 -*-
+# ---
+# jupyter:
+#   jupytext:
+#     formats: ipynb,py
+#     text_representation:
+#       extension: .py
+#       format_name: light
+#       format_version: '1.5'
+#       jupytext_version: 1.5.2
+#   kernelspec:
+#     display_name: Python 3
+#     language: python
+#     name: python3
+# ---
+
+# # Practical Deep Learning for Coders, v3
+
+# # Lesson7_wgan
+
+# # wgan
+
+# %reload_ext autoreload
+# %autoreload 2
+# %matplotlib inline
+
+from fastai.vision import *
+from fastai.vision.gan import *
+
+# ## LSun bedroom data
+
+# ## 起居室数据集
+
+# For this lesson, we'll be using the bedrooms from the LSUN dataset. The full dataset is a bit too large so we'll use a sample from kaggle.
+
+# 本节课，我们将使用[LSUN dataset](http://lsun.cs.princeton.edu/2017/)的起居室数据集。整个数据集略大，因此这里我们只从[kaggle](https://www.kaggle.com/jhoward/lsun_bedroom)节选部分数据。
+
+path = untar_data(URLs.LSUN_BEDROOMS)
+
+
+# We then grab all the images in the folder with the data block API. We don't create a validation set here for reasons we'll explain later. It consists of random noise of size 100 by default (can be changed below) as inputs and the images of bedrooms as targets. That's why we do tfm_y=True in the transforms, then apply the normalization to the ys and not the xs.
+
+# 我们使用data block API抓取所有图片到文件夹中。我们不会创建一个独立验证集，原因后面我们会做解释。输入数据里包含了均值为100的随机噪声（也可以修改），并将起居室的图片作为目标。这也是我们在转换时，使用`tfm_y=True`参数，并且对ys而不是xs执行正规化操作的原因。
+
+def get_data(bs, size):
+    return (GANItemList.from_folder(path, noise_sz=100)
+               .split_none()
+               .label_from_func(noop)
+               .transform(tfms=[[crop_pad(size=size, row_pct=(0,1), col_pct=(0,1))], []], size=size, tfm_y=True)
+               .databunch(bs=bs)
+               .normalize(stats = [torch.tensor([0.5,0.5,0.5]), torch.tensor([0.5,0.5,0.5])], do_x=False, do_y=True))
+
+
+# We'll begin with a small side and use gradual resizing.
+
+# 我们将从小规模开始，逐步调整规模。
+
+data = get_data(128, 64)
+
+data.show_batch(rows=5)
+
+# ## Models 模型
+
+# GAN stands for Generative Adversarial Nets and were invented by Ian Goodfellow. The concept is that we will train two models at the same time: a generator and a critic. The generator will try to make new images similar to the ones in our dataset, and the critic will try to classify real images from the ones the generator does. The generator returns images, the critic a single number (usually 0. for fake images and 1. for real ones).
+
+# GAN即生成对抗网络 [Generative Adversarial Nets](https://arxiv.org/pdf/1406.2661.pdf)，该网络由Ian Goodfellow发明。其核心概念是我们将同时训练两个模型：一个生成器网络和一个评判网络。生成器网络将自创和数据集中的图片相似的新图片，而评判网络则试图从生成器生成的图片中分辨出真实的图片。生成器给出图片，评判网络则输出一个（作为评判结果的）值（通常0表示赝品,1表示真品）。
+
+# We train them against each other in the sense that at each step (more or less), we:
+
+# 这两个网络每一步都在做对抗训练（或多或少可以这么解释），我们（如下这样操做）：
+
+# 1.	Freeze the generator and train the critic for one step by:
+#     -	getting one batch of true images (let's call that real)
+#     -	generating one batch of fake images (let's call that fake)
+#     -	have the critic evaluate each batch and compute a loss function from that; the important part is that it rewards positively the detection of real images and penalizes the fake ones
+#     -	update the weights of the critic with the gradients of this loss
+#
+
+# 1. 冻结生成器训练评判网络:
+#   - 获取一批真品图片(我们简称为`真`)
+#   - 产生一批赝品图片(我们简称为`假`)
+#   - 让评判网络评估这批数据并计算出损失函数值；重要的一点是，如果判断为真就要给与奖励，反之，判断为假则予以惩罚。
+#   - 使用损失函数的梯度来更新评判网络的权重
+
+# 2.	Freeze the critic and train the generator for one step by:
+#     - generating one batch of fake images
+#     - evaluate the critic on it
+#     - return a loss that rewards posisitivly the critic thinking those are real images; the important part is that it rewards positively the detection of real images and penalizes the fake ones
+#     - update the weights of the generator with the gradients of this loss
+
+# 2. 冻结评判网络训练生成器：
+#   - 产生一批赝品图片
+#   - 评判网络对这些图片进行评判
+#   - 返回损失（函数），这是评判网络得到的所有真实图片的回馈，它的重要作用是，探测出真实图片就给予奖励,反之则给予惩罚
+#   - 使用损失函数的梯度来更新生成器网络的权重
+
+# Here, we'll use the Wassertein GAN.
+
+# 这里,我们将使用[Wassertein GAN](https://arxiv.org/pdf/1701.07875.pdf)这篇论文中的GAN网络。
+
+# We create a generator and a critic that we pass to `gan_learner`. The noise_size is the size of the random vector from which our generator creates images.<br><br>
+# 我们将创建一个生成器和一个评判网络，把他们作为参数传给`gan_learner`。noise_size参数是生成器创造新图片时的随机向量的变动范围。
+
+generator = basic_generator(in_size=64, n_channels=3, n_extra_layers=1)
+critic    = basic_critic   (in_size=64, n_channels=3, n_extra_layers=1)
+
+learn = GANLearner.wgan(data, generator, critic, switch_eval=False,
+                        opt_func = partial(optim.Adam, betas = (0.,0.99)), wd=0.)
+
+learn.fit(30,2e-4)
+
+learn.gan_trainer.switch(gen_mode=True)
+learn.show_results(ds_type=DatasetType.Train, rows=16, figsize=(8,8))
